{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 \u00b6 What Is Arlon? \u00b6 Arlon is a declarative, gitops based fleet management tool for Kubernetes clusters. It allows administrators to: Deploy and upgrade a large number of workload clusters Secure clusters by installing and configuring policies Install a set of applications / add-ons on the clusters all in a structured, predictable manner. Arlon makes Kubernetes cluster fleet management secure, version controlled, auditable and easy to perform at scale. Arlon takes advantage of multiple declarative cluster management API providers for the actual cluster orchestration. The first two supported API providers are Cluster API and Crossplane. Arlon uses ArgoCD as the underlying Kubernetes manifest deployment and enforcement engine. A workload cluster is composed of the following constructs: Cluster spec : a description of the infrastructure and external settings of a cluster, e.g. Kubernetes version, cloud provider, cluster type, node instance type. Profile : a grouping of configuration bundles which will be installed into the cluster Configuration bundle : a unit of configuration which contains (or references) one or more Kubernetes manifests. A bundle can encapsulate anything that can be deployed onto a cluster: an RBAC ruleset, an add-on, an application, etc... Arlon Benefits \u00b6 Improves time to market by enabling better velocity for developers through infrastructure management that is more fluid and agile. Define, store, change and enforce your cluster infrastructure & application add-ons at scale. Reduces the risk of unexpected infrastructure downtime and outages, or unexpected security misconfiguration, with consistent management of infrastructure and security policies. Allows IT and Platform Ops admins to operate large scale of clusters, infrastructure & add-ons with significantly reduced team size & operational overhead, using GitOps. Contents \u00b6 Concepts Installation Tutorial (gen-1) Tutorial (gen-2) Architecture","title":"Home"},{"location":"#introduction","text":"","title":"Introduction"},{"location":"#_1","text":"","title":""},{"location":"#what-is-arlon","text":"Arlon is a declarative, gitops based fleet management tool for Kubernetes clusters. It allows administrators to: Deploy and upgrade a large number of workload clusters Secure clusters by installing and configuring policies Install a set of applications / add-ons on the clusters all in a structured, predictable manner. Arlon makes Kubernetes cluster fleet management secure, version controlled, auditable and easy to perform at scale. Arlon takes advantage of multiple declarative cluster management API providers for the actual cluster orchestration. The first two supported API providers are Cluster API and Crossplane. Arlon uses ArgoCD as the underlying Kubernetes manifest deployment and enforcement engine. A workload cluster is composed of the following constructs: Cluster spec : a description of the infrastructure and external settings of a cluster, e.g. Kubernetes version, cloud provider, cluster type, node instance type. Profile : a grouping of configuration bundles which will be installed into the cluster Configuration bundle : a unit of configuration which contains (or references) one or more Kubernetes manifests. A bundle can encapsulate anything that can be deployed onto a cluster: an RBAC ruleset, an add-on, an application, etc...","title":"What Is Arlon?"},{"location":"#arlon-benefits","text":"Improves time to market by enabling better velocity for developers through infrastructure management that is more fluid and agile. Define, store, change and enforce your cluster infrastructure & application add-ons at scale. Reduces the risk of unexpected infrastructure downtime and outages, or unexpected security misconfiguration, with consistent management of infrastructure and security policies. Allows IT and Platform Ops admins to operate large scale of clusters, infrastructure & add-ons with significantly reduced team size & operational overhead, using GitOps.","title":"Arlon Benefits"},{"location":"#contents","text":"Concepts Installation Tutorial (gen-1) Tutorial (gen-2) Architecture","title":"Contents"},{"location":"architecture/","text":"Architecture \u00b6 Arlon is composed of a controller, a library, and a CLI that exposes the library's functions as commands. In the future, an API server may be built from the library as well. Arlon adds CRDs (custom resource definitions) for several custom resources such as ClusterRegistration and Profile. Management cluster \u00b6 The management cluster is a Kubernetes cluster hosting all the components needed by Arlon, including: The ArgoCD server The Arlon \"database\" (implemented as Kubernetes secrets and configmaps) The Arlon controller Cluster management API providers: Cluster API or Crossplane Custom resources (CRs) that drive the involved providers and controllers Custom resource definitions (CRDs) for all the involved CRs The user is responsible for supplying the management cluster, and to have access to a kubeconfig granting administrator permissions on the cluster. Controller \u00b6 The Arlon controller observes and responds to changes in clusterregistration custom resources. The Arlon library creates a clusterregistration at the beginning of workload cluster creation, causing the controller to wait for the cluster's kubeconfig to become available, at which point it registers the cluster with ArgoCD to enable manifests described by bundles to be deployed to the cluster. Library \u00b6 The Arlon library is a Go module that contains the functions that communicate with the Management Cluster to manipulate the Arlon state (bundles, profiles, clusterspecs) and transforms them into git directory structures to drive ArgoCD's gitops engine. Initially, the library is exposed via a CLI utility. In the future, it may also be embodied into a server an exposed via a network API. Workspace repository \u00b6 As mentioned earlier, Arlon creates and maintains directory structures in a git repository to drive ArgoCD sync operations. The user is responsible for supplying this workspace repository (and base paths) hosting those structures. Arlon relies on ArgoCD for repository registration, therefore the user should register the workspace registry in ArgoCD before referencing it from Arlon data types. Starting from release v0.9.0, Arlon now includes two commands to help with managing various git repository URLs. With these commands in place, the --repo-url flag in commands requiring a hosted git repository is no longer needed. A more detailed explanation is given in the next section . Repo Aliases \u00b6 A repo(repository) alias allows an Arlon user to register a GitHub repository with ArgoCD and store a local configuration file on their system that can be referenced by the CLI to then determine a repository URL and fetch its credentials when needed. All commands that require a repository, support a --repo-url flag also support a repo-alias flag to specify an alias instead of an alias, such commands will consider the \"default\" alias to be used when no --repo-alias and no --repo-url flags are given. There are two subcommands i.e., arlon git register and arlon git unregister which allow for a basic form of git repository context management. When arlon git register is run it requires a repo URL, the username, the access token and an optional alias(which defaults to \u201cdefault\u201d)- if a \u201cdefault\u201d alias already exists, the repo isn\u2019t registered with argocd and the alias creation fails saying that the default alias already exists otherwise, the repo is registered with argocd . Lastly we also write this repository information to the local configuration file. This contains two pieces of information for each repository- it\u2019s URL and the alias. The structure of the file is as shown: { \"default\" : { \"url\" : \"\" , \"alias\" : \"default\" }, \"repos\" : [ { \"url\" : \"\" , \"alias\" : \"default\" }, { \"url\" : \"\" , \"alias\" : \"not_default\" }, {} ] } On running arlon git unregister ALIAS , it removes that entry from the configuration file. However, it does NOT remove the repository from argocd . When the \"default\" alias is deleted, we also clear the \"default\" entry from the JSON file. Examples \u00b6 Given below are some examples for registering and unregistering a repository. Registering Repositories \u00b6 Registering a repository requires the repository link, the GitHub username( --user ), and a personal access token( --password ). When the --password flag isn't provided at the command line, the CLI will prompt for a password(this is the recommended approach). arlon git register https://github.com/GhUser/manifests --user GhUser arlon git register https://github.com/GhUser/prod-manifests --user GhUser --alias prod For non-interactive registrations, the --password flag can be used. export GH_PAT = \"...\" arlon git register https://github.com/GhUser/manifests --user GhUser --password $GH_PAT arlon git register https://github.com/GhUser/prod-manifests --user GhUser --alias prod --password $GH_PAT Unregistering Repositories \u00b6 Unregistering an alias only requires a positional argument: the repository alias. # unregister the default alias locally arlon git unregister default # unregister some other alias locally arlon git unregister prod","title":"Architecture"},{"location":"architecture/#architecture","text":"Arlon is composed of a controller, a library, and a CLI that exposes the library's functions as commands. In the future, an API server may be built from the library as well. Arlon adds CRDs (custom resource definitions) for several custom resources such as ClusterRegistration and Profile.","title":"Architecture"},{"location":"architecture/#management-cluster","text":"The management cluster is a Kubernetes cluster hosting all the components needed by Arlon, including: The ArgoCD server The Arlon \"database\" (implemented as Kubernetes secrets and configmaps) The Arlon controller Cluster management API providers: Cluster API or Crossplane Custom resources (CRs) that drive the involved providers and controllers Custom resource definitions (CRDs) for all the involved CRs The user is responsible for supplying the management cluster, and to have access to a kubeconfig granting administrator permissions on the cluster.","title":"Management cluster"},{"location":"architecture/#controller","text":"The Arlon controller observes and responds to changes in clusterregistration custom resources. The Arlon library creates a clusterregistration at the beginning of workload cluster creation, causing the controller to wait for the cluster's kubeconfig to become available, at which point it registers the cluster with ArgoCD to enable manifests described by bundles to be deployed to the cluster.","title":"Controller"},{"location":"architecture/#library","text":"The Arlon library is a Go module that contains the functions that communicate with the Management Cluster to manipulate the Arlon state (bundles, profiles, clusterspecs) and transforms them into git directory structures to drive ArgoCD's gitops engine. Initially, the library is exposed via a CLI utility. In the future, it may also be embodied into a server an exposed via a network API.","title":"Library"},{"location":"architecture/#workspace-repository","text":"As mentioned earlier, Arlon creates and maintains directory structures in a git repository to drive ArgoCD sync operations. The user is responsible for supplying this workspace repository (and base paths) hosting those structures. Arlon relies on ArgoCD for repository registration, therefore the user should register the workspace registry in ArgoCD before referencing it from Arlon data types. Starting from release v0.9.0, Arlon now includes two commands to help with managing various git repository URLs. With these commands in place, the --repo-url flag in commands requiring a hosted git repository is no longer needed. A more detailed explanation is given in the next section .","title":"Workspace repository"},{"location":"architecture/#repo-aliases","text":"A repo(repository) alias allows an Arlon user to register a GitHub repository with ArgoCD and store a local configuration file on their system that can be referenced by the CLI to then determine a repository URL and fetch its credentials when needed. All commands that require a repository, support a --repo-url flag also support a repo-alias flag to specify an alias instead of an alias, such commands will consider the \"default\" alias to be used when no --repo-alias and no --repo-url flags are given. There are two subcommands i.e., arlon git register and arlon git unregister which allow for a basic form of git repository context management. When arlon git register is run it requires a repo URL, the username, the access token and an optional alias(which defaults to \u201cdefault\u201d)- if a \u201cdefault\u201d alias already exists, the repo isn\u2019t registered with argocd and the alias creation fails saying that the default alias already exists otherwise, the repo is registered with argocd . Lastly we also write this repository information to the local configuration file. This contains two pieces of information for each repository- it\u2019s URL and the alias. The structure of the file is as shown: { \"default\" : { \"url\" : \"\" , \"alias\" : \"default\" }, \"repos\" : [ { \"url\" : \"\" , \"alias\" : \"default\" }, { \"url\" : \"\" , \"alias\" : \"not_default\" }, {} ] } On running arlon git unregister ALIAS , it removes that entry from the configuration file. However, it does NOT remove the repository from argocd . When the \"default\" alias is deleted, we also clear the \"default\" entry from the JSON file.","title":"Repo Aliases"},{"location":"architecture/#examples","text":"Given below are some examples for registering and unregistering a repository.","title":"Examples"},{"location":"architecture/#registering-repositories","text":"Registering a repository requires the repository link, the GitHub username( --user ), and a personal access token( --password ). When the --password flag isn't provided at the command line, the CLI will prompt for a password(this is the recommended approach). arlon git register https://github.com/GhUser/manifests --user GhUser arlon git register https://github.com/GhUser/prod-manifests --user GhUser --alias prod For non-interactive registrations, the --password flag can be used. export GH_PAT = \"...\" arlon git register https://github.com/GhUser/manifests --user GhUser --password $GH_PAT arlon git register https://github.com/GhUser/prod-manifests --user GhUser --alias prod --password $GH_PAT","title":"Registering Repositories"},{"location":"architecture/#unregistering-repositories","text":"Unregistering an alias only requires a positional argument: the repository alias. # unregister the default alias locally arlon git unregister default # unregister some other alias locally arlon git unregister prod","title":"Unregistering Repositories"},{"location":"clustertemplate/","text":"Next-gen Cluster Provisioning using Cluster Manifest \u00b6 This proposal describes a new way of provisioning workload clusters in Arlon. The most significant change is the Cluster Manifest construct, which replaces the current ClusterSpec. To distinguish them from current generation clusters, the ones deployed from a cluster manifest are called next-gen clusters. Goals \u00b6 Allow users to deploy arbitrarily complex clusters using the full Cluster API feature set. Fully declarative and gitops compatible: a cluster deployment should be composed of one or more self-sufficient manifests that the user can choose to either apply directly (via kubectl) or store in git for later-stage deployment by a gitops tool (mainly ArgoCD). Support Linked Mode update: an update to the the cluster manifest should automatically propagate to all workload clusters deployed from it. Profile support \u00b6 While profiles are also being re-architected, the first implementation of next-gen clusters fully integrates with current-generation profiles, which are expressed as Profile custom resources and compiled into a set of intermediate files in a git repository. Profiles are optional, and a next-gen cluster can be created without a profile. One can be attached later. Architecture diagram \u00b6 This example shows a cluster manifest named capi-quickstart used to deploy two workload clusters cluster-a and cluster-b . Additionally, cluster-a is given profile xxx , while cluster-b is given profile yyy . Cluster Manifest \u00b6 A cluster manifest serves as a base for creating new workload clusters. The workload clusters are all exact copies of the cluster manifest, meaning that they acquire all unmodified resources of the cluster manifest, except for: resource names, which are prefixed during the cluster creation process to make them unique to avoid conflicts the namespace, which is set to a new namespace unique to the workload cluster Preparation \u00b6 To create a cluster manifest, a user first creates a single YAML file containing the desired Cluster API cluster and all related resources (e.g. MachineDeployments, etc...), using whatever tool the user chooses (e.g. clusterctl generate cluster ). The user is responsible for the correctness of the file and resources within. Arlon will not check for errors. For example, the specified Kubernetes version must be supported by the Cluster API providers currently installed in the management cluster. If it isn't, resulting clusters will fail and enter a perpetual OutOfSync state. The user then commits and pushes the manifest file to a dedicated directory in a git repository. The name of the cluster resource does not matter, it will be used as a suffix during workload cluster creation. The directory should be unique to the file, and not contain any other files. If not already registered, the git repository should also be registered in ArgoCD with the proper credentials for read/write access. To check whether the git directory is a compliant Arlon cluster manifest, the user runs: arlon basecluster validategit --repo-url <repoUrl> --repo-path <pathToDirectory> [ --repo-revision revision ] Note: if --repo-revision is not specified, it defaults to main. The command produces an error the first time because the git directory has not yet been \"prepped\". To \"prep\" the directory to become a compliant Arlon cluster manifest, the user runs: arlon basecluster preparegit --repo-url <repoUrl> --repo-path <pathToDirectory> [ --repo-revision revision ] This pushes a commit to the repo with these changes: A kustomization.yaml file is added to the directory to make the manifest customizable by Kustomize. A configurations.yaml file is added to configure the namereference Kustomize plugin which ensures reference fields are correctly set when pointing to resource names that ArgoCD will modify using the Kustomize nameprefix mechanism. The content of the file is sourced from this Scott Lowe blog article . All namespace properties in the cluster manifest are removed to allow Kustomize to override the namespace of all resources. If prep is successful, another invocation of arlon basecluster validategit should succeed as well. Workload clusters \u00b6 Creation \u00b6 Use arlon cluster create to create a next-gen workload cluster from a cluster manifest ( this is different from arlon cluster deploy for creating current-generation clusters ). The command creates between 2 and 3 (depending on whether a profile is used) ArgoCD application resources that together make up the cluster and its contents. The general usage is: arlon cluster create --cluster-name <clusterName> --repo-url <repoUrl> --repo-path <pathToDirectory> [ --output-yaml ] [ --profile <profileName> ] [ --repo-revision <repoRevision> ] The command supports two modes of operation: With --output-yaml : output a list of YAML resources that you can inspect, save to a file, or pipe to kubectl apply -f Without --output-yaml : create the application resources directly in the management cluster currently referenced by your KUBECONFIG and context. The --profile flag is optional; a cluster can be created with no profile. Composition \u00b6 A workload cluster is composed of 2 to 3 ArgoCD application resources, which are named based on the name of the cluster manifest and the workload cluster. For illustration purposes, the following discussion assumes that the cluster manifest is named capi-quickstart , the workload cluster is named cluster-a , and the optional profile is named xxx . Cluster app \u00b6 The cluster-a application is the cluster app for the cluster. It is responsible for deploying the cluster manifest resources, meaning the Cluster API manifests. It is named directly from the workload cluster name. The application's spec uses a ApplicationSourceKustomize that points to the cluster manifest's git directory. The spec ensures that all deployed resources are configured to: Reside in the cluster-a namespace, which is deployed by the arlon app (see below). This achieved by setting app.Spec.Destination.Namespace to the workload cluster's name ( this only works if the resources do not specify an explicit namespace; this requirement is taken care of by the \"prep\" step on the cluster manifest ). Be named cluster-a-capi-quickstart , meaning the workload cluster name followed by the cluster manifest name. This is achieved by setting app.Spec.Source.Kustomize.NamePrefix to the workload cluster name plus a hyphen. Arlon app \u00b6 The cluster-a-arlon application is the arlon app for the cluster. It is resposible for deploying: The cluster-a namespace, which holds most resources related to this workload cluster, such as the Cluster API manifests deployed by the cluster app. Resources required to register the workload cluster with argocd when available: ClusterRegistration and associated RBAC rules. Additional resources (service account, more RBAC rules) for Cluster Autoscaler if enabled. The application spec's ApplicationSource points to the existing Arlon Helm chart located here by default: Repo: https://github.com/arlonproj/arlon.git Revision: private/leb/gen2 (IMPORTANT: NEEDS TO CHANGE TO STABLE BRANCH OR TAG) Path: pkg/cluster/manifests This is the same Helm chart that current-generation clusters are deployed from, using arlon cluster deploy . When used for the arlon app for a next-gen cluster, the Helm parameters are configured to only deploy the Arlon resources, with the subchart for cluster resources disabled, since those resources will be deployed by the cluster app. Important issue : as described above, the application source resides in the public Arlon repo. To avoid breaking user's deployed clusters, the source must be stable and not change! This probably means a particular Arlon release should point the source to a stable tag (not even a branch?) As an alternative, during Arlon setup, allow the user to copy the Helm chart into a private repo, and point the source there. Profile app (optional) \u00b6 A next-gen cluster can be assigned a current-gen dynamic profile, in which case Arlon creates a profile app named <clusterName>-profile-<profileName> , or cluster-a-profile-xxx in the running example. This is similar to the profile app created when attaching a profile app to an external cluster. The application source points to the git location of the dynamic profile. Teardown \u00b6 Since a next-gen cluster is composed of multiple ArgoCD applications, destroying the cluster requires deleting all of its applications. To facilitate this, the 2 or 3 applications created by arlon cluster create are automatically labeled with arlon-cluster=<clusterName> . The user has two options for destroying a next-gen cluster: The easiest way: arlon cluster delete <clusterName> . This command automatically detects a next-gen cluster and cleans up all related applications. A more manual way: kubectl delete application -l arlon-cluster=<clusterName> Update Semantics \u00b6 A cluster manifest lives in git and is shared by all workload clusters created from it. This is sometimes referred to as Linked Mode . Any git update to the cluster can affect the associated workload clusters, therefore such updates must be planned and managed with care; there is a real risk of such an update breaking existing clusters. By default, a workload's cluster cluster app is configured with auto-sync, meaning ArgoCD will immediately apply any changes in the cluster manifest to the deployed Cluster API cluster resources. In general, a cluster manifest does not need to be \"prepped\" again after a modification to its main manifest file (the one containing the Cluster API resources). So the user is free to edit the manifest directly, commit/push the changes, and expect to see immediate changes to already-deployed clusters created from that cluster manifest. Unsupported changes \u00b6 The controllers for Cluster API and its providers disallow changes to some fields belonging to already-deployed resources. For example, changing the cluster manifest name ( medata.Name of the Cluster resource) will have disastrous consequences on already-deployed clusters, causing many resources to enter the OutOfSync state and never recover because ArgoCD fails to apply the changes (they are rejected by the controllers). Consequently, a user should never change the name of a cluster manifest. Besides the cluster name, other fields cannot change (this has been observed anecdotally, we don't yet have an exhaustive list). Changing the Kubernetes version of the control plane or data plane is supported, so long as the new version is supported by the relevant providers. If accepted, such a change will result in a rolling update of the corresponding plane. Specific to AWS: the AWSMachineTemplate.spec is immutable and a CAPI webhook disallows such updates. The user is advised to not make such modifications to a basecluster manifest. In the event that such an event does happen, the user is advised to not manually sync in those changes via argocd . If a new cluster with a different AWSMachineTemplate.spec is desired, the recommended approach is to make a copy of the manifests in the workspace repository and then issue an arlon cluster create command which would then consume this manifest.","title":"Cluster template"},{"location":"clustertemplate/#next-gen-cluster-provisioning-using-cluster-manifest","text":"This proposal describes a new way of provisioning workload clusters in Arlon. The most significant change is the Cluster Manifest construct, which replaces the current ClusterSpec. To distinguish them from current generation clusters, the ones deployed from a cluster manifest are called next-gen clusters.","title":"Next-gen Cluster Provisioning using Cluster Manifest"},{"location":"clustertemplate/#goals","text":"Allow users to deploy arbitrarily complex clusters using the full Cluster API feature set. Fully declarative and gitops compatible: a cluster deployment should be composed of one or more self-sufficient manifests that the user can choose to either apply directly (via kubectl) or store in git for later-stage deployment by a gitops tool (mainly ArgoCD). Support Linked Mode update: an update to the the cluster manifest should automatically propagate to all workload clusters deployed from it.","title":"Goals"},{"location":"clustertemplate/#profile-support","text":"While profiles are also being re-architected, the first implementation of next-gen clusters fully integrates with current-generation profiles, which are expressed as Profile custom resources and compiled into a set of intermediate files in a git repository. Profiles are optional, and a next-gen cluster can be created without a profile. One can be attached later.","title":"Profile support"},{"location":"clustertemplate/#architecture-diagram","text":"This example shows a cluster manifest named capi-quickstart used to deploy two workload clusters cluster-a and cluster-b . Additionally, cluster-a is given profile xxx , while cluster-b is given profile yyy .","title":"Architecture diagram"},{"location":"clustertemplate/#cluster-manifest","text":"A cluster manifest serves as a base for creating new workload clusters. The workload clusters are all exact copies of the cluster manifest, meaning that they acquire all unmodified resources of the cluster manifest, except for: resource names, which are prefixed during the cluster creation process to make them unique to avoid conflicts the namespace, which is set to a new namespace unique to the workload cluster","title":"Cluster Manifest"},{"location":"clustertemplate/#preparation","text":"To create a cluster manifest, a user first creates a single YAML file containing the desired Cluster API cluster and all related resources (e.g. MachineDeployments, etc...), using whatever tool the user chooses (e.g. clusterctl generate cluster ). The user is responsible for the correctness of the file and resources within. Arlon will not check for errors. For example, the specified Kubernetes version must be supported by the Cluster API providers currently installed in the management cluster. If it isn't, resulting clusters will fail and enter a perpetual OutOfSync state. The user then commits and pushes the manifest file to a dedicated directory in a git repository. The name of the cluster resource does not matter, it will be used as a suffix during workload cluster creation. The directory should be unique to the file, and not contain any other files. If not already registered, the git repository should also be registered in ArgoCD with the proper credentials for read/write access. To check whether the git directory is a compliant Arlon cluster manifest, the user runs: arlon basecluster validategit --repo-url <repoUrl> --repo-path <pathToDirectory> [ --repo-revision revision ] Note: if --repo-revision is not specified, it defaults to main. The command produces an error the first time because the git directory has not yet been \"prepped\". To \"prep\" the directory to become a compliant Arlon cluster manifest, the user runs: arlon basecluster preparegit --repo-url <repoUrl> --repo-path <pathToDirectory> [ --repo-revision revision ] This pushes a commit to the repo with these changes: A kustomization.yaml file is added to the directory to make the manifest customizable by Kustomize. A configurations.yaml file is added to configure the namereference Kustomize plugin which ensures reference fields are correctly set when pointing to resource names that ArgoCD will modify using the Kustomize nameprefix mechanism. The content of the file is sourced from this Scott Lowe blog article . All namespace properties in the cluster manifest are removed to allow Kustomize to override the namespace of all resources. If prep is successful, another invocation of arlon basecluster validategit should succeed as well.","title":"Preparation"},{"location":"clustertemplate/#workload-clusters","text":"","title":"Workload clusters"},{"location":"clustertemplate/#creation","text":"Use arlon cluster create to create a next-gen workload cluster from a cluster manifest ( this is different from arlon cluster deploy for creating current-generation clusters ). The command creates between 2 and 3 (depending on whether a profile is used) ArgoCD application resources that together make up the cluster and its contents. The general usage is: arlon cluster create --cluster-name <clusterName> --repo-url <repoUrl> --repo-path <pathToDirectory> [ --output-yaml ] [ --profile <profileName> ] [ --repo-revision <repoRevision> ] The command supports two modes of operation: With --output-yaml : output a list of YAML resources that you can inspect, save to a file, or pipe to kubectl apply -f Without --output-yaml : create the application resources directly in the management cluster currently referenced by your KUBECONFIG and context. The --profile flag is optional; a cluster can be created with no profile.","title":"Creation"},{"location":"clustertemplate/#composition","text":"A workload cluster is composed of 2 to 3 ArgoCD application resources, which are named based on the name of the cluster manifest and the workload cluster. For illustration purposes, the following discussion assumes that the cluster manifest is named capi-quickstart , the workload cluster is named cluster-a , and the optional profile is named xxx .","title":"Composition"},{"location":"clustertemplate/#cluster-app","text":"The cluster-a application is the cluster app for the cluster. It is responsible for deploying the cluster manifest resources, meaning the Cluster API manifests. It is named directly from the workload cluster name. The application's spec uses a ApplicationSourceKustomize that points to the cluster manifest's git directory. The spec ensures that all deployed resources are configured to: Reside in the cluster-a namespace, which is deployed by the arlon app (see below). This achieved by setting app.Spec.Destination.Namespace to the workload cluster's name ( this only works if the resources do not specify an explicit namespace; this requirement is taken care of by the \"prep\" step on the cluster manifest ). Be named cluster-a-capi-quickstart , meaning the workload cluster name followed by the cluster manifest name. This is achieved by setting app.Spec.Source.Kustomize.NamePrefix to the workload cluster name plus a hyphen.","title":"Cluster app"},{"location":"clustertemplate/#arlon-app","text":"The cluster-a-arlon application is the arlon app for the cluster. It is resposible for deploying: The cluster-a namespace, which holds most resources related to this workload cluster, such as the Cluster API manifests deployed by the cluster app. Resources required to register the workload cluster with argocd when available: ClusterRegistration and associated RBAC rules. Additional resources (service account, more RBAC rules) for Cluster Autoscaler if enabled. The application spec's ApplicationSource points to the existing Arlon Helm chart located here by default: Repo: https://github.com/arlonproj/arlon.git Revision: private/leb/gen2 (IMPORTANT: NEEDS TO CHANGE TO STABLE BRANCH OR TAG) Path: pkg/cluster/manifests This is the same Helm chart that current-generation clusters are deployed from, using arlon cluster deploy . When used for the arlon app for a next-gen cluster, the Helm parameters are configured to only deploy the Arlon resources, with the subchart for cluster resources disabled, since those resources will be deployed by the cluster app. Important issue : as described above, the application source resides in the public Arlon repo. To avoid breaking user's deployed clusters, the source must be stable and not change! This probably means a particular Arlon release should point the source to a stable tag (not even a branch?) As an alternative, during Arlon setup, allow the user to copy the Helm chart into a private repo, and point the source there.","title":"Arlon app"},{"location":"clustertemplate/#profile-app-optional","text":"A next-gen cluster can be assigned a current-gen dynamic profile, in which case Arlon creates a profile app named <clusterName>-profile-<profileName> , or cluster-a-profile-xxx in the running example. This is similar to the profile app created when attaching a profile app to an external cluster. The application source points to the git location of the dynamic profile.","title":"Profile app (optional)"},{"location":"clustertemplate/#teardown","text":"Since a next-gen cluster is composed of multiple ArgoCD applications, destroying the cluster requires deleting all of its applications. To facilitate this, the 2 or 3 applications created by arlon cluster create are automatically labeled with arlon-cluster=<clusterName> . The user has two options for destroying a next-gen cluster: The easiest way: arlon cluster delete <clusterName> . This command automatically detects a next-gen cluster and cleans up all related applications. A more manual way: kubectl delete application -l arlon-cluster=<clusterName>","title":"Teardown"},{"location":"clustertemplate/#update-semantics","text":"A cluster manifest lives in git and is shared by all workload clusters created from it. This is sometimes referred to as Linked Mode . Any git update to the cluster can affect the associated workload clusters, therefore such updates must be planned and managed with care; there is a real risk of such an update breaking existing clusters. By default, a workload's cluster cluster app is configured with auto-sync, meaning ArgoCD will immediately apply any changes in the cluster manifest to the deployed Cluster API cluster resources. In general, a cluster manifest does not need to be \"prepped\" again after a modification to its main manifest file (the one containing the Cluster API resources). So the user is free to edit the manifest directly, commit/push the changes, and expect to see immediate changes to already-deployed clusters created from that cluster manifest.","title":"Update Semantics"},{"location":"clustertemplate/#unsupported-changes","text":"The controllers for Cluster API and its providers disallow changes to some fields belonging to already-deployed resources. For example, changing the cluster manifest name ( medata.Name of the Cluster resource) will have disastrous consequences on already-deployed clusters, causing many resources to enter the OutOfSync state and never recover because ArgoCD fails to apply the changes (they are rejected by the controllers). Consequently, a user should never change the name of a cluster manifest. Besides the cluster name, other fields cannot change (this has been observed anecdotally, we don't yet have an exhaustive list). Changing the Kubernetes version of the control plane or data plane is supported, so long as the new version is supported by the relevant providers. If accepted, such a change will result in a rolling update of the corresponding plane. Specific to AWS: the AWSMachineTemplate.spec is immutable and a CAPI webhook disallows such updates. The user is advised to not make such modifications to a basecluster manifest. In the event that such an event does happen, the user is advised to not manually sync in those changes via argocd . If a new cluster with a different AWSMachineTemplate.spec is desired, the recommended approach is to make a copy of the manifests in the workspace repository and then issue an arlon cluster create command which would then consume this manifest.","title":"Unsupported changes"},{"location":"concepts/","text":"Concepts \u00b6 Management cluster \u00b6 This Kubernetes cluster hosts the following components: ArgoCD Arlon Cluster management stacks e.g. Cluster API and/or Crossplane The Arlon state and controllers reside in the arlon namespace. Configuration bundle \u00b6 A configuration bundle (or just \"bundle\") is grouping of data files that produce a set of Kubernetes manifests via a tool . This closely follows ArgoCD's definition of tool types . Consequently, the list of supported bundle types mirrors ArgoCD's supported set of manifest-producing tools. Each bundle is defined using a Kubernetes ConfigMap resource in the arlon namespace. Static bundle \u00b6 A static bundle embeds the manifest's YAML data itself (\"static bundle\"). A cluster consuming a static bundle will always have a snapshot copy of the bundle at the time the cluster was created, and is not affected by subsequent changes to the bundle's manifest data. Dynamic bundle \u00b6 A dynamic bundle contains a reference to the manifest data stored in git. A dynamic bundle is distinguished by having these fields set to non-empty values: git URL of the repo Directory path within the repo The git URL must be registered in ArgoCD as a valid repository. The content of the specified directory can contain manifests in any of the tool formats supported by ArgoCD, including plain YAML, Helm and Kustomize. When the user updates a dynamic bundle in git, all clusters consuming that bundle (through a profile specified at cluster creation time) will acquire the change. Other properties \u00b6 A bundle can also have a comma-separated list of tags, and a description. Tags can be useful for classifying bundles, for e.g. by type (\"addon\", \"cni\", \"rbac\", \"app\"). Profile \u00b6 A profile expresses a desired configuration for a Kubernetes cluster. It is just a set of references to bundles (static, dynamic, or a combination). A profile can be static or dynamic. Static profile \u00b6 When a cluster consumes a static profile at creation time, the set of bundles for the cluster is fixed at that time and does not change over time even when the static bundle is updated. (Note: the contents of some of those bundles referenced by the static profile may however change over time if they are dynamic). A static profile is stored as an item in the Arlon database (specifically, as a CR in the Management Cluster). Dynamic profile \u00b6 A dynamic profile, on the other hand, has two components: the specification stored in the Arlon database, and a compiled component living in the workspace repository at a path specified by the user. (Note: this repository is usually the workspace repo, but it technically doesn't have to be, as long as it's a valid repo registered in ArgoCD) The compiled component is essentially a Helm chart of multiple ArgoCD app resources, each one pointing to a bundle. Arlon automatically creates and maintains the compiled component. When a user updates the composition of a dynamic profile, meaning redefines its bundle set, the Arlon library updates the compiled component to point to the bundles specified in the new set. Any cluster consuming that dynamic profile will be affected by the change, meaning it may lose or acquire new bundles in real time. Cluster \u00b6 An Arlon cluster, also known as workload cluster, is a Kubernetes cluster that Arlon creates and manages via a git directory structure stored in the workspace repository. (Under construction) Cluster spec \u00b6 A cluster spec contains desired settings when creating a new cluster. They currently include: API Provider: the cluster orchestration technology. Supported values are CAPI (Cluster API) and xplane (Crossplane) Cloud Provider: the infrastructure cloud provider. The currently supported values is aws , with gcp and azure support coming later. Type: the cluster type. Some API providers support more than one type. On aws cloud, Cluster API supports kubeadm and eks , whereas Crossplane only supports eks . The (worker) node instance type The initial (worker) node count The Kubernetes version Cluster Template \u00b6 To know more about basecluster (Arlon gen2 clusters), read it here","title":"Concepts"},{"location":"concepts/#concepts","text":"","title":"Concepts"},{"location":"concepts/#management-cluster","text":"This Kubernetes cluster hosts the following components: ArgoCD Arlon Cluster management stacks e.g. Cluster API and/or Crossplane The Arlon state and controllers reside in the arlon namespace.","title":"Management cluster"},{"location":"concepts/#configuration-bundle","text":"A configuration bundle (or just \"bundle\") is grouping of data files that produce a set of Kubernetes manifests via a tool . This closely follows ArgoCD's definition of tool types . Consequently, the list of supported bundle types mirrors ArgoCD's supported set of manifest-producing tools. Each bundle is defined using a Kubernetes ConfigMap resource in the arlon namespace.","title":"Configuration bundle"},{"location":"concepts/#static-bundle","text":"A static bundle embeds the manifest's YAML data itself (\"static bundle\"). A cluster consuming a static bundle will always have a snapshot copy of the bundle at the time the cluster was created, and is not affected by subsequent changes to the bundle's manifest data.","title":"Static bundle"},{"location":"concepts/#dynamic-bundle","text":"A dynamic bundle contains a reference to the manifest data stored in git. A dynamic bundle is distinguished by having these fields set to non-empty values: git URL of the repo Directory path within the repo The git URL must be registered in ArgoCD as a valid repository. The content of the specified directory can contain manifests in any of the tool formats supported by ArgoCD, including plain YAML, Helm and Kustomize. When the user updates a dynamic bundle in git, all clusters consuming that bundle (through a profile specified at cluster creation time) will acquire the change.","title":"Dynamic bundle"},{"location":"concepts/#other-properties","text":"A bundle can also have a comma-separated list of tags, and a description. Tags can be useful for classifying bundles, for e.g. by type (\"addon\", \"cni\", \"rbac\", \"app\").","title":"Other properties"},{"location":"concepts/#profile","text":"A profile expresses a desired configuration for a Kubernetes cluster. It is just a set of references to bundles (static, dynamic, or a combination). A profile can be static or dynamic.","title":"Profile"},{"location":"concepts/#static-profile","text":"When a cluster consumes a static profile at creation time, the set of bundles for the cluster is fixed at that time and does not change over time even when the static bundle is updated. (Note: the contents of some of those bundles referenced by the static profile may however change over time if they are dynamic). A static profile is stored as an item in the Arlon database (specifically, as a CR in the Management Cluster).","title":"Static profile"},{"location":"concepts/#dynamic-profile","text":"A dynamic profile, on the other hand, has two components: the specification stored in the Arlon database, and a compiled component living in the workspace repository at a path specified by the user. (Note: this repository is usually the workspace repo, but it technically doesn't have to be, as long as it's a valid repo registered in ArgoCD) The compiled component is essentially a Helm chart of multiple ArgoCD app resources, each one pointing to a bundle. Arlon automatically creates and maintains the compiled component. When a user updates the composition of a dynamic profile, meaning redefines its bundle set, the Arlon library updates the compiled component to point to the bundles specified in the new set. Any cluster consuming that dynamic profile will be affected by the change, meaning it may lose or acquire new bundles in real time.","title":"Dynamic profile"},{"location":"concepts/#cluster","text":"An Arlon cluster, also known as workload cluster, is a Kubernetes cluster that Arlon creates and manages via a git directory structure stored in the workspace repository. (Under construction)","title":"Cluster"},{"location":"concepts/#cluster-spec","text":"A cluster spec contains desired settings when creating a new cluster. They currently include: API Provider: the cluster orchestration technology. Supported values are CAPI (Cluster API) and xplane (Crossplane) Cloud Provider: the infrastructure cloud provider. The currently supported values is aws , with gcp and azure support coming later. Type: the cluster type. Some API providers support more than one type. On aws cloud, Cluster API supports kubeadm and eks , whereas Crossplane only supports eks . The (worker) node instance type The initial (worker) node count The Kubernetes version","title":"Cluster spec"},{"location":"concepts/#cluster-template","text":"To know more about basecluster (Arlon gen2 clusters), read it here","title":"Cluster Template"},{"location":"contributing/","text":"How to contribute to Arlon \u00b6 Team Arlon welcomes and encourages everyone to participate in its development via pull requests on GitHub. We prefer to take in pull requests to our active development branch i.e. the main branch. To report a bug or request a feature, we rely on GitHub issues. There are a number of points to keep in mind when submitting a feature request, reporting a bug or contributing in the development of Arlon. Before making a feature request, or reporting a bug please browse through the existing open issues to be sure that it hasn't been already tracked. If a feature request is subsumed by some other open issue, please add your valuable feedback as a comment to the issue. If a bug discovered by you is already being tracked, please provide additional information as you see fit(steps to reproduce, particulars of the environment, version information etc.) as a comment. Before submitting code for a new feature(or a complex, untracked bugfix) please create a new issue. This issue needs to undergo a review process which may involve a discussion on the same GitHub issue to discuss possible approaches and motivation for the said proposal. Please reach out to us on Slack for discussions, help, questions and the roadmap. Code changes \u00b6 Open a pull request (PR) on GitHub following the typical GitHub workflow here . Most of the new code changes are merged to the main branch except backports, bookkeeping changes, library upgrades and some bugs that manifest only a particular version. Before contributing new code, contributors are encouraged to either write unit tests, e2e tests or perform some form of manual validation as a sanity-check. Please adhere to standard good practices for Golang and do ensure that the code is properly formatted and vet succeeds, for which we have fmt and vet targets respectively. Issues / Bug reports \u00b6 We track issues on GitHub . You are encouraged to browse through these, add relevant feedback, create new issues or participate in the development. If you are interested in a particular issue or feature request, please leave a comment to reach out to the team. In particular, the issues labeled as help wanted are a great starting point for adding code changes to the project. Documentation \u00b6 The documentation for Arlon is hosted on Read the Docs and comprises of contents from the \"docs\" directory of Arlon source. For making changes to the documentation, please follow the below steps: Fork the Arlon repository on GitHub and make the desired changes. Prerequisites Ensure that python3 , pip3 is installed. Optionally, create a venv by running python3 -m venv ./venv to create a virtual environment if you don't have one. From the root of the Arlon repository, run pip3 install -r docs/requirements.txt to install mkdocs and other pre-requisites. To test your local changes, run mkdocs serve from the repository root. This starts a local server to host the documentation website where you can preview the changes. To publish the changes, just push the changes to your fork repository and open a PR (pull request). Once your PR is accepted by one of the maintainers/ owners of Arlon project, the Arlon website will be updated.","title":"Contributing"},{"location":"contributing/#how-to-contribute-to-arlon","text":"Team Arlon welcomes and encourages everyone to participate in its development via pull requests on GitHub. We prefer to take in pull requests to our active development branch i.e. the main branch. To report a bug or request a feature, we rely on GitHub issues. There are a number of points to keep in mind when submitting a feature request, reporting a bug or contributing in the development of Arlon. Before making a feature request, or reporting a bug please browse through the existing open issues to be sure that it hasn't been already tracked. If a feature request is subsumed by some other open issue, please add your valuable feedback as a comment to the issue. If a bug discovered by you is already being tracked, please provide additional information as you see fit(steps to reproduce, particulars of the environment, version information etc.) as a comment. Before submitting code for a new feature(or a complex, untracked bugfix) please create a new issue. This issue needs to undergo a review process which may involve a discussion on the same GitHub issue to discuss possible approaches and motivation for the said proposal. Please reach out to us on Slack for discussions, help, questions and the roadmap.","title":"How to contribute to Arlon"},{"location":"contributing/#code-changes","text":"Open a pull request (PR) on GitHub following the typical GitHub workflow here . Most of the new code changes are merged to the main branch except backports, bookkeeping changes, library upgrades and some bugs that manifest only a particular version. Before contributing new code, contributors are encouraged to either write unit tests, e2e tests or perform some form of manual validation as a sanity-check. Please adhere to standard good practices for Golang and do ensure that the code is properly formatted and vet succeeds, for which we have fmt and vet targets respectively.","title":"Code changes"},{"location":"contributing/#issues-bug-reports","text":"We track issues on GitHub . You are encouraged to browse through these, add relevant feedback, create new issues or participate in the development. If you are interested in a particular issue or feature request, please leave a comment to reach out to the team. In particular, the issues labeled as help wanted are a great starting point for adding code changes to the project.","title":"Issues / Bug reports"},{"location":"contributing/#documentation","text":"The documentation for Arlon is hosted on Read the Docs and comprises of contents from the \"docs\" directory of Arlon source. For making changes to the documentation, please follow the below steps: Fork the Arlon repository on GitHub and make the desired changes. Prerequisites Ensure that python3 , pip3 is installed. Optionally, create a venv by running python3 -m venv ./venv to create a virtual environment if you don't have one. From the root of the Arlon repository, run pip3 install -r docs/requirements.txt to install mkdocs and other pre-requisites. To test your local changes, run mkdocs serve from the repository root. This starts a local server to host the documentation website where you can preview the changes. To publish the changes, just push the changes to your fork repository and open a PR (pull request). Once your PR is accepted by one of the maintainers/ owners of Arlon project, the Arlon website will be updated.","title":"Documentation"},{"location":"design/","text":"Arlon Design and Concepts \u00b6 Management cluster \u00b6 This Kubernetes cluster hosts the following components: ArgoCD Arlon Cluster management stacks e.g. Cluster API and/or Crossplane The Arlon state and controllers reside in the arlon namespace. Configuration bundle \u00b6 A configuration bundle (or just \"bundle\") is grouping of data files that produce a set of Kubernetes manifests via a tool . This closely follows ArgoCD's definition of tool types . Consequently, the list of supported bundle types mirrors ArgoCD's supported set of manifest-producing tools. Each bundle is defined using a Kubernetes ConfigMap resource in the arlo namespace. Additionally, a bundle can embed the data itself (\"static bundle\"), or contain a reference to the data (\"dynamic bundle\"). A reference can be a URL, GitHub location, or Helm repo location. The current list of supported bundle types is: manifest_inline: a single manifest yaml file embedded in the resource manifest_ref: a reference to a single manifest yaml file dir_inline: an embedded tarball that expands to a directory of YAML files helm_inline: an embedded Helm chart package helm_ref: an external reference to a Helm chart Bundle purpose \u00b6 Bundles can specify an optional purpose to help classify and organize them. In the future, Arlon may order bundle installation by purpose order (for e.g. install bundles with purpose= networking before others) but that is not the case today. The currently suggested purpose values are: networking add-on data-service application Profile \u00b6 A profile expresses a desired configuration for a Kubernetes cluster. It is composed of An optional clusterspec. If specified, it allows the profile to be used to create new clusters. If absent, the profile can only be applied to existing clusters. A list of bundles specifying the configuration to apply onto the cluster once it is operational An optional list of values.yaml settings for any Helm Chart type bundle in the bundle list Cluster \u00b6 Cluster Specification/ Metadata \u00b6 A Cluster Specification contains desired settings when creating a new cluster. These settings are the values that define the shape and the configurations of the cluster. Currently, there is a difference in the cluster specification for gen1 and gen2 clusters. The main difference in these cluster specifications is that gen2 Cluster Specification allow users to deploy arbitrarily complex clusters using the full Cluster API feature set.This is also closer to the gitops and declarative style of cluster creation and gives users more control over the cluster that they deploy. gen1 \u00b6 A clusterspec contains desired settings when creating a new cluster. For gen1 clusters, this Cluster Specification is called ClusterSpec . Clusterspec currently includes: Stack: the cluster provisioning stack, for e.g. cluster-api or crossplane Provider: the specific cluster management provider under that stack, if applicable. Example: for cluster-api , the possible values are eks and kubeadm Other settings that specify the \"shape\" of the cluster, such as the size of the control plane and the initial number of nodes of the data plane. The pod networking technology (under discussion: this may be moved to a bundle because most if not all CNI providers can be installed as manifests) gen2 \u00b6 for gen2 clusters, the Cluster Specification is called the cluster template, which is described in detail here . A cluster template consists of: A predefined list of Cluster API objects: Cluster, Machines, Machine Deployments, etc. to be deployed in the current namespace The specific infrastructure provider to be used (e.g aws).\u00df Kubernetes version Cluster templates/ flavors that need to be used for creating the cluster template (e.g eks, eks-managedmachinepool) Cluster Preparation \u00b6 Once these cluster specifications are created successfully, the next step is to prepare the cluster for deployment. gen1 \u00b6 Once the clusterspec is created for a gen-1 cluster, there is no need to prepare a workspace repository to create a new cluster. gen2 \u00b6 Once the cluster manifest is created, the next step is to preare the workspace repository directory in which this cluster manifest is present. This is explained in detail here Cluster Creation \u00b6 Now, all the prerequisites for creating a cluster are completed and the cluster can be created/deployed. Cluster Chart \u00b6 The cluster chart is a Helm chart that creates (and optionally applies) the manifests necessary to create a cluster and deploy desired configurations and applications to it as a part of cluster creation, the following resources are created: The profile's Cluster Specification, bundle list and other settings are used to generate values for the cluster chart, and the chart is deployed as a Helm release into the arlon namespace in the management cluster. Here is a summary of the kinds of resources generated and deployed by the chart: A unique namespace with a name based on the cluster's name. All subsequent resources below are created inside that namespace. The stack-specific resources to create the cluster (for e.g. Cluster API resources) A ClusterRegistration to automatically register the cluster with ArgoCD A GitRepoDir to automatically create a git repo and/or directory to host a copy of the expanded bundles. Every bundle referenced by the profile is copied/unpacked into its own subdirectory. One ArgoCD Application resource for each bundle. gen1 \u00b6 Cluster deployment is explained here gen2 \u00b6 cluster template creation is explained here","title":"Design"},{"location":"design/#arlon-design-and-concepts","text":"","title":"Arlon Design and Concepts"},{"location":"design/#management-cluster","text":"This Kubernetes cluster hosts the following components: ArgoCD Arlon Cluster management stacks e.g. Cluster API and/or Crossplane The Arlon state and controllers reside in the arlon namespace.","title":"Management cluster"},{"location":"design/#configuration-bundle","text":"A configuration bundle (or just \"bundle\") is grouping of data files that produce a set of Kubernetes manifests via a tool . This closely follows ArgoCD's definition of tool types . Consequently, the list of supported bundle types mirrors ArgoCD's supported set of manifest-producing tools. Each bundle is defined using a Kubernetes ConfigMap resource in the arlo namespace. Additionally, a bundle can embed the data itself (\"static bundle\"), or contain a reference to the data (\"dynamic bundle\"). A reference can be a URL, GitHub location, or Helm repo location. The current list of supported bundle types is: manifest_inline: a single manifest yaml file embedded in the resource manifest_ref: a reference to a single manifest yaml file dir_inline: an embedded tarball that expands to a directory of YAML files helm_inline: an embedded Helm chart package helm_ref: an external reference to a Helm chart","title":"Configuration bundle"},{"location":"design/#bundle-purpose","text":"Bundles can specify an optional purpose to help classify and organize them. In the future, Arlon may order bundle installation by purpose order (for e.g. install bundles with purpose= networking before others) but that is not the case today. The currently suggested purpose values are: networking add-on data-service application","title":"Bundle purpose"},{"location":"design/#profile","text":"A profile expresses a desired configuration for a Kubernetes cluster. It is composed of An optional clusterspec. If specified, it allows the profile to be used to create new clusters. If absent, the profile can only be applied to existing clusters. A list of bundles specifying the configuration to apply onto the cluster once it is operational An optional list of values.yaml settings for any Helm Chart type bundle in the bundle list","title":"Profile"},{"location":"design/#cluster","text":"","title":"Cluster"},{"location":"design/#cluster-specification-metadata","text":"A Cluster Specification contains desired settings when creating a new cluster. These settings are the values that define the shape and the configurations of the cluster. Currently, there is a difference in the cluster specification for gen1 and gen2 clusters. The main difference in these cluster specifications is that gen2 Cluster Specification allow users to deploy arbitrarily complex clusters using the full Cluster API feature set.This is also closer to the gitops and declarative style of cluster creation and gives users more control over the cluster that they deploy.","title":"Cluster Specification/ Metadata"},{"location":"design/#gen1","text":"A clusterspec contains desired settings when creating a new cluster. For gen1 clusters, this Cluster Specification is called ClusterSpec . Clusterspec currently includes: Stack: the cluster provisioning stack, for e.g. cluster-api or crossplane Provider: the specific cluster management provider under that stack, if applicable. Example: for cluster-api , the possible values are eks and kubeadm Other settings that specify the \"shape\" of the cluster, such as the size of the control plane and the initial number of nodes of the data plane. The pod networking technology (under discussion: this may be moved to a bundle because most if not all CNI providers can be installed as manifests)","title":"gen1"},{"location":"design/#gen2","text":"for gen2 clusters, the Cluster Specification is called the cluster template, which is described in detail here . A cluster template consists of: A predefined list of Cluster API objects: Cluster, Machines, Machine Deployments, etc. to be deployed in the current namespace The specific infrastructure provider to be used (e.g aws).\u00df Kubernetes version Cluster templates/ flavors that need to be used for creating the cluster template (e.g eks, eks-managedmachinepool)","title":"gen2"},{"location":"design/#cluster-preparation","text":"Once these cluster specifications are created successfully, the next step is to prepare the cluster for deployment.","title":"Cluster Preparation"},{"location":"design/#gen1_1","text":"Once the clusterspec is created for a gen-1 cluster, there is no need to prepare a workspace repository to create a new cluster.","title":"gen1"},{"location":"design/#gen2_1","text":"Once the cluster manifest is created, the next step is to preare the workspace repository directory in which this cluster manifest is present. This is explained in detail here","title":"gen2"},{"location":"design/#cluster-creation","text":"Now, all the prerequisites for creating a cluster are completed and the cluster can be created/deployed.","title":"Cluster Creation"},{"location":"design/#cluster-chart","text":"The cluster chart is a Helm chart that creates (and optionally applies) the manifests necessary to create a cluster and deploy desired configurations and applications to it as a part of cluster creation, the following resources are created: The profile's Cluster Specification, bundle list and other settings are used to generate values for the cluster chart, and the chart is deployed as a Helm release into the arlon namespace in the management cluster. Here is a summary of the kinds of resources generated and deployed by the chart: A unique namespace with a name based on the cluster's name. All subsequent resources below are created inside that namespace. The stack-specific resources to create the cluster (for e.g. Cluster API resources) A ClusterRegistration to automatically register the cluster with ArgoCD A GitRepoDir to automatically create a git repo and/or directory to host a copy of the expanded bundles. Every bundle referenced by the profile is copied/unpacked into its own subdirectory. One ArgoCD Application resource for each bundle.","title":"Cluster Chart"},{"location":"design/#gen1_2","text":"Cluster deployment is explained here","title":"gen1"},{"location":"design/#gen2_2","text":"cluster template creation is explained here","title":"gen2"},{"location":"dev_setup/","text":"TODO (Under construction) \u00b6","title":"Setup"},{"location":"dev_setup/#todo-under-construction","text":"","title":"TODO (Under construction)"},{"location":"docs_help/","text":"Help with Arlon Documentation \u00b6 Prerequisites \u00b6 Documentation for Arlon is written in Markdown format, and generated using the mkdocs site generator. The theme mkdocs-material has been used, and docs from the main branch are published to a GitHub pages site. Install Python 3.x (3.8 or later recommended. 3.11 has been tested) Install the pip python package manager for Python 3. Install the Python modules mentioned in docs/requirements.txt Run mkdocs serve to test any local changes to docs, and commit them using git workflow Help with MkDocs \u00b6 Welcome to Arlon documentation with MkDocs For full documentation, visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout \u00b6 mkdocs.yml # The configuration file. # Contains the navigation structure docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Help with Documentation"},{"location":"docs_help/#help-with-arlon-documentation","text":"","title":"Help with Arlon Documentation"},{"location":"docs_help/#prerequisites","text":"Documentation for Arlon is written in Markdown format, and generated using the mkdocs site generator. The theme mkdocs-material has been used, and docs from the main branch are published to a GitHub pages site. Install Python 3.x (3.8 or later recommended. 3.11 has been tested) Install the pip python package manager for Python 3. Install the Python modules mentioned in docs/requirements.txt Run mkdocs serve to test any local changes to docs, and commit them using git workflow","title":"Prerequisites"},{"location":"docs_help/#help-with-mkdocs","text":"Welcome to Arlon documentation with MkDocs For full documentation, visit mkdocs.org .","title":"Help with MkDocs"},{"location":"docs_help/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"docs_help/#project-layout","text":"mkdocs.yml # The configuration file. # Contains the navigation structure docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"gen2_Tutorial/","text":"Next-Generation (gen2) Clusters - New in version 0.9.x \u00b6 Gen1 clusters are limited in capability by the Helm chart used to deploy the infrastructure resources. Advanced Cluster API configurations, such as those using multiple MachinePools, each with different instance types, is not supported. Gen2 clusters solve this problem by allowing you to create workload clusters from a cluster template that you design and provide in the form of a manifest file stored in a git directory. The manifest typically contains multiple related resources that together define an arbitrarily complex cluster. If you make subsequent changes to the cluster template, workload clusters originally created from it will automatically acquire the changes. Earlier, cluster templates were called base clusters . Some parts of the code still refer to base cluster manifests. This should be considered as a synonym for cluster template manifests. Creating Cluster-API cluster template \u00b6 Note: The CAPA version used here is v2.0 and the manifests created here are in accordance with this version. Refer the compatibility matrix for Cluster API provider and CAPA versions for supported versions. Before deploying a EKS cluster, make sure to setup the AWS Environment as stated in the quickstart giude for CAPI MachineDeployment \u00b6 Here is an example of a manifest file that we can use to create a cluster template . This manifest file helps in deploying an EKS cluster with 'machine deployment' component from the cluster API (CAPI). This file has been generated by the following command clusterctl generate cluster capi-quickstart --flavor eks \\ --kubernetes-version v1.24.0 \\ --control-plane-machine-count = 3 \\ --worker-machine-count = 3 \\ > capi-quickstart.yaml # YAML apiVersion : cluster.x-k8s.io/v1beta1 kind : Cluster metadata : name : capi-quickstart namespace : default spec : clusterNetwork : pods : cidrBlocks : - 192.168.0.0/16 controlPlaneRef : apiVersion : controlplane.cluster.x-k8s.io/v1beta2 kind : AWSManagedControlPlane name : capi-quickstart-control-plane infrastructureRef : apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSManagedCluster name : capi-quickstart --- apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSManagedCluster metadata : name : capi-quickstart spec : {} --- apiVersion : controlplane.cluster.x-k8s.io/v1beta2 kind : AWSManagedControlPlane metadata : name : capi-quickstart-control-plane namespace : default spec : region : { REGION } sshKeyName : { SSH_KEYNAME } version : v1.24.0 --- apiVersion : cluster.x-k8s.io/v1beta1 kind : MachineDeployment metadata : name : capi-quickstart-md-0 namespace : default spec : clusterName : capi-quickstart replicas : 3 selector : matchLabels : null template : spec : bootstrap : configRef : apiVersion : bootstrap.cluster.x-k8s.io/v1beta2 kind : EKSConfigTemplate name : capi-quickstart-md-0 clusterName : capi-quickstart infrastructureRef : apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSMachineTemplate name : capi-quickstart-md-0 version : v1.24.0 --- apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSMachineTemplate metadata : name : capi-quickstart-md-0 namespace : default spec : template : spec : iamInstanceProfile : nodes.cluster-api-provider-aws.sigs.k8s.io instanceType : { INSTANCE_TYPE } sshKeyName : { SSH_KEYNAME } --- apiVersion : bootstrap.cluster.x-k8s.io/v1beta2 kind : EKSConfigTemplate metadata : name : capi-quickstart-md-0 namespace : default spec : template : {} gen2 cluster creation using Arlon \u00b6 This manifest file needs to be pushed to the workspace repository before the manifest directory is prepped and then validated. Before a manifest directory can be used as a cluster manifest, it must first be \"prepared\" or \"prepped\" by Arlon. The \"prep\" phase makes minor changes to the directory and manifest to help Arlon deploy multiple copies of the cluster without naming conflicts. manifest directory preparation \u00b6 To prepare a git directory to serve as cluster template, use the clustertemplate preparegit command: arlon clustertemplate preparegit --repo-url <repoUrl> --repo-path <pathToDirectory> [ --repo-revision revision ] # OR # using repository aliases # using the default alias arlon clustertemplate preparegit --repo-path <pathToDirectory> [ --repo-revision revision ] # using the prod alias arlon clustertemplate preparegit --repo-alias prod --repo-path <pathToDirectory> [ --repo-revision revision ] manifest directory validation \u00b6 Post the successful preparation of the cluster template directory using clustertemplate preparegit , the cluster template directory needs to be validated before the cluster template is created. To determine if a git directory is eligible to serve as cluster template, run the clustertemplate validategit command: arlon clustertemplate validategit --repo-url <repoUrl> --repo-path <pathToDirectory> [ --repo-revision revision ] # OR # using repository aliases # using the default alias arlon clustertemplate validategit --repo-path <pathToDirectory> [ --repo-revision revision ] # using the prod alias arlon clustertemplate validategit --repo-alias prod --repo-path <pathToDirectory> [ --repo-revision revision ] gen2 cluster creation \u00b6 Note: Cluster templates only support dynamic profiles. To create a gen2 workload cluster from the cluster template: arlon cluster create --cluster-name <clusterName> --repo-url <repoUrl> --repo-path <pathToDirectory> [ --output-yaml ] [ --profile <profileName> ] [ --repo-revision <repoRevision> ] # OR # using repository aliases # using the default alias arlon cluster create --cluster-name <clusterName> --repo-path <pathToDirectory> [ --output-yaml ] [ --profile <profileName> ] [ --repo-revision <repoRevision> ] # using the prod alias arlon cluster create --cluster-name <clusterName> --repo-alias prod --repo-path <pathToDirectory> [ --output-yaml ] [ --profile <profileName> ] [ --repo-revision <repoRevision> ] gen2 cluster update \u00b6 To update the profiles of a gen2 workload cluster: # To add a new profile to the existing cluster arlon cluster ngupdate <clustername> --profile <profilename> # To delete an existing profile from the existing cluster arlon cluster ngupdate <clustername> --delete-profile A gen2 cluster can be created without any profile app associated with the cluster. So, the above commands can be used to add a new profile to the existing cluster which will create profile app in argocd along with bundle apps associated with the profile. An existing profile can be deleted from the cluster as well using the above command. Executing this command will delete the profile app and all the bundles associated with the profile in argocd. gen2 cluster deletion \u00b6 To destroy a gen2 workload cluster: arlon cluster delete <clusterName> Arlon creates between 2 and 3 ArgoCD application resources to compose a gen2 cluster (the 3rd application, called \"profile app\", is used when an optional profile is specified at cluster creation time). When you destroy a gen2 cluster, Arlon will find all related ArgoCD applications and clean them up. Known issues and limitations \u00b6 Gen2 clusters are powerful because the cluster template can be arbitrarily complex and feature rich. Since they are fairly new and still evolving, gen2 clusters have several known limitations relative to gen1. You cannot customize/override any property of the cluster template on the fly when creating a workload cluster, which is an exact clone of the cluster template except for the names of its resources and their namespace. The work-around is to make a copy of the cluster template directory, push the new directory, make the desired changes, commit & push the changes, and register the directory as a new cluster template. If you modify and commit a change to one or more properties of the cluster template that the underlying Cluster API provider deems as \"immutable\", new workload clusters created from the cluster template will have the modified propert(ies), but ArgoCD will flag existing clusters as OutOfSync, since the provider will continually reject attempts to apply the new property values. The existing clusters continue to function, despite appearing unhealthy in the ArgoCD UI and CLI outputs. Examples of mutable properties in Cluster API resources: Number of replicas (modification will result in a scale-up / down) Kubernetes version (modification will result in an upgrade) Examples of immutable properties: Most fields of AWSMachineTemplate (instance type, labels, etc...) For more information \u00b6 For more details on gen2 clusters, refer to the design document .","title":"Tutorial (gen2)"},{"location":"gen2_Tutorial/#next-generation-gen2-clusters-new-in-version-09x","text":"Gen1 clusters are limited in capability by the Helm chart used to deploy the infrastructure resources. Advanced Cluster API configurations, such as those using multiple MachinePools, each with different instance types, is not supported. Gen2 clusters solve this problem by allowing you to create workload clusters from a cluster template that you design and provide in the form of a manifest file stored in a git directory. The manifest typically contains multiple related resources that together define an arbitrarily complex cluster. If you make subsequent changes to the cluster template, workload clusters originally created from it will automatically acquire the changes. Earlier, cluster templates were called base clusters . Some parts of the code still refer to base cluster manifests. This should be considered as a synonym for cluster template manifests.","title":"Next-Generation (gen2) Clusters - New in version 0.9.x"},{"location":"gen2_Tutorial/#creating-cluster-api-cluster-template","text":"Note: The CAPA version used here is v2.0 and the manifests created here are in accordance with this version. Refer the compatibility matrix for Cluster API provider and CAPA versions for supported versions. Before deploying a EKS cluster, make sure to setup the AWS Environment as stated in the quickstart giude for CAPI","title":"Creating Cluster-API cluster template"},{"location":"gen2_Tutorial/#machinedeployment","text":"Here is an example of a manifest file that we can use to create a cluster template . This manifest file helps in deploying an EKS cluster with 'machine deployment' component from the cluster API (CAPI). This file has been generated by the following command clusterctl generate cluster capi-quickstart --flavor eks \\ --kubernetes-version v1.24.0 \\ --control-plane-machine-count = 3 \\ --worker-machine-count = 3 \\ > capi-quickstart.yaml # YAML apiVersion : cluster.x-k8s.io/v1beta1 kind : Cluster metadata : name : capi-quickstart namespace : default spec : clusterNetwork : pods : cidrBlocks : - 192.168.0.0/16 controlPlaneRef : apiVersion : controlplane.cluster.x-k8s.io/v1beta2 kind : AWSManagedControlPlane name : capi-quickstart-control-plane infrastructureRef : apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSManagedCluster name : capi-quickstart --- apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSManagedCluster metadata : name : capi-quickstart spec : {} --- apiVersion : controlplane.cluster.x-k8s.io/v1beta2 kind : AWSManagedControlPlane metadata : name : capi-quickstart-control-plane namespace : default spec : region : { REGION } sshKeyName : { SSH_KEYNAME } version : v1.24.0 --- apiVersion : cluster.x-k8s.io/v1beta1 kind : MachineDeployment metadata : name : capi-quickstart-md-0 namespace : default spec : clusterName : capi-quickstart replicas : 3 selector : matchLabels : null template : spec : bootstrap : configRef : apiVersion : bootstrap.cluster.x-k8s.io/v1beta2 kind : EKSConfigTemplate name : capi-quickstart-md-0 clusterName : capi-quickstart infrastructureRef : apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSMachineTemplate name : capi-quickstart-md-0 version : v1.24.0 --- apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSMachineTemplate metadata : name : capi-quickstart-md-0 namespace : default spec : template : spec : iamInstanceProfile : nodes.cluster-api-provider-aws.sigs.k8s.io instanceType : { INSTANCE_TYPE } sshKeyName : { SSH_KEYNAME } --- apiVersion : bootstrap.cluster.x-k8s.io/v1beta2 kind : EKSConfigTemplate metadata : name : capi-quickstart-md-0 namespace : default spec : template : {}","title":"MachineDeployment"},{"location":"gen2_Tutorial/#gen2-cluster-creation-using-arlon","text":"This manifest file needs to be pushed to the workspace repository before the manifest directory is prepped and then validated. Before a manifest directory can be used as a cluster manifest, it must first be \"prepared\" or \"prepped\" by Arlon. The \"prep\" phase makes minor changes to the directory and manifest to help Arlon deploy multiple copies of the cluster without naming conflicts.","title":"gen2 cluster creation using Arlon"},{"location":"gen2_Tutorial/#manifest-directory-preparation","text":"To prepare a git directory to serve as cluster template, use the clustertemplate preparegit command: arlon clustertemplate preparegit --repo-url <repoUrl> --repo-path <pathToDirectory> [ --repo-revision revision ] # OR # using repository aliases # using the default alias arlon clustertemplate preparegit --repo-path <pathToDirectory> [ --repo-revision revision ] # using the prod alias arlon clustertemplate preparegit --repo-alias prod --repo-path <pathToDirectory> [ --repo-revision revision ]","title":"manifest directory preparation"},{"location":"gen2_Tutorial/#manifest-directory-validation","text":"Post the successful preparation of the cluster template directory using clustertemplate preparegit , the cluster template directory needs to be validated before the cluster template is created. To determine if a git directory is eligible to serve as cluster template, run the clustertemplate validategit command: arlon clustertemplate validategit --repo-url <repoUrl> --repo-path <pathToDirectory> [ --repo-revision revision ] # OR # using repository aliases # using the default alias arlon clustertemplate validategit --repo-path <pathToDirectory> [ --repo-revision revision ] # using the prod alias arlon clustertemplate validategit --repo-alias prod --repo-path <pathToDirectory> [ --repo-revision revision ]","title":"manifest directory validation"},{"location":"gen2_Tutorial/#gen2-cluster-creation","text":"Note: Cluster templates only support dynamic profiles. To create a gen2 workload cluster from the cluster template: arlon cluster create --cluster-name <clusterName> --repo-url <repoUrl> --repo-path <pathToDirectory> [ --output-yaml ] [ --profile <profileName> ] [ --repo-revision <repoRevision> ] # OR # using repository aliases # using the default alias arlon cluster create --cluster-name <clusterName> --repo-path <pathToDirectory> [ --output-yaml ] [ --profile <profileName> ] [ --repo-revision <repoRevision> ] # using the prod alias arlon cluster create --cluster-name <clusterName> --repo-alias prod --repo-path <pathToDirectory> [ --output-yaml ] [ --profile <profileName> ] [ --repo-revision <repoRevision> ]","title":"gen2 cluster creation"},{"location":"gen2_Tutorial/#gen2-cluster-update","text":"To update the profiles of a gen2 workload cluster: # To add a new profile to the existing cluster arlon cluster ngupdate <clustername> --profile <profilename> # To delete an existing profile from the existing cluster arlon cluster ngupdate <clustername> --delete-profile A gen2 cluster can be created without any profile app associated with the cluster. So, the above commands can be used to add a new profile to the existing cluster which will create profile app in argocd along with bundle apps associated with the profile. An existing profile can be deleted from the cluster as well using the above command. Executing this command will delete the profile app and all the bundles associated with the profile in argocd.","title":"gen2 cluster update"},{"location":"gen2_Tutorial/#gen2-cluster-deletion","text":"To destroy a gen2 workload cluster: arlon cluster delete <clusterName> Arlon creates between 2 and 3 ArgoCD application resources to compose a gen2 cluster (the 3rd application, called \"profile app\", is used when an optional profile is specified at cluster creation time). When you destroy a gen2 cluster, Arlon will find all related ArgoCD applications and clean them up.","title":"gen2 cluster deletion"},{"location":"gen2_Tutorial/#known-issues-and-limitations","text":"Gen2 clusters are powerful because the cluster template can be arbitrarily complex and feature rich. Since they are fairly new and still evolving, gen2 clusters have several known limitations relative to gen1. You cannot customize/override any property of the cluster template on the fly when creating a workload cluster, which is an exact clone of the cluster template except for the names of its resources and their namespace. The work-around is to make a copy of the cluster template directory, push the new directory, make the desired changes, commit & push the changes, and register the directory as a new cluster template. If you modify and commit a change to one or more properties of the cluster template that the underlying Cluster API provider deems as \"immutable\", new workload clusters created from the cluster template will have the modified propert(ies), but ArgoCD will flag existing clusters as OutOfSync, since the provider will continually reject attempts to apply the new property values. The existing clusters continue to function, despite appearing unhealthy in the ArgoCD UI and CLI outputs. Examples of mutable properties in Cluster API resources: Number of replicas (modification will result in a scale-up / down) Kubernetes version (modification will result in an upgrade) Examples of immutable properties: Most fields of AWSMachineTemplate (instance type, labels, etc...)","title":"Known issues and limitations"},{"location":"gen2_Tutorial/#for-more-information","text":"For more details on gen2 clusters, refer to the design document .","title":"For more information"},{"location":"installation/","text":"Installation \u00b6 Arlon CLI downloads are provided on GitHub. The CLI is not a self-contained standalone executable though. It is required to point the CLI to a management cluster and set up the Arlon controller in this management cluster. For a quickstart minimal demonstration setup, follow the instructions to set up a KIND based testbed with Arlon and ArgoCD running here . Please follow the manual instructions in this section for a customised setup or refer the instructions for automated installation here . Customised Setup \u00b6 Management cluster \u00b6 You can use any Kubernetes cluster that you have admin access to. Ensure: kubectl is in your path KUBECONFIG is pointing to the right file and the context set properly ArgoCD \u00b6 Follow steps 1-4 of the ArgoCD installation guide to install ArgoCD onto your management cluster. After this step, you should be logged in as admin and a config file was created at ${HOME}/.config/argocd/config Create your workspace repository in your git provider if necessary, then register it. Example: argocd repo add https://github.com/myname/arlon_workspace --username myname --password secret . -- Note: type argocd repo add --help to see all available options. -- For Arlon developers, this is not your fork of the Arlon source code repository, but a separate git repo where some artifacts like profiles created by Arlon will be stored. Highly recommended: configure a webhook to immediately notify ArgoCD of changes to the repo. This will be especially useful during the tutorial. Without a webhook, repo changes may take up to 3 minutes to be detected, delaying cluster configuration updates. Create a local user named arlon with the apiKey capability. This involves editing the argocd-cm ConfigMap using kubectl . Adjust the RBAC settings to grant admin permissions to the arlon user. This involves editing the argocd-rbac-cm ConfigMap to add the entry g, arlon, role:admin under the policy.csv section. Example: apiVersion : v1 data : policy.csv : | g, arlon, role:admin kind : ConfigMap [ ... ] Generate an account token: argocd account generate-token --account arlon Make a temporary copy of this config-file in /tmp/config then edit it to replace the value of auth-token with the token from the previous step. Save changes. This file will be used to configure the Arlon controller's ArgoCD credentials during the next steps. Arlon controller \u00b6 Create the arlon namespace: kubectl create ns arlon Create the ArgoCD credentials secret from the temporary config file: kubectl -n arlon create secret generic argocd-creds --from-file /tmp/config Delete the temporary config file Clone the arlon git repo and cd to its top directory Create the CRDs: kubectl apply -f config/crd/bases/ Deploy the controller: kubectl apply -f deploy/manifests/ Ensure the controller eventually enters the Running state: watch kubectl -n arlon get pod Arlon CLI \u00b6 Download the CLI for the latest release from GitHub. Currently, Linux and MacOS operating systems are supported. Uncompress the tarball, rename it as arlon and add to your PATH Run arlon verify to check for prerequisites. Run arlon install to install any missing prerequisites. The following instructions are to manually build CLI from this code repository. Building the CLI \u00b6 Clone this repository and pull the latest version of a branch (main by default) From the top directory, run make build Optionally create a symlink from a directory (e.g. /usr/local/bin ) included in your ${PATH} to the bin/arlon binary to make it easy to invoke the command. Cluster orchestration API providers \u00b6 Arlon currently supports Cluster API on AWS cloud. It also has experimental support for Crossplane on AWS. Cluster API \u00b6 Using the Cluster API Quickstart Guide as reference, complete these steps: Install clusterctl Initialize the management cluster. In particular, follow instructions for your specific cloud provider (AWS in this example) Ensure clusterctl init completes successfully and produces the expected output. Crossplane (experimental) \u00b6 Using the Upbound AWS Reference Platform Quickstart Guide as reference, complete these steps: Install UXP on your management cluster Install Crossplane kubectl extension Install the platform configuration Configure the cloud provider credentials You do not need to go any further, but you're welcome to try the Network Fabric example. FYI: we noticed the dozens/hundreds of CRDs that Crossplane installs in the management cluster can noticeably slow down kubectl, and you may see a warning that looks like : I0222 17 :31:14.112689 27922 request.go:668 ] Waited for 1 .046146023s due to client-side throttling, not priority and fairness, request: GET:https://AA61XXXXXXXXXXX.gr7.us-west-2.eks.amazonaws.com/apis/servicediscovery.aws.crossplane.io/v1alpha1?timeout = 32s Automatic setup \u00b6 Starting from version 0.10 (v0.10), Arlon CLI provides an init command to install \"itself\" on a management cluster. This command performs a basic setup of argocd(if needed) and arlon controller.","title":"Installation"},{"location":"installation/#installation","text":"Arlon CLI downloads are provided on GitHub. The CLI is not a self-contained standalone executable though. It is required to point the CLI to a management cluster and set up the Arlon controller in this management cluster. For a quickstart minimal demonstration setup, follow the instructions to set up a KIND based testbed with Arlon and ArgoCD running here . Please follow the manual instructions in this section for a customised setup or refer the instructions for automated installation here .","title":"Installation"},{"location":"installation/#customised-setup","text":"","title":"Customised Setup"},{"location":"installation/#management-cluster","text":"You can use any Kubernetes cluster that you have admin access to. Ensure: kubectl is in your path KUBECONFIG is pointing to the right file and the context set properly","title":"Management cluster"},{"location":"installation/#argocd","text":"Follow steps 1-4 of the ArgoCD installation guide to install ArgoCD onto your management cluster. After this step, you should be logged in as admin and a config file was created at ${HOME}/.config/argocd/config Create your workspace repository in your git provider if necessary, then register it. Example: argocd repo add https://github.com/myname/arlon_workspace --username myname --password secret . -- Note: type argocd repo add --help to see all available options. -- For Arlon developers, this is not your fork of the Arlon source code repository, but a separate git repo where some artifacts like profiles created by Arlon will be stored. Highly recommended: configure a webhook to immediately notify ArgoCD of changes to the repo. This will be especially useful during the tutorial. Without a webhook, repo changes may take up to 3 minutes to be detected, delaying cluster configuration updates. Create a local user named arlon with the apiKey capability. This involves editing the argocd-cm ConfigMap using kubectl . Adjust the RBAC settings to grant admin permissions to the arlon user. This involves editing the argocd-rbac-cm ConfigMap to add the entry g, arlon, role:admin under the policy.csv section. Example: apiVersion : v1 data : policy.csv : | g, arlon, role:admin kind : ConfigMap [ ... ] Generate an account token: argocd account generate-token --account arlon Make a temporary copy of this config-file in /tmp/config then edit it to replace the value of auth-token with the token from the previous step. Save changes. This file will be used to configure the Arlon controller's ArgoCD credentials during the next steps.","title":"ArgoCD"},{"location":"installation/#arlon-controller","text":"Create the arlon namespace: kubectl create ns arlon Create the ArgoCD credentials secret from the temporary config file: kubectl -n arlon create secret generic argocd-creds --from-file /tmp/config Delete the temporary config file Clone the arlon git repo and cd to its top directory Create the CRDs: kubectl apply -f config/crd/bases/ Deploy the controller: kubectl apply -f deploy/manifests/ Ensure the controller eventually enters the Running state: watch kubectl -n arlon get pod","title":"Arlon controller"},{"location":"installation/#arlon-cli","text":"Download the CLI for the latest release from GitHub. Currently, Linux and MacOS operating systems are supported. Uncompress the tarball, rename it as arlon and add to your PATH Run arlon verify to check for prerequisites. Run arlon install to install any missing prerequisites. The following instructions are to manually build CLI from this code repository.","title":"Arlon CLI"},{"location":"installation/#building-the-cli","text":"Clone this repository and pull the latest version of a branch (main by default) From the top directory, run make build Optionally create a symlink from a directory (e.g. /usr/local/bin ) included in your ${PATH} to the bin/arlon binary to make it easy to invoke the command.","title":"Building the CLI"},{"location":"installation/#cluster-orchestration-api-providers","text":"Arlon currently supports Cluster API on AWS cloud. It also has experimental support for Crossplane on AWS.","title":"Cluster orchestration API providers"},{"location":"installation/#cluster-api","text":"Using the Cluster API Quickstart Guide as reference, complete these steps: Install clusterctl Initialize the management cluster. In particular, follow instructions for your specific cloud provider (AWS in this example) Ensure clusterctl init completes successfully and produces the expected output.","title":"Cluster API"},{"location":"installation/#crossplane-experimental","text":"Using the Upbound AWS Reference Platform Quickstart Guide as reference, complete these steps: Install UXP on your management cluster Install Crossplane kubectl extension Install the platform configuration Configure the cloud provider credentials You do not need to go any further, but you're welcome to try the Network Fabric example. FYI: we noticed the dozens/hundreds of CRDs that Crossplane installs in the management cluster can noticeably slow down kubectl, and you may see a warning that looks like : I0222 17 :31:14.112689 27922 request.go:668 ] Waited for 1 .046146023s due to client-side throttling, not priority and fairness, request: GET:https://AA61XXXXXXXXXXX.gr7.us-west-2.eks.amazonaws.com/apis/servicediscovery.aws.crossplane.io/v1alpha1?timeout = 32s","title":"Crossplane (experimental)"},{"location":"installation/#automatic-setup","text":"Starting from version 0.10 (v0.10), Arlon CLI provides an init command to install \"itself\" on a management cluster. This command performs a basic setup of argocd(if needed) and arlon controller.","title":"Automatic setup"},{"location":"tutorial/","text":"Tutorial (gen1) \u00b6 This assumes that you plan to deploy workload clusters on AWS cloud, with Cluster API (\"CAPI\") as the cluster orchestration API provider. Also ensure you have set up a workspace repository , and it is registered as a git repo in ArgoCD. The tutorial will assume the existence of these environment variables: ${ARLON_REPO} : where the arlon repo is locally checked out ${WORKSPACE_REPO} : where the workspace repo is locally checked out ${WORKSPACE_REPO_URL} : the workspace repo's git URL. It typically looks like https://github.com/${username}/${reponame}.git ${CLOUD_REGION} : the region where you want to deploy example clusters and workloads (e.g. us-west-2) ${SSH_KEY_NAME} : the name of a public ssh key name registered in your cloud account, to enable ssh to your cluster nodes Additionally, for examples assuming arlon git register , \"default\" and a \"prod\" git repo aliases will also be given. _Note: for the best experience, make sure your workspace repo is configured to send change notifications to ArgoCD via a webhook. See the Installation section for details. Cluster specs \u00b6 We first create a few cluster specs with different combinations of API providers and cluster types (kubeadm vs EKS). One of the cluster specs is for an unconfigured API provider (Crossplane); this is for illustrative purposes, since we will not use it in this tutorial. arlon clusterspec create capi-kubeadm-3node --api capi --cloud aws --type kubeadm --kubeversion v1.21.10 --nodecount 3 --nodetype t2.medium --tags devel,test --desc \"3 node kubeadm for dev/test\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } arlon clusterspec create capi-eks --api capi --cloud aws --type eks --kubeversion v1.21.10 --nodecount 2 --nodetype t2.large --tags staging --desc \"2 node eks for general purpose\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } arlon clusterspec create xplane-eks-3node --api xplane --cloud aws --type eks --kubeversion v1.21.10 --nodecount 4 --nodetype t2.small --tags experimental --desc \"4 node eks managed by crossplane\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } Ensure you can now list the cluster specs: $ arlon clusterspec list NAME APIPROV CLOUDPROV TYPE KUBEVERSION NODETYPE NODECNT MSTNODECNT SSHKEY CAS CASMIN CASMAX TAGS DESCRIPTION capi-eks capi aws eks v1.21.10 t2.large 2 3 leb false 1 9 staging 2 node eks for general purpose capi-kubeadm-3node capi aws kubeadm v1.21.10 t2.medium 3 3 leb false 1 9 devel,test 3 node kubeadm for dev/test xplane-eks-3node xplane aws eks v1.21.10 t2.small 4 3 leb false 1 9 experimental 4 node eks managed by crossplane Bundles \u00b6 First create a static bundle containing raw YAML for the guestbook sample application from this example file: cd ${ ARLON_REPO } arlon bundle create guestbook-static --tags applications --desc \"guestbook app\" --from-file examples/bundles/guestbook.yaml ( Note: the YAML is simply a concatenation of the files found in the ArgoCD Example Apps repo ) To illustrate the difference between static and dynamic bundles, we create a dynamic version of the same application, this time using a reference to a git directory containing the YAML. We could point it directly to the copy in the ArgoCD Example Apps repo , but we'll want to make modifications to it, so we instead create a new directory to host our own copy in our workspace directory: cd ${ WORKSPACE_REPO } mkdir -p bundles/guestbook cp ${ ARLON_REPO } /examples/bundles/guestbook.yaml bundles/guestbook git add bundles/guestbook git commit -m \"add guestbook\" git push origin main arlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-url ${ WORKSPACE_REPO_URL } --repo-path bundles/guestbook # OR # using repository aliases # using the default alias arlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-path bundles/guestbook # using the prod alias arlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-path bundles/guestbook --repo-alias prod Next, we create a static bundle for another \"dummy\" application, an Ubuntu pod (OS version: \"Xenial\") that does nothing but print the date-time in an infinite sleep loop: cd ${ ARLON_REPO } arlon bundle create xenial-static --tags applications --desc \"xenial pod\" --from-file examples/bundles/xenial.yaml Finally, we create a bundle for the Calico CNI, which provides pod networking. Some types of clusters (e.g. kubeadm) require a CNI provider to be installed onto a newly created cluster, so encapsulating the provider as a bundle will give us a flexible way to install it. We download a known copy from the authoritative source and store it the workspace repo in order to create a dynamic bundle from it: cd ${ WORKSPACE_REPO } mkdir -p bundles/calico curl https://docs.projectcalico.org/v3.21/manifests/calico.yaml -o bundles/calico/calico.yaml git add bundles/calico git commit -m \"add calico\" git push origin main arlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-url ${ WORKSPACE_REPO_URL } --repo-path bundles/calico # OR # using repository aliases # using the default alias arlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-path bundles/calico # using the prod alias arlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-path bundles/calico --repo-alias prod List your bundles to verify they were correctly entered: $ arlon bundle list NAME TYPE TAGS REPO-URL REPO-PATH DESCRIPTION calico dynamic networking,cni ${ WORKSPACE_REPO_URL } bundles/calico Calico CNI guestbook-dynamic dynamic applications ${ WORKSPACE_REPO_URL } bundles/guestbook guestbook app ( dynamic ) guestbook-static static applications ( N/A ) ( N/A ) guestbook app xenial-static static applications ( N/A ) ( N/A ) ubuntu pod in infinite sleep loop Profiles \u00b6 We can now create profiles to group bundles into useful, deployable sets. First, create a static profile containing bundles xenial-static and guestbook-static: arlon profile create static-1 --static --bundles guestbook-static,xenial-static --desc \"static profile 1\" --tags examples Secondly, create a dynamic version of the same profile. We'll store the compiled form of the profile in the profiles/dynamic-1 directory of the workspace repo. We don't create it manually; instead, the arlon CLI will create it for us, and it will push the change to git: arlon profile create dynamic-1 --repo-url ${ WORKSPACE_REPO_URL } --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples # OR # using repository aliases # using the default alias arlon profile create dynamic-1 --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples # using the prod alias arlon profile create dynamic-1 --repo-alias prod --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples Note: the --repo-base-path profiles option tells arlon to create the profile under a base directory profiles/ (to be created if it doesn't exist). That is in fact the default value of that option, so it is not necessary to specify it in this case. To verify that the compiled profile was created correctly: $ cd ${ WORKSPACE_REPO } $ git pull $ tree profiles profiles \u251c\u2500\u2500 dynamic-1 \u2502 \u251c\u2500\u2500 mgmt \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u251c\u2500\u2500 guestbook-dynamic.yaml \u2502 \u2502 \u251c\u2500\u2500 placeholder_configmap.yaml \u2502 \u2502 \u2514\u2500\u2500 xenial.yaml \u2502 \u2514\u2500\u2500 workload \u2502 \u2514\u2500\u2500 xenial \u2502 \u2514\u2500\u2500 xenial.yaml [ ... ] Since xenial is a static bundle, a copy of its YAML was stored in workload/xenial/xenial.yaml . This is not done for guestbook-dynamic because it is dynamic. Finally, we create another variant of the same profile, with the only difference being the addition of Calico bundle. It'll be used on clusters that need a CNI provider: arlon profile create dynamic-2-calico --repo-url ${ WORKSPACE_REPO_URL } --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples # OR # using repository aliases # using the default alias arlon profile create dynamic-2-calico --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples # using the prod alias arlon profile create dynamic-2-calico --repo-alias prod --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples Listing the profiles should show: $ arlon profile list NAME TYPE BUNDLES REPO-URL REPO-PATH TAGS DESCRIPTION dynamic-1 dynamic guestbook-static,xenial-static ${ WORKSPACE_REPO_URL } profiles/dynamic-1 examples dynamic test 1 dynamic-2-calico dynamic calico,guestbook-static,xenial-static ${ WORKSPACE_REPO_URL } profiles/dynamic-2-calico examples dynamic test 1 static-1 static guestbook-dynamic,xenial-static ( N/A ) ( N/A ) examples static profile 1 Clusters (gen1) \u00b6 We are now ready to deploy our first cluster. It will be of type EKS. Since EKS clusters come configured with pod networking out of the box, we choose a profile that does not include Calico: dynamic-1 . When deploying a cluster, arlon creates in git a Helm chart containing the manifests for creating and bootstrapping the cluster. Arlon then creates an ArgoCD App referencing the chart, thereby relying on ArgoCD to orchestrate the whole process of deploying and configuring the cluster. The arlon deploy command accepts a git URL and path for this git location. Any git repo can be used (so long as it's registered with ArgoCD), but we'll use the workspace cluster for convenience: arlon cluster deploy --repo-url ${ WORKSPACE_REPO_URL } --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks # OR # using repository aliases # using the default alias arlon cluster deploy --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks # using the prod alias arlon cluster deploy --repo-alias prod --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks The git directory hosting the cluster Helm chart is created as a subdirectory of a base path in the repo. The base path can be specified with --base-path , but we'll leave it unspecified in order to use the default value of clusters . Consequently, this example produces the directory clusters/eks-1/ in the repo. To verify its presence: $ cd ${ WORKSPACE_REPO } $ git pull $ tree clusters/eks-1 clusters/eks-1 \u2514\u2500\u2500 mgmt \u251c\u2500\u2500 charts \u2502 \u251c\u2500\u2500 capi-aws-eks \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u251c\u2500\u2500 capi-aws-kubeadm \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u2514\u2500\u2500 xplane-aws-eks \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2514\u2500\u2500 templates \u2502 \u251c\u2500\u2500 cluster.yaml \u2502 \u2514\u2500\u2500 network.yaml \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 clusterregistration.yaml \u2502 \u251c\u2500\u2500 ns.yaml \u2502 \u251c\u2500\u2500 profile.yaml \u2502 \u2514\u2500\u2500 rbac.yaml \u2514\u2500\u2500 values.yaml The chart contains several subcharts under mgmt/charts/ , one for each supported type of cluster. Only one of them will be enabled, in this case capi-aws-eks (Cluster API on AWS with type EKS). At this point, the cluster is provisioning and can be seen in arlon and AWS EKS: $ arlon cluster list NAME CLUSTERSPEC PROFILE eks-1 capi-eks dynamic-1 $ aws eks list-clusters { \"clusters\" : [ \"eks-1_eks-1-control-plane\" , ] } Eventually, it will also be seen as a registered cluster in argocd, but this won't be visible for a while, because the cluster is not registered until its control plane (the Kubernetes API) is ready: $ argocd cluster list SERVER NAME VERSION STATUS MESSAGE https://9F07DC211252C6F7686F90FA5B8B8447.gr7.us-west-2.eks.amazonaws.com eks-1 1 .18+ Successful https://kubernetes.default.svc in -cluster 1 .20+ Successful To monitor the progress of the cluster deployment, check the status of the ArgoCD app of the same name: $ argocd app list NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGET eks-1 https://kubernetes.default.svc default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } clusters/eks-1/mgmt main eks-1-guestbook-static default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-1/workload/guestbook-static HEAD eks-1-profile-dynamic-1 https://kubernetes.default.svc argocd default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-1/mgmt HEAD eks-1-xenial default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-1/workload/xenial HEAD The top-level app eks-1 is the root of all argocd apps that make up the cluster and its configuration contents. The next level app eks-1-profile-dynamic-1 represents the profile, and its children apps eks-1-guestbook-static and eks-1-xenial correspond to the bundles. Note: The overall tree-like organization of the apps and their health status can be visually observed in the ArgoCD web user interface._ The cluster is fully deployed once those apps are all Synced and Healthy . An EKS cluster typically takes 10-15 minutes to finish deploying. Behavioral differences between static and dynamic bundles & profiles \u00b6 Static bundle \u00b6 A change to a static bundle does not affect existing clusters using that bundle (through a profile). To illustrate this, bring up the ArgoCD UI and open the detailed view of the eks-1-guestbook-static application, which applies the guestbook-static bundle to the eks-1 cluster. Note that there is only one guestbook-ui pod. Next, update the guestbook-static bundle to have 3 replicas of the pod: arlon bundle update guestbook-static --from-file examples/bundles/guestbook-3replicas.yaml Note that the UI continues to show one pod. Only new clusters consuming this bundle will have the 3 replicas. Dynamic profile \u00b6 Before discussing dynamic bundles, we take a small detour to introduce dynamic profiles, since this will help understand the relationship between profiles and bundles. To illustrate how a profile can be updated, we remove guestbook-static bundle from dynamic-1 by specifying a new bundle set: arlon profile update dynamic-1 --bundles xenial Since the old bundle set was guestbook-static,xenial-static , that command resulted in the removal of guestbook-static from the profile. In the UI, observe the eks-1-profile-dynamic-1 app going through Sync and Progressing phases, eventually reaching the healthy (green) state. And most importantly, the eks-1-guestbook-static app is gone. The reason this real-time change to the cluster was possible is that the dynamic-1 profile is dynamic, meaning any change to its composition results in arlon updating the corresponding compiled Helm chart in git. ArgoCD detects this git change and propagates the app / configuration updates to the cluster. If the profile were of the static type, a change in its composition (the set of bundles) would not have affected existing clusters using that profile. It would only affect new clusters created with the profile. Dynamic bundle \u00b6 To illustrate the defining characteristic of a dynamic bundle, we first add guestbook-dynamic to dynamic-1 : arlon profile update dynamic-1 --bundles xenial,guestbook-dynamic Observe the re-appearance of the guestbook application, which is managed by the eks-1-guestbook-dynamic ArgoCD app. A detailed view of the app shows 1 guestbook-ui pod. Remember that a dynamic bundle's manifest content is stored in git. Use these commands to change the number of pod replicas to 3: cd ${ WORKSPACE_REPO } git pull # to get all latest changes pushed by arlon vim bundles/guestbook/guestbook.yaml # edit to change deployment's replicas to 3 git commit -am \"increase guestbook replicas\" git push origin main Observe the number of pods increasing to 3 in the UI. Any existing cluster consuming this dynamic bundle will be updated similarly, regardless of whether the bundle is consumed via a dynamic or static profile. Static profile \u00b6 Finally, a profile can be static. It means that it has no corresponding \"compiled\" component (a Helm chart) living in git. When a cluster is deployed using a static profile, the set of bundles (whether static or dynamic) it receives is determined by the bundle set defined by the profile at deployment time, and will not change in the future, even if the profile is updated to a new set at a later time. Cluster updates and upgrades \u00b6 The arlon cluster update [flags] command allows you to make changes to an existing cluster. The clusterspec, profile, or both can change, provided that the following rules and guidelines are followed. Clusterspec \u00b6 There are two scenarios. In the first, the clusterspec name associated with the cluster hasn't changed, meaning the cluster is using the same clusterspec. However, some properties of the clusterspec's properties have changed since the cluster was deployed or last updated, using arlon clusterspec update Arlon supports updating the cluster to use updated values of the following properties: kubernetesVersion nodeCount nodeType Note: Updating the cluster is not allowed if other properties of its clusterspec (e.g. cluster orchestration API provider, cloud, cluster type, region, pod CIDR block, etc...) have changed, however new clusters can always be created/deployed using the changed clusterspec. A change in kubernetesVersion will result in a cluster upgrade/downgrade. There are some restrictions and caveats you need to be aware of: The specific Kubernetes version must be supported by the particular implementation and release of the underlying cluster orchestration API provider cloud, and cluster type. In general, the control plane will be upgraded first Existing nodes are not typically not upgraded to the new Kubernetes version. Only new nodes (added as part of manual nodeCount change or autoscaling) In the second scenario, as part of an update operation, you may choose to associate the cluster with a different clusterspec altogether. The rule governing the allowed property changes remains the same: the cluster update operation is allowed if, relative to the previously associated clusterspec, the new clusterspec's properties differ only in the values listed above. Profile \u00b6 You can specify a completely different profile when updating a cluster. All bundles previously used will be removed from the cluster, and new ones specified by the new profile will be applied. This is regardless of whether the old and new profiles are static or dynamic. Examples \u00b6 These sequence of commands updates a clusterspec to a newer Kubernetes version and a higher node count, then upgrades the cluster to the newer specifications: arlon clusterspec update capi-eks --nodecount 3 --kubeversion v1.19.15 arlon cluster update eks-1 Note that the 2nd command didn't need any flags because the clusterspec used is the same as before. This example updates a cluster to use a new profile my-new-profile : arlon cluster update eks-1 --profile my-new-profile Enabling Cluster Autoscaler in the workload cluster \u00b6 Bundle creation \u00b6 Register a dynamic bundle pointing to the bundles/capi-cluster-autoscaler in the Arlon repo. The capi-cluster-autoscaler bundle requires the name of the cluster, so that it knows what namespace in the management cluster to scan for CAPI resources. To enable the cluster-autoscaler bundle, add one more parameter during cluster creation: srcType . This is the ArgoCD-defined application source type (Helm, Kustomize, Directory). This example creates a bundle pointing to the bundles/capi-cluster-autoscaler in Arlon repo arlon bundle create cas-bundle --tags cas,devel,test --desc \"CAS Bundle\" --repo-url https://github.com/arlonproj/arlon.git --repo-path bundles/capi-cluster-autoscaler --srctype helm Profile creation \u00b6 Create a profile that contains this capi-cluster-autoscaler bundle. arlon profile create dynamic-cas --repo-url ${ WORKSPACE_REPO_URL } --repo-base-path profiles --bundles cas-bundle --desc \"dynamic cas profile\" --tags examples Clusterspec creation \u00b6 Create a clusterspec with CAPI as ApiProvider and autoscaling enabled.In addition to this, the ClusterAutoscaler(Min|Max)Nodes properties are used to set 2 annotations on MachineDeployment required by the cluster autoscaler for CAPI. arlon clusterspec create cas-spec --api capi --cloud aws --type eks --kubeversion v1.21.10 --nodecount 2 --nodetype t2.medium --tags devel,test --desc \"dev/test\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } --casenabled Cluster creation \u00b6 Deploy a cluster from this cluster spec and profile created in the previous steps. arlon cluster deploy --repo-url ${ WORKSPACE_REPO_URL } --cluster-name cas-cluster --profile dynamic-cas --cluster-spec cas-spec Consequently, this example produces the directory clusters/cas-cluster/ in the repo. This will contain the capi-autoscaler subchart and manifests mgmt/charts/ . To verify its contents: $ cd ${ WORKSPACE_REPO_URL } $ tree clusters/cas-cluster clusters/cas-cluster \u2514\u2500\u2500 mgmt \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 charts \u2502 \u251c\u2500\u2500 capi-aws-eks \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u251c\u2500\u2500 capi-aws-kubeadm \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u251c\u2500\u2500 capi-cluster-autoscaler \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u251c\u2500\u2500 callhomeconfig.yaml \u2502 \u2502 \u2514\u2500\u2500 rbac.yaml \u2502 \u2514\u2500\u2500 xplane-aws-eks \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2514\u2500\u2500 templates \u2502 \u251c\u2500\u2500 cluster.yaml \u2502 \u2514\u2500\u2500 network.yaml \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 clusterregistration.yaml \u2502 \u251c\u2500\u2500 ns.yaml \u2502 \u251c\u2500\u2500 profile.yaml \u2502 \u2514\u2500\u2500 rbac.yaml \u2514\u2500\u2500 values.yaml At this point, the cluster is provisioning. To monitor the progress of the cluster deployment, check the status of the ArgoCD app of the same name. Eventually, the ArgoCD apps will be synced and healthy. $ argocd app list NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGET cas-cluster https://kubernetes.default.svc default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } clusters/cas-cluster/mgmt main cas-cluster-cas-bundle cas-cluster default default Synced Healthy Auto-Prune <none> https://github.com/arlonproj/arlon.git bundles/capi-cluster-autoscaler HEAD cas-cluster-profile-dynamic-cas https://kubernetes.default.svc argocd default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-cas/mgmt","title":"Tutorial (gen1)"},{"location":"tutorial/#tutorial-gen1","text":"This assumes that you plan to deploy workload clusters on AWS cloud, with Cluster API (\"CAPI\") as the cluster orchestration API provider. Also ensure you have set up a workspace repository , and it is registered as a git repo in ArgoCD. The tutorial will assume the existence of these environment variables: ${ARLON_REPO} : where the arlon repo is locally checked out ${WORKSPACE_REPO} : where the workspace repo is locally checked out ${WORKSPACE_REPO_URL} : the workspace repo's git URL. It typically looks like https://github.com/${username}/${reponame}.git ${CLOUD_REGION} : the region where you want to deploy example clusters and workloads (e.g. us-west-2) ${SSH_KEY_NAME} : the name of a public ssh key name registered in your cloud account, to enable ssh to your cluster nodes Additionally, for examples assuming arlon git register , \"default\" and a \"prod\" git repo aliases will also be given. _Note: for the best experience, make sure your workspace repo is configured to send change notifications to ArgoCD via a webhook. See the Installation section for details.","title":"Tutorial (gen1)"},{"location":"tutorial/#cluster-specs","text":"We first create a few cluster specs with different combinations of API providers and cluster types (kubeadm vs EKS). One of the cluster specs is for an unconfigured API provider (Crossplane); this is for illustrative purposes, since we will not use it in this tutorial. arlon clusterspec create capi-kubeadm-3node --api capi --cloud aws --type kubeadm --kubeversion v1.21.10 --nodecount 3 --nodetype t2.medium --tags devel,test --desc \"3 node kubeadm for dev/test\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } arlon clusterspec create capi-eks --api capi --cloud aws --type eks --kubeversion v1.21.10 --nodecount 2 --nodetype t2.large --tags staging --desc \"2 node eks for general purpose\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } arlon clusterspec create xplane-eks-3node --api xplane --cloud aws --type eks --kubeversion v1.21.10 --nodecount 4 --nodetype t2.small --tags experimental --desc \"4 node eks managed by crossplane\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } Ensure you can now list the cluster specs: $ arlon clusterspec list NAME APIPROV CLOUDPROV TYPE KUBEVERSION NODETYPE NODECNT MSTNODECNT SSHKEY CAS CASMIN CASMAX TAGS DESCRIPTION capi-eks capi aws eks v1.21.10 t2.large 2 3 leb false 1 9 staging 2 node eks for general purpose capi-kubeadm-3node capi aws kubeadm v1.21.10 t2.medium 3 3 leb false 1 9 devel,test 3 node kubeadm for dev/test xplane-eks-3node xplane aws eks v1.21.10 t2.small 4 3 leb false 1 9 experimental 4 node eks managed by crossplane","title":"Cluster specs"},{"location":"tutorial/#bundles","text":"First create a static bundle containing raw YAML for the guestbook sample application from this example file: cd ${ ARLON_REPO } arlon bundle create guestbook-static --tags applications --desc \"guestbook app\" --from-file examples/bundles/guestbook.yaml ( Note: the YAML is simply a concatenation of the files found in the ArgoCD Example Apps repo ) To illustrate the difference between static and dynamic bundles, we create a dynamic version of the same application, this time using a reference to a git directory containing the YAML. We could point it directly to the copy in the ArgoCD Example Apps repo , but we'll want to make modifications to it, so we instead create a new directory to host our own copy in our workspace directory: cd ${ WORKSPACE_REPO } mkdir -p bundles/guestbook cp ${ ARLON_REPO } /examples/bundles/guestbook.yaml bundles/guestbook git add bundles/guestbook git commit -m \"add guestbook\" git push origin main arlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-url ${ WORKSPACE_REPO_URL } --repo-path bundles/guestbook # OR # using repository aliases # using the default alias arlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-path bundles/guestbook # using the prod alias arlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-path bundles/guestbook --repo-alias prod Next, we create a static bundle for another \"dummy\" application, an Ubuntu pod (OS version: \"Xenial\") that does nothing but print the date-time in an infinite sleep loop: cd ${ ARLON_REPO } arlon bundle create xenial-static --tags applications --desc \"xenial pod\" --from-file examples/bundles/xenial.yaml Finally, we create a bundle for the Calico CNI, which provides pod networking. Some types of clusters (e.g. kubeadm) require a CNI provider to be installed onto a newly created cluster, so encapsulating the provider as a bundle will give us a flexible way to install it. We download a known copy from the authoritative source and store it the workspace repo in order to create a dynamic bundle from it: cd ${ WORKSPACE_REPO } mkdir -p bundles/calico curl https://docs.projectcalico.org/v3.21/manifests/calico.yaml -o bundles/calico/calico.yaml git add bundles/calico git commit -m \"add calico\" git push origin main arlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-url ${ WORKSPACE_REPO_URL } --repo-path bundles/calico # OR # using repository aliases # using the default alias arlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-path bundles/calico # using the prod alias arlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-path bundles/calico --repo-alias prod List your bundles to verify they were correctly entered: $ arlon bundle list NAME TYPE TAGS REPO-URL REPO-PATH DESCRIPTION calico dynamic networking,cni ${ WORKSPACE_REPO_URL } bundles/calico Calico CNI guestbook-dynamic dynamic applications ${ WORKSPACE_REPO_URL } bundles/guestbook guestbook app ( dynamic ) guestbook-static static applications ( N/A ) ( N/A ) guestbook app xenial-static static applications ( N/A ) ( N/A ) ubuntu pod in infinite sleep loop","title":"Bundles"},{"location":"tutorial/#profiles","text":"We can now create profiles to group bundles into useful, deployable sets. First, create a static profile containing bundles xenial-static and guestbook-static: arlon profile create static-1 --static --bundles guestbook-static,xenial-static --desc \"static profile 1\" --tags examples Secondly, create a dynamic version of the same profile. We'll store the compiled form of the profile in the profiles/dynamic-1 directory of the workspace repo. We don't create it manually; instead, the arlon CLI will create it for us, and it will push the change to git: arlon profile create dynamic-1 --repo-url ${ WORKSPACE_REPO_URL } --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples # OR # using repository aliases # using the default alias arlon profile create dynamic-1 --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples # using the prod alias arlon profile create dynamic-1 --repo-alias prod --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples Note: the --repo-base-path profiles option tells arlon to create the profile under a base directory profiles/ (to be created if it doesn't exist). That is in fact the default value of that option, so it is not necessary to specify it in this case. To verify that the compiled profile was created correctly: $ cd ${ WORKSPACE_REPO } $ git pull $ tree profiles profiles \u251c\u2500\u2500 dynamic-1 \u2502 \u251c\u2500\u2500 mgmt \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u251c\u2500\u2500 guestbook-dynamic.yaml \u2502 \u2502 \u251c\u2500\u2500 placeholder_configmap.yaml \u2502 \u2502 \u2514\u2500\u2500 xenial.yaml \u2502 \u2514\u2500\u2500 workload \u2502 \u2514\u2500\u2500 xenial \u2502 \u2514\u2500\u2500 xenial.yaml [ ... ] Since xenial is a static bundle, a copy of its YAML was stored in workload/xenial/xenial.yaml . This is not done for guestbook-dynamic because it is dynamic. Finally, we create another variant of the same profile, with the only difference being the addition of Calico bundle. It'll be used on clusters that need a CNI provider: arlon profile create dynamic-2-calico --repo-url ${ WORKSPACE_REPO_URL } --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples # OR # using repository aliases # using the default alias arlon profile create dynamic-2-calico --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples # using the prod alias arlon profile create dynamic-2-calico --repo-alias prod --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples Listing the profiles should show: $ arlon profile list NAME TYPE BUNDLES REPO-URL REPO-PATH TAGS DESCRIPTION dynamic-1 dynamic guestbook-static,xenial-static ${ WORKSPACE_REPO_URL } profiles/dynamic-1 examples dynamic test 1 dynamic-2-calico dynamic calico,guestbook-static,xenial-static ${ WORKSPACE_REPO_URL } profiles/dynamic-2-calico examples dynamic test 1 static-1 static guestbook-dynamic,xenial-static ( N/A ) ( N/A ) examples static profile 1","title":"Profiles"},{"location":"tutorial/#clusters-gen1","text":"We are now ready to deploy our first cluster. It will be of type EKS. Since EKS clusters come configured with pod networking out of the box, we choose a profile that does not include Calico: dynamic-1 . When deploying a cluster, arlon creates in git a Helm chart containing the manifests for creating and bootstrapping the cluster. Arlon then creates an ArgoCD App referencing the chart, thereby relying on ArgoCD to orchestrate the whole process of deploying and configuring the cluster. The arlon deploy command accepts a git URL and path for this git location. Any git repo can be used (so long as it's registered with ArgoCD), but we'll use the workspace cluster for convenience: arlon cluster deploy --repo-url ${ WORKSPACE_REPO_URL } --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks # OR # using repository aliases # using the default alias arlon cluster deploy --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks # using the prod alias arlon cluster deploy --repo-alias prod --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks The git directory hosting the cluster Helm chart is created as a subdirectory of a base path in the repo. The base path can be specified with --base-path , but we'll leave it unspecified in order to use the default value of clusters . Consequently, this example produces the directory clusters/eks-1/ in the repo. To verify its presence: $ cd ${ WORKSPACE_REPO } $ git pull $ tree clusters/eks-1 clusters/eks-1 \u2514\u2500\u2500 mgmt \u251c\u2500\u2500 charts \u2502 \u251c\u2500\u2500 capi-aws-eks \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u251c\u2500\u2500 capi-aws-kubeadm \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u2514\u2500\u2500 xplane-aws-eks \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2514\u2500\u2500 templates \u2502 \u251c\u2500\u2500 cluster.yaml \u2502 \u2514\u2500\u2500 network.yaml \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 clusterregistration.yaml \u2502 \u251c\u2500\u2500 ns.yaml \u2502 \u251c\u2500\u2500 profile.yaml \u2502 \u2514\u2500\u2500 rbac.yaml \u2514\u2500\u2500 values.yaml The chart contains several subcharts under mgmt/charts/ , one for each supported type of cluster. Only one of them will be enabled, in this case capi-aws-eks (Cluster API on AWS with type EKS). At this point, the cluster is provisioning and can be seen in arlon and AWS EKS: $ arlon cluster list NAME CLUSTERSPEC PROFILE eks-1 capi-eks dynamic-1 $ aws eks list-clusters { \"clusters\" : [ \"eks-1_eks-1-control-plane\" , ] } Eventually, it will also be seen as a registered cluster in argocd, but this won't be visible for a while, because the cluster is not registered until its control plane (the Kubernetes API) is ready: $ argocd cluster list SERVER NAME VERSION STATUS MESSAGE https://9F07DC211252C6F7686F90FA5B8B8447.gr7.us-west-2.eks.amazonaws.com eks-1 1 .18+ Successful https://kubernetes.default.svc in -cluster 1 .20+ Successful To monitor the progress of the cluster deployment, check the status of the ArgoCD app of the same name: $ argocd app list NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGET eks-1 https://kubernetes.default.svc default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } clusters/eks-1/mgmt main eks-1-guestbook-static default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-1/workload/guestbook-static HEAD eks-1-profile-dynamic-1 https://kubernetes.default.svc argocd default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-1/mgmt HEAD eks-1-xenial default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-1/workload/xenial HEAD The top-level app eks-1 is the root of all argocd apps that make up the cluster and its configuration contents. The next level app eks-1-profile-dynamic-1 represents the profile, and its children apps eks-1-guestbook-static and eks-1-xenial correspond to the bundles. Note: The overall tree-like organization of the apps and their health status can be visually observed in the ArgoCD web user interface._ The cluster is fully deployed once those apps are all Synced and Healthy . An EKS cluster typically takes 10-15 minutes to finish deploying.","title":"Clusters (gen1)"},{"location":"tutorial/#behavioral-differences-between-static-and-dynamic-bundles-profiles","text":"","title":"Behavioral differences between static and dynamic bundles &amp; profiles"},{"location":"tutorial/#static-bundle","text":"A change to a static bundle does not affect existing clusters using that bundle (through a profile). To illustrate this, bring up the ArgoCD UI and open the detailed view of the eks-1-guestbook-static application, which applies the guestbook-static bundle to the eks-1 cluster. Note that there is only one guestbook-ui pod. Next, update the guestbook-static bundle to have 3 replicas of the pod: arlon bundle update guestbook-static --from-file examples/bundles/guestbook-3replicas.yaml Note that the UI continues to show one pod. Only new clusters consuming this bundle will have the 3 replicas.","title":"Static bundle"},{"location":"tutorial/#dynamic-profile","text":"Before discussing dynamic bundles, we take a small detour to introduce dynamic profiles, since this will help understand the relationship between profiles and bundles. To illustrate how a profile can be updated, we remove guestbook-static bundle from dynamic-1 by specifying a new bundle set: arlon profile update dynamic-1 --bundles xenial Since the old bundle set was guestbook-static,xenial-static , that command resulted in the removal of guestbook-static from the profile. In the UI, observe the eks-1-profile-dynamic-1 app going through Sync and Progressing phases, eventually reaching the healthy (green) state. And most importantly, the eks-1-guestbook-static app is gone. The reason this real-time change to the cluster was possible is that the dynamic-1 profile is dynamic, meaning any change to its composition results in arlon updating the corresponding compiled Helm chart in git. ArgoCD detects this git change and propagates the app / configuration updates to the cluster. If the profile were of the static type, a change in its composition (the set of bundles) would not have affected existing clusters using that profile. It would only affect new clusters created with the profile.","title":"Dynamic profile"},{"location":"tutorial/#dynamic-bundle","text":"To illustrate the defining characteristic of a dynamic bundle, we first add guestbook-dynamic to dynamic-1 : arlon profile update dynamic-1 --bundles xenial,guestbook-dynamic Observe the re-appearance of the guestbook application, which is managed by the eks-1-guestbook-dynamic ArgoCD app. A detailed view of the app shows 1 guestbook-ui pod. Remember that a dynamic bundle's manifest content is stored in git. Use these commands to change the number of pod replicas to 3: cd ${ WORKSPACE_REPO } git pull # to get all latest changes pushed by arlon vim bundles/guestbook/guestbook.yaml # edit to change deployment's replicas to 3 git commit -am \"increase guestbook replicas\" git push origin main Observe the number of pods increasing to 3 in the UI. Any existing cluster consuming this dynamic bundle will be updated similarly, regardless of whether the bundle is consumed via a dynamic or static profile.","title":"Dynamic bundle"},{"location":"tutorial/#static-profile","text":"Finally, a profile can be static. It means that it has no corresponding \"compiled\" component (a Helm chart) living in git. When a cluster is deployed using a static profile, the set of bundles (whether static or dynamic) it receives is determined by the bundle set defined by the profile at deployment time, and will not change in the future, even if the profile is updated to a new set at a later time.","title":"Static profile"},{"location":"tutorial/#cluster-updates-and-upgrades","text":"The arlon cluster update [flags] command allows you to make changes to an existing cluster. The clusterspec, profile, or both can change, provided that the following rules and guidelines are followed.","title":"Cluster updates and upgrades"},{"location":"tutorial/#clusterspec","text":"There are two scenarios. In the first, the clusterspec name associated with the cluster hasn't changed, meaning the cluster is using the same clusterspec. However, some properties of the clusterspec's properties have changed since the cluster was deployed or last updated, using arlon clusterspec update Arlon supports updating the cluster to use updated values of the following properties: kubernetesVersion nodeCount nodeType Note: Updating the cluster is not allowed if other properties of its clusterspec (e.g. cluster orchestration API provider, cloud, cluster type, region, pod CIDR block, etc...) have changed, however new clusters can always be created/deployed using the changed clusterspec. A change in kubernetesVersion will result in a cluster upgrade/downgrade. There are some restrictions and caveats you need to be aware of: The specific Kubernetes version must be supported by the particular implementation and release of the underlying cluster orchestration API provider cloud, and cluster type. In general, the control plane will be upgraded first Existing nodes are not typically not upgraded to the new Kubernetes version. Only new nodes (added as part of manual nodeCount change or autoscaling) In the second scenario, as part of an update operation, you may choose to associate the cluster with a different clusterspec altogether. The rule governing the allowed property changes remains the same: the cluster update operation is allowed if, relative to the previously associated clusterspec, the new clusterspec's properties differ only in the values listed above.","title":"Clusterspec"},{"location":"tutorial/#profile","text":"You can specify a completely different profile when updating a cluster. All bundles previously used will be removed from the cluster, and new ones specified by the new profile will be applied. This is regardless of whether the old and new profiles are static or dynamic.","title":"Profile"},{"location":"tutorial/#examples","text":"These sequence of commands updates a clusterspec to a newer Kubernetes version and a higher node count, then upgrades the cluster to the newer specifications: arlon clusterspec update capi-eks --nodecount 3 --kubeversion v1.19.15 arlon cluster update eks-1 Note that the 2nd command didn't need any flags because the clusterspec used is the same as before. This example updates a cluster to use a new profile my-new-profile : arlon cluster update eks-1 --profile my-new-profile","title":"Examples"},{"location":"tutorial/#enabling-cluster-autoscaler-in-the-workload-cluster","text":"","title":"Enabling Cluster Autoscaler in the workload cluster"},{"location":"tutorial/#bundle-creation","text":"Register a dynamic bundle pointing to the bundles/capi-cluster-autoscaler in the Arlon repo. The capi-cluster-autoscaler bundle requires the name of the cluster, so that it knows what namespace in the management cluster to scan for CAPI resources. To enable the cluster-autoscaler bundle, add one more parameter during cluster creation: srcType . This is the ArgoCD-defined application source type (Helm, Kustomize, Directory). This example creates a bundle pointing to the bundles/capi-cluster-autoscaler in Arlon repo arlon bundle create cas-bundle --tags cas,devel,test --desc \"CAS Bundle\" --repo-url https://github.com/arlonproj/arlon.git --repo-path bundles/capi-cluster-autoscaler --srctype helm","title":"Bundle creation"},{"location":"tutorial/#profile-creation","text":"Create a profile that contains this capi-cluster-autoscaler bundle. arlon profile create dynamic-cas --repo-url ${ WORKSPACE_REPO_URL } --repo-base-path profiles --bundles cas-bundle --desc \"dynamic cas profile\" --tags examples","title":"Profile creation"},{"location":"tutorial/#clusterspec-creation","text":"Create a clusterspec with CAPI as ApiProvider and autoscaling enabled.In addition to this, the ClusterAutoscaler(Min|Max)Nodes properties are used to set 2 annotations on MachineDeployment required by the cluster autoscaler for CAPI. arlon clusterspec create cas-spec --api capi --cloud aws --type eks --kubeversion v1.21.10 --nodecount 2 --nodetype t2.medium --tags devel,test --desc \"dev/test\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } --casenabled","title":"Clusterspec creation"},{"location":"tutorial/#cluster-creation","text":"Deploy a cluster from this cluster spec and profile created in the previous steps. arlon cluster deploy --repo-url ${ WORKSPACE_REPO_URL } --cluster-name cas-cluster --profile dynamic-cas --cluster-spec cas-spec Consequently, this example produces the directory clusters/cas-cluster/ in the repo. This will contain the capi-autoscaler subchart and manifests mgmt/charts/ . To verify its contents: $ cd ${ WORKSPACE_REPO_URL } $ tree clusters/cas-cluster clusters/cas-cluster \u2514\u2500\u2500 mgmt \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 charts \u2502 \u251c\u2500\u2500 capi-aws-eks \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u251c\u2500\u2500 capi-aws-kubeadm \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u251c\u2500\u2500 capi-cluster-autoscaler \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u251c\u2500\u2500 callhomeconfig.yaml \u2502 \u2502 \u2514\u2500\u2500 rbac.yaml \u2502 \u2514\u2500\u2500 xplane-aws-eks \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2514\u2500\u2500 templates \u2502 \u251c\u2500\u2500 cluster.yaml \u2502 \u2514\u2500\u2500 network.yaml \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 clusterregistration.yaml \u2502 \u251c\u2500\u2500 ns.yaml \u2502 \u251c\u2500\u2500 profile.yaml \u2502 \u2514\u2500\u2500 rbac.yaml \u2514\u2500\u2500 values.yaml At this point, the cluster is provisioning. To monitor the progress of the cluster deployment, check the status of the ArgoCD app of the same name. Eventually, the ArgoCD apps will be synced and healthy. $ argocd app list NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGET cas-cluster https://kubernetes.default.svc default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } clusters/cas-cluster/mgmt main cas-cluster-cas-bundle cas-cluster default default Synced Healthy Auto-Prune <none> https://github.com/arlonproj/arlon.git bundles/capi-cluster-autoscaler HEAD cas-cluster-profile-dynamic-cas https://kubernetes.default.svc argocd default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-cas/mgmt","title":"Cluster creation"}]}