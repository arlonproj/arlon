{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#_1","title":"Overview","text":""},{"location":"#what-is-arlon","title":"What Is Arlon?","text":"<p>Arlon is a declarative, gitops based fleet management tool for Kubernetes clusters. It allows administrators to:</p> <ul> <li>Deploy and upgrade a large number of workload clusters</li> <li>Secure clusters by installing and configuring policies</li> <li>Install a set of applications / add-ons on the clusters</li> </ul> <p>all in a structured, predictable manner. Arlon makes Kubernetes cluster fleet management secure, version controlled, auditable and easy to perform at scale. </p> <p>Arlon takes advantage of multiple declarative cluster management API providers for the actual cluster orchestration. The first two supported API providers are Cluster API and Crossplane. Arlon uses ArgoCD as the underlying Kubernetes manifest deployment and enforcement engine.</p> <p>A workload cluster is composed of the following constructs:</p> <ul> <li>Cluster spec: a description of the infrastructure and external settings of a cluster, e.g. Kubernetes version, cloud provider, cluster type, node instance type.</li> <li>Profile: a grouping of configuration bundles which will be installed into the cluster</li> <li>Configuration bundle: a unit of configuration which contains (or references) one or more Kubernetes manifests. A bundle can encapsulate anything that can be deployed onto a cluster: an RBAC ruleset, an add-on, an application, etc...</li> </ul>"},{"location":"#arlon-benefits","title":"Arlon Benefits","text":"<ul> <li>Improves time to market by enabling better velocity for developers through infrastructure management that is more fluid and agile. Define, store, change and enforce your cluster infrastructure &amp; application add-ons at scale.  </li> <li>Reduces the risk of unexpected infrastructure downtime and outages, or unexpected security misconfiguration, with consistent management of infrastructure and security policies.</li> <li>Allows IT and Platform Ops admins to operate large scale of clusters, infrastructure &amp; add-ons with significantly reduced team size &amp; operational overhead, using GitOps.</li> </ul>"},{"location":"#contents","title":"Contents","text":"<ul> <li>Concepts</li> <li>Installation</li> <li>Tutorial</li> <li>Architecture</li> </ul>"},{"location":"appprofiles/","title":"Application Profiles (new in v0.10.0)","text":"<p>The Application Profiles feature, also known as Gen2 Profiles, is an addition to Arlon v0.10.0 that provides a new way to describe, group, and deploy manifests to workload clusters. The new feature introduces these concepts:</p> <ul> <li>Arlon Application (or just \"App\")</li> <li>Application Profile (a.k.a. AppProfile)</li> <li>Targeting application profiles to workload clusters via annotation</li> </ul> <p>The feature provides an alternative to the Bundle and Profile concepts (\"the old way\") of earlier versions of Arlon. Specifically, the Arlon Application can be viewed as a replacement for Bundles, and AppProfile is a substitute for Profile. In release v0.10.0, the \"old way\" continues to be supported, but is deprecated, meaning it will likely be retired in an upcoming release.</p>"},{"location":"appprofiles/#arlon-application-aka-arlon-app","title":"Arlon Application (a.k.a. \"Arlon App\")","text":"<p>An Arlon Application is similar to a Dynamic Bundle from earlier releases. It specifies a source of one or more manifests stored in git in any \"tool\" format supported by ArgoCD</p> <p>Internally, Arlon represents an App as a specialized ArgoCD ApplicationSet resource. This allows you to specify the manifest source in the <code>spec.template.spec</code> section, while Arlon takes care of targeting the deployment to the correct workload cluster(s) by automatically manipulating the <code>spec.generators</code> section. ApplicationSets managed by Arlon to represent apps are distinguished from other ApplicationSets via the <code>arlon-type=application</code> label. The ApplicationSet's Generators list must contain a single generator of List type. The Arlon AppProfile controller will modify this list in real-time to deploy the application to the right workload clusters (or no cluster at all).</p> <p>While it is possible for you create and edit an ApplicationSet resource manifest satisfying the requirements to be an Arlon App from scratch, Arlon makes this easier with the <code>arlon app create --output-yaml</code> command, which outputs an initial compliant manifest that you can save to a file and edit to your liking before applying to the management cluster to actually create the app. (Without the <code>--output-yaml</code> flag, the command will apply the resource for you).</p> <p>Here's an example of an initial Arlon Application manifest:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\nlabels:\narlon-type: application\nmanaged-by: arlon\nname: myconfigmap\nnamespace: argocd\nspec:\ngenerators:\n- list: {}\ntemplate:\nmetadata:\nname: '{{cluster_name}}-app-myconfigmap'\nspec:\ndestination:\nnamespace: default\nserver: '{{cluster_server}}'\nproject: default\nsource:\npath: apps/my-cfgmap-1\nrepoURL: https://github.com/bcle/fleet-infra.git\ntargetRevision: HEAD\nsyncPolicy:\nautomated:\nprune: true\n</code></pre> <p>The List generator that the AppProfile controller maintains supplies two variables for template substitution:</p> <ul> <li><code>cluster_name</code>: the name of the target workload cluster</li> <li><code>cluster_server</code>: the URL+FQDN of the workload cluster's Kubernetes API endpoint</li> </ul> <p>Notice how the initial manifest takes advantage of those variables to set</p> <ul> <li><code>spec.template.metadata.name</code> to <code>{{cluster-name}}-app-myconfigmap</code> to ensure that any actual ArgoCD Application resources deployed from the ApplicationSet are uniquely named, by prefixing the cluster name.</li> <li><code>spec.template.spec.destination.server</code> to <code>{{cluster_server}}</code> to target the correct workload cluster</li> </ul> <p>Arlon apps can be listed in two ways. The first is to use the <code>arlon app list</code> command. One advantage is that it's simple to use and also displays additional information about the app, such as which AppProfiles are currently associated with the app.</p> <p>Example:</p> <pre><code>$ arlon app list\nNAME         REPO                                     PATH              REVISION  APP_PROFILES\nmyconfigmap  https://github.com/bcle/fleet-infra.git  apps/my-cfgmap-1  HEAD      [marketing]\n</code></pre> <p>The second way is to use pure kubectl to list ApplicationSets with the Arlon specific label:</p> <pre><code>$ kubectl -n argocd get applicationset -l arlon-type=application\nNAME          AGE\nmyconfigmap   21d\n</code></pre> <p>Similarly, an Arlon app can be deleted in two ways:</p> <ul> <li><code>arlon app delete &lt;appName&gt;</code></li> <li><code>kubectl -n argocd delete applicationset &lt;appName&gt;</code></li> </ul>"},{"location":"appprofiles/#appprofile","title":"AppProfile","text":"<p>An AppProfile is simply a grouping (or set) of Arlon Apps. Unlike an Arlon Application (which is represented by an ApplicationSet resource), an AppProfile is represented by an Arlon-native custom resource.</p> <p>An AppProfile specifies the apps it is associated with via the <code>appNames</code> list. It is legal for <code>appNames</code> to contain names of Arlon Apps that don't exist yet. To indicate whether some app names are currently invalid, the AppProfile controller will update the resource's <code>status</code> section as follows:</p> <ul> <li>If all specified app names refer to valid Arlon apps, <code>status.health</code>   is set to <code>healthy</code>.</li> <li>If one or more specified app names refer to non-existent Arlon apps,   then <code>status.health</code> is set to <code>degraded</code>, and <code>status.invalidAppNames</code>   lists the invalid names.</li> </ul> <p>Here is an example of an AppProfile manifest that includes 3 apps, one of which does not exist:</p> <pre><code>apiVersion: core.arlon.io/v1\nkind: AppProfile\nmetadata:\nname: marketing\nnamespace: arlon\nspec:\nappNames:\n- myconfigmap\n- wordpress\n- nonexistent-app\nstatus:\nhealth: degraded\ninvalidAppNames:\n- nonexistent-app\n</code></pre> <p>Note: AppProfile resources reside in the <code>arlon</code> namespace. This is in contrast to Arlon App resources, which reside in the <code>argocd</code> namespace since they are actually ArgoCD ApplicationSet resources.</p> <p>Since AppProfiles are defined by their own custom resource and are fairly straightforward, their lifecycle can be managed entirely using <code>kubectl</code>. That said, Arlon provides the <code>arlon appprofile list</code> to display useful information about current AppProfiles. Example:</p> <pre><code>$ arlon appprofile list\nNAME         APPS                     HEALTH   INVALID_APPS\nengineering  [wordpress]              healthy  []\nmarketing    [myconfigmap wordpress]  degraded [nonexistent-app]\n</code></pre> <p>As the example illustrates, it is totally legal for two or more AppProfiles to include the same app(s). When those AppProfiles are targeted to a workload cluster, the cluster receives the union of these app sets.</p>"},{"location":"appprofiles/#targeting-appprofiles-to-workload-clusters","title":"Targeting AppProfiles to Workload Clusters","text":"<p>Apps are actually deployed to workload clusters by annotating the desired cluster(s) with <code>arlon.io/profiles=&lt;comma separated list of AppProfiles&gt;</code>. This is supported only on Arlon Gen2 clusters, which themselves are represented as ArgoCD Application resources.</p> <p>Example: suppose we have a Gen2 workload cluster named clust1.  Since the cluster is represented as an ArgoCD Application resource, targeting the <code>engineering</code> and <code>marketing</code> app profiles to it is achieved with:</p> <pre><code>kubectl -n argocd annotate --overwrite application clust1 arlon.io/profiles=engineering,marketing\n</code></pre> <p>The set of app profiles for a cluster can always be updated that way, in real time. Behind the scenes, Arlon's AppProfile controller updates the <code>spec.generators</code> section of each affected app to target the cluster. The set of apps targeting a cluster is the union of all the valid appNames from every app profile listed in the annotation.</p> <p>By default, every ArgoCD Application Resource generated by the app's ApplicationSet is named with the pattern <code>&lt;clustername&gt;-app-&lt;appname&gt;</code>. In the running example, these two ArgoCD application resources will be generated:</p> <ul> <li>clust1-app-wordpress</li> <li>clust1-app-myconfigmap</li> </ul> <p>To detach all profiles from a cluster, simply remove the annotation:</p> <pre><code>kubectl -n argocd annotate application clust1 arlon.io/profiles-\n</code></pre> <p>The last command will automatically destroy any ArgoCD application resources generated for that cluster (assuming they originate from Arlon apps).</p> <p>Note: users are free to create and maintain their own ApplicationSets not managed by Arlon. Those will work side-by-side with the ones managed by Arlon, which is unaware of those other ApplicationSets. This allows the user to take advantage of other types of ApplicationSet generators.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>Arlon is composed of a controller, a library, and a CLI that exposes the library's functions as commands. In the future, an API server may be built from the library as well. Arlon adds CRDs (custom resource definitions) for several custom resources such as ClusterRegistration and Profile.</p>"},{"location":"architecture/#management-cluster","title":"Management cluster","text":"<p>The management cluster is a Kubernetes cluster hosting all the components needed by Arlon, including:</p> <ul> <li>The ArgoCD server</li> <li>The Arlon \"database\" (implemented as Kubernetes secrets and configmaps)</li> <li>The Arlon controller</li> <li>Cluster management API providers: Cluster API or Crossplane</li> <li>Custom resources (CRs) that drive the involved providers and controllers</li> <li>Custom resource definitions (CRDs) for all the involved CRs</li> </ul> <p>The user is responsible for supplying the management cluster, and to have access to a kubeconfig granting administrator permissions on the cluster.</p>"},{"location":"architecture/#controller","title":"Controller","text":"<p>The Arlon controller observes and responds to changes in <code>clusterregistration</code> custom resources. The Arlon library creates a <code>clusterregistration</code> at the beginning of workload cluster creation, causing the controller to wait for the cluster's kubeconfig to become available, at which point it registers the cluster with ArgoCD to enable manifests described by bundles to be deployed to the cluster.</p>"},{"location":"architecture/#library","title":"Library","text":"<p>The Arlon library is a Go module that contains the functions that communicate with the Management Cluster to manipulate the Arlon state (bundles, profiles, clusterspecs) and transforms them into git directory structures to drive ArgoCD's gitops engine. Initially, the library is exposed via a CLI utility. In the future, it may also be embodied into a server an exposed via a network API.</p>"},{"location":"architecture/#workspace-repository","title":"Workspace repository","text":"<p>As mentioned earlier, Arlon creates and maintains directory structures in a git repository to drive ArgoCD sync operations. The user is responsible for supplying this workspace repository (and base paths) hosting those structures. Arlon relies on ArgoCD for repository registration, therefore the user should register the workspace registry in ArgoCD before referencing it from Arlon data types.</p> <p>Starting from release v0.9.0, Arlon now includes two commands to help with managing various git repository URLs. With these commands in place, the <code>--repo-url</code> flag in commands requiring a hosted git repository is no longer needed. A more detailed explanation is given in the next section.</p>"},{"location":"architecture/#repo-aliases","title":"Repo Aliases","text":"<p>A repo(repository) alias allows an Arlon user to register a GitHub repository with ArgoCD and store a local configuration file on their system that can be referenced by the CLI to then determine a repository URL and fetch its credentials when needed. All commands that require a repository, support a <code>--repo-url</code> flag also support a <code>repo-alias</code> flag to specify an alias instead of an alias, such commands will consider the \"default\" alias to be used when no <code>--repo-alias</code> and no <code>--repo-url</code> flags are given. There are two subcommands i.e., <code>arlon git register</code> and <code>arlon git unregister</code> which allow for a basic form of git repository context management.</p> <p>When <code>arlon git register</code> is run it requires a repo URL, the username, the access token and an optional alias(which defaults to \u201cdefault\u201d)- if a \u201cdefault\u201d alias already exists, the repo isn\u2019t registered with <code>argocd</code> and the alias creation fails saying that the default alias already exists otherwise, the repo is registered with <code>argocd</code>. Lastly we also write this repository information to the local configuration file. This contains two pieces of information for each repository- it\u2019s URL and the alias. The structure of the file is as shown:</p> <pre><code>{\n\"default\": {\n\"url\": \"\",\n\"alias\": \"default\"\n},\n\"repos\": [\n{\n\"url\": \"\",\n\"alias\": \"default\"\n},\n{\n\"url\": \"\",\n\"alias\": \"not_default\"\n}, {}\n]\n}\n</code></pre> <p>On running <code>arlon git unregister ALIAS</code>, it removes that entry from the configuration file. However, it does NOT remove the repository from <code>argocd</code>. When the \"default\" alias is deleted, we also clear the \"default\" entry from the JSON file.</p>"},{"location":"architecture/#examples","title":"Examples","text":"<p>Given below are some examples for registering and unregistering a repository.</p>"},{"location":"architecture/#registering-repositories","title":"Registering Repositories","text":"<p>Registering a repository requires the repository link, the GitHub username(<code>--user</code>), and a personal access token(<code>--password</code>). When the <code>--password</code> flag isn't provided at the command line, the CLI will prompt for a password(this is the recommended approach).</p> <pre><code>arlon git register https://github.com/GhUser/manifests --user GhUser\narlon git register https://github.com/GhUser/prod-manifests --user GhUser --alias prod\n</code></pre> <p>For non-interactive registrations, the <code>--password</code> flag can be used.</p> <pre><code>export GH_PAT=\"...\"\narlon git register https://github.com/GhUser/manifests --user GhUser --password $GH_PAT\narlon git register https://github.com/GhUser/prod-manifests --user GhUser --alias prod --password $GH_PAT\n</code></pre>"},{"location":"architecture/#unregistering-repositories","title":"Unregistering Repositories","text":"<p>Unregistering an alias only requires a positional argument: the repository alias.</p> <pre><code># unregister the default alias locally\narlon git unregister default\n# unregister some other alias locally\narlon git unregister prod\n</code></pre>"},{"location":"clustertemplate/","title":"Cluster Templates","text":"<p>The Cluster Template object is designed with the following goals: - Allow users to deploy arbitrarily complex clusters using the full Cluster API feature set. - Fully declarative and gitops compatible: a cluster deployment should be composed of one or more self-sufficient manifests that the user can choose to either apply directly (via kubectl) or store in git for later-stage deployment by a gitops tool (mainly ArgoCD). - Support Linked Mode update: an update to the the cluster template should automatically propagate to all workload clusters deployed from it.</p> <p>A Cluster Template serves as a base for creating new workload clusters. The workload clusters are all exact copies of the cluster template, meaning that they acquire all unmodified resources of the cluster template, except for:</p> <ul> <li>resource names, which are prefixed during the cluster creation process to make them unique to avoid conflicts</li> <li>the namespace, which is set to a new namespace unique to the workload cluster</li> </ul>"},{"location":"clustertemplate/#architecture-diagram","title":"Architecture diagram","text":"<p>This example shows a cluster template named <code>capi-quickstart</code> used to deploy two workload clusters <code>cluster-a</code> and <code>cluster-b</code>. Additionally, <code>cluster-a</code> is given profile <code>xxx</code>, while <code>cluster-b</code> is given profile <code>yyy</code>.</p> <p></p>"},{"location":"clustertemplate/#template-creation-workflow","title":"Template Creation Workflow","text":"<ul> <li>To create a cluster template, a user first creates a single YAML file containing the desired Cluster API cluster and all related resources (e.g. MachineDeployments, etc...), using whatever tool the user chooses (e.g. <code>clusterctl generate cluster</code>). The user is responsible for the correctness of the file and resources within. Arlon will not check for errors. For example, the specified Kubernetes version must be supported by the Cluster API providers currently installed in the management cluster. If it isn't, resulting clusters will fail and enter a perpetual OutOfSync state.</li> <li>The user then commits and pushes the manifest file to a dedicated directory in a git repository. The name of the cluster resource does not matter, it will be used as a suffix during workload cluster creation. The directory should be unique to the file, and not contain any other files.</li> <li>If not already registered, the git repository should also be registered in ArgoCD with the proper credentials for read/write access.</li> </ul> <p>To check whether the git directory is a compliant Arlon cluster template, the user runs:</p> <pre><code>arlon clustertemplate validategit --repo-url &lt;repoUrl&gt; --repo-path &lt;pathToDirectory&gt; [--repo-revision revision]  </code></pre> <p>Note: if --repo-revision is not specified, it defaults to main.</p> <p>The command produces an error the first time because the git directory has not yet been \"prepped\". To \"prep\" the directory to become a compliant Arlon cluster template, the user runs:</p> <pre><code>arlon clustertemplate preparegit --repo-url &lt;repoUrl&gt; --repo-path &lt;pathToDirectory&gt; [--repo-revision revision]  </code></pre> <p>This pushes a commit to the repo with these changes:</p> <ul> <li>A <code>kustomization.yaml</code> file is added to the directory to make the manifest customizable by Kustomize.</li> <li>A <code>configurations.yaml</code> file is added to configure the namereference Kustomize plugin which ensures <code>reference</code> fields are correctly set when pointing to resource names that ArgoCD will modify using the Kustomize nameprefix mechanism. The content of the file is sourced from this Scott Lowe blog article.</li> <li>All <code>namespace</code> properties in the cluster manifest are removed to allow Kustomize to override the namespace of all resources.</li> </ul> <p>If prep is successful, another invocation of <code>arlon clustertemplate validategit</code> should succeed as well.</p>"},{"location":"clustertemplate/#workload-clusters","title":"Workload clusters","text":""},{"location":"clustertemplate/#creation","title":"Creation","text":"<p>Use <code>arlon cluster create</code> to create a next-gen workload cluster from a cluster template (this is different from <code>arlon cluster deploy</code> for creating older gen1 clusters). The command creates between 2 and 3 (depending on whether a profile is used) ArgoCD application resources that together make up the cluster and its contents. The general usage is:</p> <pre><code>arlon cluster create --cluster-name &lt;clusterName&gt; --repo-url &lt;repoUrl&gt; --repo-path &lt;pathToDirectory&gt; [--output-yaml] [--profile &lt;profileName&gt;] [--repo-revision &lt;repoRevision&gt;]\n</code></pre> <p>The command supports two modes of operation:</p> <ul> <li>With <code>--output-yaml</code>: output a list of YAML resources that you can inspect, save to a file, or pipe to <code>kubectl apply -f</code></li> <li>Without <code>--output-yaml</code>: create the application resources directly in the management cluster currently referenced by your KUBECONFIG and context.  </li> </ul> <p>The <code>--profile</code> flag is optional; a cluster can be created with no profile.</p>"},{"location":"clustertemplate/#composition","title":"Composition","text":"<p>A workload cluster is composed of 2 to 3 ArgoCD application resources, which are named based on the name of the cluster template and the workload cluster. For illustration purposes, the following discussion assumes that the cluster template is named <code>capi-quickstart</code>, the workload cluster is named <code>cluster-a</code>, and the optional profile is named <code>xxx</code>.</p>"},{"location":"clustertemplate/#cluster-app","title":"Cluster app","text":"<p>The <code>cluster-a</code> application is the cluster app for the cluster. It is responsible for deploying the cluster template resources, meaning the Cluster API manifests. It is named directly from the workload cluster name.</p> <p>The application's spec uses a ApplicationSourceKustomize that points to the cluster template's git directory. The spec ensures that all deployed resources are configured to:</p> <ul> <li>Reside in the <code>cluster-a</code> namespace, which is deployed by the arlon app (see below). This achieved by setting <code>app.Spec.Destination.Namespace</code> to the workload cluster's name   (this only works if the resources do not specify an explicit namespace; this requirement is taken care of by the \"prep\" step on the cluster template).</li> <li>Be named <code>cluster-a-capi-quickstart</code>, meaning the workload cluster name followed by the cluster template name. This is achieved by setting <code>app.Spec.Source.Kustomize.NamePrefix</code> to the workload cluster name plus a hyphen.</li> </ul>"},{"location":"clustertemplate/#arlon-app","title":"Arlon app","text":"<p>The <code>cluster-a-arlon</code> application is the arlon app for the cluster. It is resposible for deploying:</p> <ul> <li>The <code>cluster-a</code> namespace, which holds most resources related to this workload cluster, such as the Cluster API manifests deployed by the cluster app.</li> <li>Resources required to register the workload cluster with argocd when available: ClusterRegistration and associated RBAC rules.</li> <li>Additional resources (service account, more RBAC rules) for Cluster Autoscaler if enabled.</li> </ul> <p>The application spec's ApplicationSource points to the existing Arlon Helm chart located here by default:</p> <ul> <li>Repo: https://github.com/arlonproj/arlon.git</li> <li>Revision: v0.10</li> <li>Path: pkg/cluster/manifests</li> </ul> <p>This is the same Helm chart that gen1 clusters using <code>arlon cluster deploy</code> are deployed from. When used for the arlon app for a gen2 cluster, the Helm parameters are configured to only deploy the Arlon resources, with the subchart for cluster resources disabled, since those resources will be deployed by the cluster app.</p> <p>Important issue: as described above, the application source resides in the public Arlon repo. To avoid breaking user's deployed clusters, the source must be stable and not change!</p> <ul> <li>This probably means a particular Arlon release should point the source to a stable tag (not even a branch?)</li> <li>As an alternative, during Arlon setup, allow the user to copy the Helm chart into a private repo, and point the source there.</li> </ul>"},{"location":"clustertemplate/#profile-app-optional","title":"Profile app (optional)","text":"<p>A next-gen cluster can be assigned a current-gen dynamic profile, in which case Arlon creates a profile app named <code>&lt;clusterName&gt;-profile-&lt;profileName&gt;</code>, or <code>cluster-a-profile-xxx</code> in the running example.</p> <p>This is similar to the profile app created when attaching a profile app to an external cluster. The application source points to the git location of the dynamic profile.</p>"},{"location":"clustertemplate/#teardown","title":"Teardown","text":"<p>Since a next-gen cluster is composed of multiple ArgoCD applications, destroying the cluster requires deleting all of its applications. To facilitate this, the 2 or 3 applications created by <code>arlon cluster create</code> are automatically labeled with <code>arlon-cluster=&lt;clusterName&gt;</code>.</p> <p>The user has two options for destroying a next-gen cluster:</p> <ul> <li>The easiest way: <code>arlon cluster delete &lt;clusterName&gt;</code>. This command automatically detects a next-gen cluster and cleans up all related applications.</li> <li>A more manual way: <code>kubectl delete application -l arlon-cluster=&lt;clusterName&gt;</code></li> </ul>"},{"location":"clustertemplate/#update-semantics","title":"Update Semantics","text":"<p>A cluster template lives in git and is shared by all workload clusters created from it. This is sometimes referred to as Linked Mode. Any git update to the cluster can affect the associated workload clusters, therefore such updates must be planned and managed with care; there is a real risk of such an update breaking existing clusters.</p> <ul> <li>By default, a workload's cluster cluster app is configured with auto-sync, meaning ArgoCD will immediately apply any changes in the cluster template to the deployed Cluster API cluster resources.</li> <li>In general, a cluster template does not need to be \"prepped\" again after a modification to its main manifest file (the one containing the Cluster API resources). So the user is free to edit the manifest directly, commit/push the changes, and expect to see immediate changes to already-deployed clusters created from that cluster template.</li> </ul>"},{"location":"clustertemplate/#unsupported-changes","title":"Unsupported changes","text":"<p>The controllers for Cluster API and its providers disallow changes to some fields belonging to already-deployed resources.</p> <ul> <li>For example, changing the cluster template name (<code>medata.Name</code> of the <code>Cluster</code> resource) will have disastrous consequences on already-deployed clusters, causing many resources to enter the OutOfSync state and never recover because ArgoCD fails to apply the changes (they are rejected by the controllers). Consequently, a user should never change the name of a cluster template.</li> <li>Besides the cluster name, other fields cannot change (this has been observed anecdotally, we don't yet have an exhaustive list).</li> <li>Changing the Kubernetes version of the control plane or data plane is supported, so long as the new version is supported by the relevant providers. If accepted, such a change will result in a rolling update of the corresponding plane.</li> <li>Specific to AWS: the <code>AWSMachineTemplate.spec</code> is immutable and a CAPI webhook disallows such updates. The user is advised to not make such modifications to a cluster template manifest. In the event that such an event does happen, the user is advised to not manually sync in those changes via <code>argocd</code>. If a new cluster with a different <code>AWSMachineTemplate.spec</code> is desired, the recommended approach is to make a copy of the manifests in the workspace repository and then issue an <code>arlon cluster create</code> command which would then consume this manifest.</li> </ul>"},{"location":"concepts/","title":"Concepts","text":""},{"location":"concepts/#management-cluster","title":"Management cluster","text":"<p>This Kubernetes cluster hosts the following components:</p> <ul> <li>ArgoCD</li> <li>Arlon</li> <li>Cluster management stacks e.g. Cluster API and/or Crossplane</li> </ul> <p>The Arlon state and controllers reside in the arlon namespace.</p>"},{"location":"concepts/#workload-cluster","title":"Workload Cluster","text":"<p>An Arlon workload cluster is a Kubernetes cluster that Arlon creates and manages via a git directory structure stored in the workspace repository.</p> <p>The new way of provisioning workload clusters in Arlon since v0.9 is gen2 clusters using cluster template that replace gen1 clusters using cluster spec The most significant change in gen2 clusters is the Cluster Template construct, which replaces the older cluster spec from gen1 clusters. To distinguish them from the older gen1 clusters, the ones deployed from a cluster template are called next-gen clusters or gen2 clusters.</p>"},{"location":"concepts/#cluster-template","title":"Cluster Template","text":"<p>A cluster template is a base cluster manifest that can be \"cloned\" to produce one or more identical or similar workload clusters. To create a cluster template, you first compose a manifest containing one or more declarative resources that define the kind and shape of cluster that you desire. You then store the manifest in its own directoy somewhere in git. You then instruct Arlon to \"prep\" the directory to promote it to cluster template. To know more about cluster template for Arlon gen2 clusters including the difference with cluster spec and the process to create gen2 clusters; read the document cluster template</p>"},{"location":"concepts/#application-app","title":"Application (App)","text":"<p>An Arlon Application (or \"App\" for short) defines a source of Kubernetes manifests that can be applied/deployed to a workload cluster. It can take the form of raw YAML files, a Helm chart, or a Kustomize directory. The source resides in a git repository. Arlon represents an App as a specialized ArgoCD ApplicationSet resource. An App is not limited traditional \"applications\" and can refer to any set of deployable resources, for e.g. Kubernetes RBAC rules or other types of configurations. For more details about Apps, refer to AppProfiles article</p>"},{"location":"concepts/#application-profile-appprofile","title":"Application Profile (AppProfile)","text":"<p>An AppProfile is simply a set of App names referring to Arlon Apps. You use AppProfile resources to define common groupings of apps, for example \"monitoring-stack\", or \"security-policies-1\". It is perfectly legal for multiple AppProfiles to refer to some common App names, meaning they can overlap. For more details about AppProfiles, refer to AppProfiles article</p>"},{"location":"concepts/#deploying-apps-to-workload-clusters","title":"Deploying apps to workload clusters","text":"<p>You deploy apps to workload clusters by annotating a workload cluster with the desired AppProfile(s). The union of all Apps referenced by those AppProfiles is deployed to the cluster. For more details about annotating/targeting clusters, refer to AppProfiles article</p>"},{"location":"contributing/","title":"How to contribute to Arlon","text":"<p>Team Arlon welcomes and encourages everyone to participate in its development via pull requests on GitHub.</p> <p>We prefer to take in pull requests to our active development branch i.e. the <code>main</code> branch. To report a bug or request a feature, we rely on GitHub issues. There are a number of points to keep in mind when submitting a feature request, reporting a bug or contributing in the development of Arlon.</p> <ol> <li>Before making a feature request, or reporting a bug please browse through the existing open issues to be sure that it hasn't been already tracked.</li> <li>If a feature request is subsumed by some other open issue, please add your valuable feedback as a comment to the issue.</li> <li>If a bug discovered by you is already being tracked, please provide additional information as you see fit(steps to reproduce, particulars of the environment, version information etc.) as a comment.</li> <li>Before submitting code for a new feature(or a complex, untracked bugfix) please create a new issue. This issue needs to undergo a review process which may involve a discussion on the same GitHub issue to discuss possible approaches and motivation for the said proposal.</li> <li>Please reach out to us on Slack for discussions, help, questions and the roadmap.</li> </ol>"},{"location":"contributing/#code-changes","title":"Code changes","text":"<p>Open a pull request (PR) on GitHub following the typical GitHub workflow here. Most of the new code changes are merged to the <code>main</code> branch except backports, bookkeeping changes, library upgrades and some bugs that manifest only a particular version. Before contributing new code, contributors are encouraged to either write unit tests, e2e tests or perform some form of manual validation as a sanity-check. Please adhere to standard good practices for Golang and do ensure that the code is properly formatted and <code>vet</code> succeeds, for which we have <code>fmt</code> and <code>vet</code> targets respectively.</p> <p>Since Arlon is a growing project, various areas require improvements- improving code coverage with unit tests, e2e tests, documentation, CI/CD pipelines using GitHub Actions are a few to name, we highly encourage to contribute to those areas to start with. The e2e test documentation is an excellent starting point to grasp the workings of our e2e test setup.</p>"},{"location":"contributing/#issues-bug-reports","title":"Issues / Bug reports","text":"<p>We track issues on GitHub. You are encouraged to browse through these, add relevant feedback, create new issues or participate in the development. If you are interested in a particular issue or feature request, please leave a comment to reach out to the team. In particular, the issues labeled as help wanted are a great starting point for adding code changes to the project.</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>The documentation for Arlon is hosted on Read the Docs and comprises of contents from the \"docs\" directory of Arlon source.</p> <p>For making changes to the documentation, please follow the below steps:</p> <ol> <li>Fork the Arlon repository on GitHub and make the desired changes.</li> <li>Prerequisites<ol> <li>Ensure that <code>python3</code>, <code>pip3</code> is installed.</li> <li>Optionally, create a <code>venv</code> by running <code>python3 -m venv ./venv</code> to create a virtual environment if you don't have one.</li> <li>From the root of the Arlon repository, run <code>pip3 install -r docs/requirements.txt</code> to install <code>mkdocs</code> and other pre-requisites.</li> </ol> </li> <li>To test your local changes, run <code>mkdocs serve</code> from the repository root.<ol> <li>This starts a local server to host the documentation website where you can preview the changes.</li> </ol> </li> <li>To publish the changes, just push the changes to your fork repository and open a PR (pull request).</li> <li>Once your PR is accepted by one of the maintainers/ owners of Arlon project, the Arlon website will be updated.</li> </ol>"},{"location":"design/","title":"Arlon Design and Concepts","text":""},{"location":"design/#management-cluster","title":"Management cluster","text":"<p>This Kubernetes cluster hosts the following components:</p> <ul> <li>ArgoCD</li> <li>Arlon</li> <li>Cluster management stacks e.g. Cluster API and/or Crossplane</li> </ul> <p>The Arlon state and controllers reside in the arlon namespace.</p>"},{"location":"design/#configuration-bundle","title":"Configuration bundle","text":"<p>A configuration bundle (or just \"bundle\") is grouping of data files that produce a set of Kubernetes manifests via a tool. This closely follows ArgoCD's definition of tool types. Consequently, the list of supported bundle types mirrors ArgoCD's supported set of manifest-producing tools. Each bundle is defined using a Kubernetes ConfigMap resource in the arlo namespace. Additionally, a bundle can embed the data itself (\"static bundle\"), or contain a reference to the data (\"dynamic bundle\"). A reference can be a URL, GitHub location, or Helm repo location. The current list of supported bundle types is:</p> <ul> <li>manifest_inline: a single manifest yaml file embedded in the resource</li> <li>manifest_ref: a reference to a single manifest yaml file</li> <li>dir_inline: an embedded tarball that expands to a directory of YAML files</li> <li>helm_inline: an embedded Helm chart package</li> <li>helm_ref: an external reference to a Helm chart</li> </ul>"},{"location":"design/#bundle-purpose","title":"Bundle purpose","text":"<p>Bundles can specify an optional purpose to help classify and organize them. In the future, Arlon may order bundle installation by purpose order (for e.g. install bundles with purpose=networking before others) but that is not the case today. The currently suggested purpose values are:</p> <ul> <li>networking</li> <li>add-on</li> <li>data-service</li> <li>application</li> </ul>"},{"location":"design/#profile","title":"Profile","text":"<p>A profile expresses a desired configuration for a Kubernetes cluster. It is composed of</p> <ul> <li>An optional clusterspec. If specified, it allows the profile   to be used to create new clusters.   If absent, the profile can only be applied to existing clusters.</li> <li>A list of bundles specifying the configuration to apply onto the cluster   once it is operational</li> <li>An optional list of <code>values.yaml</code> settings for any Helm Chart type bundle   in the bundle list</li> </ul>"},{"location":"design/#cluster","title":"Cluster","text":""},{"location":"design/#cluster-specification-metadata","title":"Cluster Specification/ Metadata","text":"<p>A Cluster Specification contains desired settings when creating a new cluster. These settings are the values that define the shape and the configurations of the cluster.</p> <p>There is a difference in the cluster specification for gen1 (deprecated) and gen2 clusters. The main difference in these cluster specifications is that gen2 Cluster Specification allow users to deploy arbitrarily complex clusters using the full Cluster API feature set. This is also closer to the gitops and declarative style of cluster creation and gives users more control over the cluster that they deploy.</p> <p>The Cluster Specification of gen-2 clusrers is called the cluster template, which is described in detail here.</p> <p>A cluster template manifest consists of:</p> <ul> <li>A predefined list of Cluster API objects: Cluster, Machines, Machine Deployments, etc. to be deployed in the current namespace</li> <li>The specific infrastructure provider to be used (e.g aws).\u00df</li> <li>Kubernetes version</li> <li>Cluster templates/ flavors that need to be used for creating the cluster manifest (e.g eks, eks-managedmachinepool)</li> </ul>"},{"location":"design/#cluster-preparation","title":"Cluster Preparation","text":"<p>Once these cluster specifications are created successfully, the next step is to prepare the cluster for deployment.</p> <p>Once the cluster template manifest is created, the next step is to preare the workspace repository directory in which this cluster template manifest is present. This is explained in detail here</p>"},{"location":"design/#cluster-creation","title":"Cluster Creation","text":"<p>Now, all the prerequisites for creating a cluster are completed and the cluster can be created/deployed.</p>"},{"location":"design/#cluster-chart","title":"Cluster Chart","text":"<p>The cluster chart is a Helm chart that creates (and optionally applies) the manifests necessary to create a cluster and deploy desired configurations and applications to it as a part of cluster creation, the following resources are created: The profile's Cluster Specification, bundle list and other settings are used to generate values for the cluster chart, and the chart is deployed as a Helm release into the arlon namespace in the management cluster.</p> <p>Here is a summary of the kinds of resources generated and deployed by the chart:</p> <ul> <li>A unique namespace with a name based on the cluster's name. All subsequent   resources below are created inside that namespace.</li> <li>The stack-specific resources to create the cluster (for e.g. Cluster API resources)</li> <li>A ClusterRegistration to automatically register the cluster with ArgoCD</li> <li>A GitRepoDir to automatically create a git repo and/or directory to host a copy   of the expanded bundles. Every bundle referenced by the profile is   copied/unpacked into its own subdirectory.</li> <li>One ArgoCD Application resource for each bundle.</li> </ul> <p>cluster template creation is explained here</p>"},{"location":"dev_setup/","title":"TODO (Under construction)","text":""},{"location":"docs_help/","title":"Help with Arlon Documentation","text":""},{"location":"docs_help/#prerequisites","title":"Prerequisites","text":"<p>Documentation for Arlon is written in Markdown format, and generated using the mkdocs site generator. The theme mkdocs-material has been used, and docs from the main branch are published to a <code>GitHub pages</code> site.</p> <ul> <li>Install Python 3.x (3.8 or later recommended. 3.11 has been tested)</li> <li>Install the pip python package manager for Python 3.</li> <li>Install the Python modules mentioned in docs/requirements.txt</li> <li>Run <code>mkdocs serve</code> to test any local changes to docs, and commit them using git workflow</li> </ul>"},{"location":"docs_help/#help-with-mkdocs","title":"Help with MkDocs","text":"<p>Welcome to Arlon documentation with MkDocs</p> <p>For full documentation, visit mkdocs.org.</p>"},{"location":"docs_help/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"docs_help/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file. \n              # Contains the navigation structure \ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"e2e_testing/","title":"End to End Tests","text":""},{"location":"e2e_testing/#arlon-e2e-tests","title":"Arlon e2e tests","text":""},{"location":"e2e_testing/#setup","title":"Setup","text":"<ul> <li> <p>First, we run the <code>testing/e2e_setup.sh</code> script, which helps us setup a kind management cluster, a git-server gitea based workspace repository and installs services like argocd, arlon, capi in the management cluster. It also installs other required tools like kubectl, docker, kind, kuttl, clusterctl, helm and gitea.</p> </li> <li> <p>In addition to this, the script also creates a capi-eks cluster manifest which serves as the cluster template manifest. This cluster is created as a part of the e2e tests and is pushed to the workspace repository created in the previous step.</p> </li> <li> <p>This script also adds a  xenial bundle manifest to the workspace repository which is required for creating a xenial bundle and a corresponding profile which is consumed by the test.</p> </li> <li> <p>Any prerequisites for any e2e test must be added to this script or to a seperate script as a part of the setup for the e2e test. These scripts must be executed as a part of the e2e test setup before any additional arlon commands can be executed.</p> </li> </ul>"},{"location":"e2e_testing/#e2e-integration-tests-using-kuttl","title":"e2e integration tests using KUTTL","text":"<ul> <li> <p>We are using KUTTL to write e2e integration tests for arlon.</p> </li> <li> <p>KUTTL is a declarative integration testing harness for testing operators, KUDO, Helm charts, and any other Kubernetes applications or controllers.</p> </li> <li> <p>The KUTTL test CLI organizes tests into suites:</p> </li> <li>A \"test step\" defines a set of Kubernetes manifests to apply and a state to assert on (wait for or expect).</li> <li> <p>A \"test assert\" are the part of a test step that define the state to wait for Kubernetes to reach</p> </li> <li> <p>All the e2e tests are placed in <code>/testing/e2e</code> in the respecitive directory for the given test. For example, the test <code>00-deploy</code> is used to deploy a cluster and all the files related to this test are placed in <code>/testing/e2e/00-deploy</code>. To add a new e2e test here, create a new directory with the correct step index. The tests in <code>/testing/e2e</code> run in the order of the index specified.</p> </li> <li> <p>Each filename in the test case directory should start with an index (in this example 00) that indicates which test step the file is a part of. The first filename in the test case will begin with index 00, the second will have index 01 and so on. e.g In <code>/testing/e2e/00-deploy</code> we have <code>00-prepare.yaml</code>, <code>01-validate.yaml</code> etc.</p> </li> <li> <p>Do note that the tests will run in the order of the index specified. This is the case for both the test case directory and the individual test files within these directories. Files that do not start with a step index are ignored and can be used for documentation or other test data.</p> </li> <li> <p>As a part of the test step, we can run commands and execute scripts. Here, for arlon e2e tests, we execute the e2e_setup script as a part of the test step and then run the arlon commands specific to the test case.</p> </li> <li> <p>Once we have created a test step case, we can also create a test assert for a given filename. The assert's filename should be the test step index followed by <code>-assert.yaml</code> e.g. In <code>/testing/e2e/00-deploy</code> we have <code>02-assert.yaml</code>, which will run after <code>02-deploy.yaml</code> test step.</p> </li> <li> <p>In a test assert step, we look for the desired resource state. This is the state of the resource after the test step has finished. As a part of the test assert, we can add a timeout incase the resource that we are waiting on takes a long time to reach the desired state.</p> </li> <li> <p>All the test steps and test asserts run in order and each must be successful for the test case to be considered successful. If any test step or test assertion fails then the test will fail.</p> </li> </ul>"},{"location":"e2e_testing/#testbed-teardown","title":"Testbed Teardown","text":"<ul> <li> <p>Currently, as a part of the <code>/testing/e2e_setup_teardown.sh</code> script, we delete the kind management cluster, the cloned workspace repository, bundles, profiles and cluster manifests present in this repository.</p> </li> <li> <p>This script runs at the end of every e2e test run regardless of the success or failure of the test for cleaning up any resources that might have been created as a part of the test.</p> </li> <li> <p>Any additional resource that needs to be cleaned up post the e2e test should be added to this script.</p> </li> </ul>"},{"location":"e2e_testing/#e2e-tests-integration-with-github-actions","title":"e2e tests integration with Github Actions","text":"<ul> <li> <p>Currently, we are running the arlon e2e tests on an ubuntu VM using Github Actions.</p> </li> <li> <p>To invoke the arlon e2e tests, we are adding a new build target <code>make test-e2e</code> This command will execute the e2e test job using Github Actions.</p> </li> </ul>"},{"location":"gen1_profiles/","title":"Bundles and Profiles - Concepts","text":""},{"location":"gen1_profiles/#configuration-bundle","title":"Configuration bundle","text":"<p>A configuration bundle (or just \"bundle\") is grouping of data files that produce a set of Kubernetes manifests via a tool. This closely follows ArgoCD's definition of tool types. Consequently, the list of supported bundle types mirrors ArgoCD's supported set of manifest-producing tools. Each bundle is defined using a Kubernetes ConfigMap resource in the arlon namespace.</p>"},{"location":"gen1_profiles/#static-bundle","title":"Static bundle","text":"<p>A static bundle embeds the manifest's YAML data itself (\"static bundle\"). A cluster consuming a static bundle will always have a snapshot copy of the bundle at the time the cluster was created, and is not affected by subsequent changes to the bundle's manifest data.</p>"},{"location":"gen1_profiles/#dynamic-bundle","title":"Dynamic bundle","text":"<p>A dynamic bundle contains a reference to the manifest data stored in git. A dynamic bundle is distinguished by having these fields set to non-empty values:</p> <ul> <li>git URL of the repo</li> <li>Directory path within the repo</li> </ul> <p>The git URL must be registered in ArgoCD as a valid repository. The content of the specified directory can contain manifests in any of the tool formats supported by ArgoCD, including plain YAML, Helm and Kustomize.</p> <p>When the user updates a dynamic bundle in git, all clusters consuming that bundle (through a profile specified at cluster creation time) will acquire the change.</p>"},{"location":"gen1_profiles/#other-properties","title":"Other properties","text":"<p>A bundle can also have a comma-separated list of tags, and a description. Tags can be useful for classifying bundles, for e.g. by type (\"addon\", \"cni\", \"rbac\", \"app\").</p>"},{"location":"gen1_profiles/#profile","title":"Profile","text":"<p>A profile expresses a desired configuration for a Kubernetes cluster. It is just a set of references to bundles (static, dynamic, or a combination). A profile can be static or dynamic.</p>"},{"location":"gen1_profiles/#static-profile","title":"Static profile","text":"<p>When a cluster consumes a static profile at creation time, the set of bundles for the cluster is fixed at that time and does not change over time even when the static bundle is updated. (Note: the contents of some of those bundles referenced by the static profile may however change over time if they are dynamic). A static profile is stored as an item in the Arlon database (specifically, as a CR in the Management Cluster).</p>"},{"location":"gen1_profiles/#dynamic-profile","title":"Dynamic profile","text":"<p>A dynamic profile, on the other hand, has two components: the specification stored in the Arlon database, and a compiled component living in the workspace repository at a path specified by the user. (Note: this repository is usually the workspace repo, but it technically doesn't have to be, as long as it's a valid repo registered in ArgoCD) The compiled component is essentially a Helm chart of multiple ArgoCD app resources, each one pointing to a bundle. Arlon automatically creates and maintains the compiled component. When a user updates the composition of a dynamic profile, meaning redefines its bundle set, the Arlon library updates the compiled component to point to the bundles specified in the new set. Any cluster consuming that dynamic profile will be affected by the change, meaning it may lose or acquire new bundles in real time.</p>"},{"location":"gen2_Tutorial/","title":"Tutorial","text":"<p>This tutorial will go through the step by step process to create a Cluster Template and then deploy one or more AWS EKS Clusters using Arlon. </p> <p>Arlon allows you to create one or more workload clusters from a Cluster Template that you design and provide in the form of a Cluster API manifest file stored in a git directory. The manifest typically contains multiple related resources that together define an arbitrarily complex cluster.  If you make subsequent changes to the Cluster Template, workload clusters originally created from it will automatically acquire the changes.</p> <p>(NOTE: In earlier versions of arlon (before v0.10), Cluster Templates were called Base Clusters. Some parts of the code still refer to base cluster manifests. This should be considered as a synonym for cluster template manifests.)</p>"},{"location":"gen2_Tutorial/#pre-requisites","title":"Pre-requisites","text":"<p>Note: The CAPA version used here is v2.0 and the manifests created here are in accordance with this version.</p> <p>Refer the compatibility matrix for Cluster API provider and CAPA versions for supported versions.</p> <p>Before deploying a EKS cluster, make sure to setup the AWS Environment as stated in the quickstart guide for CAPI</p> <p>Please read here to understand the Cluster API concepts that we will use in rest of this Tutorial. </p>"},{"location":"gen2_Tutorial/#creating-cluster-api-cluster-manifest","title":"Creating Cluster-API Cluster Manifest","text":"<p>In this section, we will go through the steps to create a CAPI cluster manifest for the three different types of node pooling constructs that CAPI EKS clusters support.</p>"},{"location":"gen2_Tutorial/#machinedeployment","title":"MachineDeployment","text":"<p>Here is an example of a manifest file that we can use to create a Cluster Template. This manifest file helps in creating an EKS cluster with 'Machine Deployment' component from the Cluster API (CAPI). This file has been generated by the following command</p> <pre><code>clusterctl generate cluster capi-quickstart --flavor eks \\\n--kubernetes-version v1.24.0 \\\n--control-plane-machine-count=3 \\\n--worker-machine-count=3 \\\n&gt; capi-quickstart.yaml\n</code></pre> <pre><code># YAML\napiVersion: cluster.x-k8s.io/v1beta1\nkind: Cluster\nmetadata:\nname: capi-quickstart\nnamespace: default\nspec:\nclusterNetwork:\npods:\ncidrBlocks:\n- 192.168.0.0/16\ncontrolPlaneRef:\napiVersion: controlplane.cluster.x-k8s.io/v1beta2\nkind: AWSManagedControlPlane\nname: capi-quickstart-control-plane\ninfrastructureRef:\napiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSManagedCluster\nname: capi-quickstart\n---\napiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSManagedCluster\nmetadata:\nname: capi-quickstart\nspec: {}\n---\napiVersion: controlplane.cluster.x-k8s.io/v1beta2\nkind: AWSManagedControlPlane\nmetadata:\nname: capi-quickstart-control-plane\nnamespace: default\nspec:\nregion: {REGION}\nsshKeyName: {SSH_KEYNAME}\nversion: v1.24.0\n---\napiVersion: cluster.x-k8s.io/v1beta1\nkind: MachineDeployment\nmetadata:\nname: capi-quickstart-md-0\nnamespace: default\nspec:\nclusterName: capi-quickstart\nreplicas: 3\nselector:\nmatchLabels: null\ntemplate:\nspec:\nbootstrap:\nconfigRef:\napiVersion: bootstrap.cluster.x-k8s.io/v1beta2\nkind: EKSConfigTemplate\nname: capi-quickstart-md-0\nclusterName: capi-quickstart\ninfrastructureRef:\napiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSMachineTemplate\nname: capi-quickstart-md-0\nversion: v1.24.0\n---\napiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSMachineTemplate\nmetadata:\nname: capi-quickstart-md-0\nnamespace: default\nspec:\ntemplate:\nspec:\niamInstanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io\ninstanceType: {INSTANCE_TYPE}\nsshKeyName: {SSH_KEYNAME}\n---\napiVersion: bootstrap.cluster.x-k8s.io/v1beta2\nkind: EKSConfigTemplate\nmetadata:\nname: capi-quickstart-md-0\nnamespace: default\nspec:\ntemplate: {}\n</code></pre>"},{"location":"gen2_Tutorial/#awsmanagedmachinepool","title":"AWSManagedMachinePool","text":"<p>Initialize the environment for AWSManagedMachinePool as stated here</p> <p>Before deploying an EKS cluster, make sure that the MachinePool feature gate is enabled. To do so, run this command:</p> <pre><code>kubectl describe deployment capa-controller-manager -n capa-system\n</code></pre> <p>In the output, in the feature gates section of the deployment, MachinePool must be set to true.</p> <pre><code>&gt; kubectl describe deployment capa-controller-manager -n capa-system\n..........\n..........\n--featuregates=EKS=true,EKSEnableIAM=false,EKSAllowAddRoles=false,EKSFargate=true,MachinePool=true,EventBridgeInstanceState=false,\nAutoControllerIdentityCreator=true,BootstrapFormatIgnition=false,ExternalResourceGC=false\n..........\n..........\n</code></pre> <p>This manifest file helps in deploying an EKS cluster with 'AWSManagedMachinePool' component from the cluster API (CAPI). This file has been generated by the following command</p> <pre><code>clusterctl generate cluster awsmanaged-cluster --kubernetes-version v1.22.0 --flavor eks-managedmachinepool &gt; manifest.yaml\n</code></pre> <pre><code># YAML\napiVersion: cluster.x-k8s.io/v1beta1\nkind: Cluster\nmetadata:\nname: awsmanaged-cluster\nnamespace: default\nspec:\nclusterNetwork:\npods:\ncidrBlocks:\n- 192.168.0.0/16\ncontrolPlaneRef:\napiVersion: controlplane.cluster.x-k8s.io/v1beta2\nkind: AWSManagedControlPlane\nname: awsmanaged-cluster-control-plane\ninfrastructureRef:\napiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSManagedCluster\nname: awsmanaged-cluster\n---\napiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSManagedCluster\nmetadata:\nname: awsmanaged-cluster\nnamespace: default\nspec: {}\n---\napiVersion: controlplane.cluster.x-k8s.io/v1beta2\nkind: AWSManagedControlPlane\nmetadata:\nname: awsmanaged-cluster-control-plane\nnamespace: default\nspec:\nregion: {REGION}\nsshKeyName: {SSH_KEYNAME}\nversion: v1.22.0\n---\napiVersion: cluster.x-k8s.io/v1beta1\nkind: MachinePool\nmetadata:\nname: awsmanaged-cluster-pool-0\nnamespace: default\nspec:\nclusterName: awsmanaged-cluster\nreplicas: 1\ntemplate:\nspec:\nbootstrap:\ndataSecretName: \"\"\nclusterName: awsmanaged-cluster\ninfrastructureRef:\napiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSManagedMachinePool\nname: awsmanaged-cluster-pool-0\n---\napiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSManagedMachinePool\nmetadata:\nname: awsmanaged-cluster-pool-0\nnamespace: default\nspec: {}\n</code></pre>"},{"location":"gen2_Tutorial/#awsmachinepool","title":"AWSMachinePool","text":"<p>An AWSMachinePool corresponds to an AWS AutoScaling Groups, which provides the cloud provider specific resource for orchestrating a group of EC2 machines.</p> <p>Initialize the environment for AWSMachinePool as stated here</p> <p>Before deploying an EKS cluster, make sure that the AWSMachinePool feature gate is enabled. To do so, run this command:</p> <pre><code>kubectl describe deployment capa-controller-manager -n capa-system\n</code></pre> <p>In the output, in the feature gates section of the deployment, MachinePool must be set to true.</p> <pre><code>&gt; kubectl describe deployment capa-controller-manager -n capa-system\n..........\n..........\n--featuregates=EKS=true,EKSEnableIAM=false,EKSAllowAddRoles=false,EKSFargate=true,MachinePool=true,EventBridgeInstanceState=false,\nAutoControllerIdentityCreator=true,BootstrapFormatIgnition=false,ExternalResourceGC=false\n..........\n..........\n</code></pre> <p>This manifest file helps in deploying an EKS cluster with 'AWSManagedMachinePool' component from the cluster API (CAPI). This file has been generated by the following command</p> <pre><code>clusterctl generate cluster awsmanaged-cluster --kubernetes-version v1.22.0 --flavor eks-machinepool &gt; manifest.yaml\n</code></pre> <pre><code># YAML\napiVersion: cluster.x-k8s.io/v1beta1\nkind: Cluster\nmetadata:\nname: awsmanaged-cluster\nnamespace: default\nspec:\nclusterNetwork:\npods:\ncidrBlocks:\n- 192.168.0.0/16\ncontrolPlaneRef:\napiVersion: controlplane.cluster.x-k8s.io/v1beta2\nkind: AWSManagedControlPlane\nname: awsmanaged-cluster-control-plane\ninfrastructureRef:\napiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSManagedCluster\nname: awsmanaged-cluster\n---\napiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSManagedCluster\nmetadata:\nname: awsmanaged-cluster\nnamespace: default\nspec: {}\n---\napiVersion: controlplane.cluster.x-k8s.io/v1beta2\nkind: AWSManagedControlPlane\nmetadata:\nname: awsmanaged-cluster-control-plane\nnamespace: default\nspec:\nregion: {REGION}\nsshKeyName: {SSH_KEYNAME}\nversion: v1.22.0\n---\napiVersion: cluster.x-k8s.io/v1beta1\nkind: MachinePool\nmetadata:\nname: awsmanaged-cluster-mp-0\nnamespace: default\nspec:\nclusterName: awsmanaged-cluster\nreplicas: 1\ntemplate:\nspec:\nbootstrap:\nconfigRef:\napiVersion: bootstrap.cluster.x-k8s.io/v1beta2\nkind: EKSConfig\nname: awsmanaged-cluster-mp-0\nclusterName: awsmanaged-cluster\ninfrastructureRef:\napiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSMachinePool\nname: awsmanaged-cluster-mp-0\nversion: v1.22.0\n---\napiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSMachinePool\nmetadata:\nname: awsmanaged-cluster-mp-0\nnamespace: default\nspec:\nawsLaunchTemplate:\niamInstanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io\ninstanceType: {INSTANCE_TYPE}\nsshKeyName: {SSH_KEYNAME}\nmaxSize: 10\nminSize: 1\n---\napiVersion: bootstrap.cluster.x-k8s.io/v1beta2\nkind: EKSConfig\nmetadata:\nname: awsmanaged-cluster-mp-0\nnamespace: default\nspec: {}\n</code></pre>"},{"location":"gen2_Tutorial/#prepare-manifest","title":"Prepare Manifest","text":"<p>This manifest file needs to be pushed to the workspace repository before the manifest directory is prepped and then validated.</p> <p>Before a manifest directory can be used as a cluster template, it must first be \"prepared\" or \"prepped\" by Arlon. The \"prep\" phase makes minor changes to the directory and manifest to help Arlon deploy multiple copies of the cluster without naming conflicts.</p>"},{"location":"gen2_Tutorial/#manifest-directory-preparation","title":"manifest directory preparation","text":"<p>To prepare a git directory to serve as cluster template, use the <code>clustertemplate preparegit</code> command:</p> <pre><code>arlon clustertemplate preparegit --repo-url &lt;repoUrl&gt; --repo-path &lt;pathToDirectory&gt; [--repo-revision revision]\n# OR\n# using repository aliases\n# using the default alias\narlon clustertemplate preparegit --repo-path &lt;pathToDirectory&gt; [--repo-revision revision]\n# using the prod alias\narlon clustertemplate preparegit --repo-alias prod --repo-path &lt;pathToDirectory&gt; [--repo-revision revision]\n</code></pre>"},{"location":"gen2_Tutorial/#manifest-directory-validation","title":"manifest directory validation","text":"<p>Post the successful preparation of the cluster template manifest directory using <code>clustertemplate preparegit</code>, the cluster template manifest directory needs to be validated before the cluster template is created.</p> <p>To determine if a git directory is eligible to serve as cluster template, run the <code>clustertemplate validategit</code> command:</p> <pre><code>arlon clustertemplate validategit --repo-url &lt;repoUrl&gt; --repo-path &lt;pathToDirectory&gt; [--repo-revision revision]\n# OR\n# using repository aliases\n# using the default alias\narlon clustertemplate validategit --repo-path &lt;pathToDirectory&gt; [--repo-revision revision]\n# using the prod alias\narlon clustertemplate validategit --repo-alias prod --repo-path &lt;pathToDirectory&gt; [--repo-revision revision]\n</code></pre>"},{"location":"gen2_Tutorial/#create-cluster","title":"Create Cluster","text":"<p>To create a workload cluster from the Cluster Template:</p> <pre><code>arlon cluster create --cluster-name &lt;clusterName&gt; --repo-url &lt;repoUrl&gt; --repo-path &lt;pathToDirectory&gt; [--output-yaml] [--profile &lt;profileName&gt;] [--repo-revision &lt;repoRevision&gt;]\n# OR\n# using repository aliases\n# using the default alias\narlon cluster create --cluster-name &lt;clusterName&gt; --repo-path &lt;pathToDirectory&gt; [--output-yaml] [--profile &lt;profileName&gt;] [--repo-revision &lt;repoRevision&gt;]\n# using the prod alias\narlon cluster create --cluster-name &lt;clusterName&gt; --repo-alias prod --repo-path &lt;pathToDirectory&gt; [--output-yaml] [--profile &lt;profileName&gt;] [--repo-revision &lt;repoRevision&gt;]\n</code></pre>"},{"location":"gen2_Tutorial/#create-cluster-with-overrides","title":"Create Cluster with Overrides","text":"<p>We call the concept of constructing various clusters with patches from the same base manifest as cluster overrides. The cluster overrides feature is built on top of the existing cluster template design. So, a user can create a cluster from the base manifest using the same command specified in the section above. Now, to create a cluster with overrides in the base manifest, a user should have the corresponding patch files in a single yaml file in local. Here is an example of a patch file where we want to override replicas count to 2 and change the sshkeyname:</p> <pre><code>---\napiVersion: cluster.x-k8s.io/v1beta1\nkind: MachineDeployment\nmetadata:\n  name: capi-quickstart-eks-md-0\nspec:\n  replicas: 2\n---\napiVersion: controlplane.cluster.x-k8s.io/v1beta1\nkind: AWSManagedControlPlane\nmetadata:\n  name: capi-quickstart-eks-control-plane\nspec:\n  sshKeyName: random\n---\n\n  ```\n\nNote: The metadata field in the patch file should be same as of the metadata field in the resources file.\n\nRefer to this [document](https://blog.scottlowe.org/2019/11/12/using-kustomize-with-cluster-api-manifests/) to know more about patch files\n\nCommand to create a workload cluster from the Cluster Template manifest with overrides to the manifest is:\n\n```shell\narlon cluster create &lt;cluster-name&gt; --repo-url &lt;repo url where cluster template is present&gt; --repo-path &lt;repo path to the cluster template&gt; --overrides-path &lt;path to the patch file&gt; --patch-repo-url &lt;repo url where patch file should be stored&gt; --patch-repo-path &lt;repo path to store the patch files&gt;\n````\n\nRunnning the above command will create a folder with the specified cluster name in patch repo path of patch repo url which contains the patch files, kustomization.yaml and configurations.yaml which are used to create the cluster.\n\nNote that the patch file repo url can be different or same from the cluster template repo url acoording to the requirement of the user. A user can use a different repo url for string patch files for the cluster.\n\n## Update Cluster\n\nTo update the profiles of a workload cluster:\n\n```shell\n# To add a new profile to the existing cluster\narlon cluster ngupdate &lt;clustername&gt; --profile &lt;profilename&gt;\n# To delete an existing profile from the existing cluster\narlon cluster ngupdate &lt;clustername&gt; --delete-profile\n</code></pre> <p>A cluster can be created without any profile associated with the cluster. So, the above commands can be used to add a new profile to the existing cluster which will create profile app in argocd along with bundle apps associated with the profile.</p> <p>An existing profile can be deleted from the cluster as well using the above command. Executing this command will delete the profile app and all the bundles associated with the profile in argocd.</p>"},{"location":"gen2_Tutorial/#delete-cluster","title":"Delete Cluster","text":"<p>To destroy a workload cluster:</p> <pre><code>arlon cluster delete &lt;clusterName&gt;\n</code></pre> <p>Arlon creates between 2 and 3 ArgoCD application resources to compose a gen2 cluster (the 3rd application, called \"profile app\", is used when an optional profile is specified at cluster creation time). When you destroy a gen2 cluster, Arlon will find all related ArgoCD applications and clean them up.</p> <p>If the cluster which which is being deleted is a cluster created using patch files, the controller first cleans the git repo where the respective patch files of the cluster are present and then it destroys all the related ArgoCD applications and clean them up.</p>"},{"location":"gen2_Tutorial/#enabling-cluster-autoscaler-in-the-workload-cluster","title":"Enabling Cluster Autoscaler in the workload cluster","text":"<p>To create a cluster with autoscaler, we need:</p> <ul> <li>bundle pointing to the bundle/capi-cluster-autoscaler in the arlon repository.</li> <li>dynamic profile that contains the above bundle.</li> <li>a cluster template manifest(that makes use of MachineDeployment and not MachinePools) which has the CAPI annotations for min and max nodes set ( as a part of <code>preparegit</code> or manually add it ).</li> <li>run <code>arlon cluster create</code> with the repo-path pointing to the cluster template manifest described in the step above, set the profile to  be the one created in step 2 and pass the <code>autoscaler</code> flag.</li> </ul>"},{"location":"gen2_Tutorial/#bundle-creation","title":"Bundle creation","text":"<p>Register a dynamic bundle pointing to the bundles/capi-cluster-autoscaler in the Arlon repo.</p> <p>To enable the cluster-autoscaler bundle, add one more parameter during cluster creation: <code>srcType</code>. This is the ArgoCD-defined application source type (Helm, Kustomize, Directory). In addition to this, the <code>repo-revision</code> parameter should also be set to a stable arlon release branch ( in this case v0.10 ).</p> <p>This example creates a bundle pointing to the bundles/capi-cluster-autoscaler in Arlon repo</p> <pre><code>arlon bundle create cas-bundle --tags cas,devel,test --desc \"CAS Bundle\" --repo-url https://github.com/arlonproj/arlon.git --repo-path bundles/capi-cluster-autoscaler --srctype helm --repo-revision v0.10\n</code></pre>"},{"location":"gen2_Tutorial/#profile-creation","title":"Profile creation","text":"<p>Create a profile that contains this capi-cluster-autoscaler bundle.</p> <pre><code>arlon profile create dynamic-cas --repo-url &lt;repoUrl&gt; --repo-base-path profiles --bundles cas-bundle --desc \"dynamic cas profile\" --tags examples\n</code></pre>"},{"location":"gen2_Tutorial/#manifest-directory-preparation_1","title":"manifest directory preparation","text":"<p>Two additional properties <code>cas-max</code> and <code>cas-min</code> are used to set 2 annotations for Max/Min nodes on MachineDeployment required by the cluster autoscaler for CAPI as a part of the manifest directory preparation. These are the annotations required by the MachineDeployment for autoscaling. </p> <p>Note: These are the default values for the <code>cas-min</code> and <code>cas-max</code> properties</p> <pre><code> annotations:\n     cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: '1'\ncluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: '9'\n</code></pre> <p>These annotations are added during the preparegit step. If these annotations are already present in the manifest file, then they will not be added again as a part of <code>preparegit</code></p> <p>Preparing the git directory to serve as the cluster template:</p> <pre><code>arlon clustertemplate preparegit --repo-path &lt;pathToDirectory&gt; --cas-min 1 --cas-max 9 --repo-url &lt;repoUrl&gt; </code></pre>"},{"location":"gen2_Tutorial/#manifest-directory-validation_1","title":"manifest directory validation","text":"<p>To determine if a git directory is eligible to serve as cluster template, run the <code>clustertemplate validategit</code> command:</p> <pre><code>arlon clustertemplate validategit --repo-path &lt;pathToDirectory&gt; --repo-url &lt;repoUrl&gt; </code></pre>"},{"location":"gen2_Tutorial/#cluster-creation-with-autoscaling-enabled","title":"Cluster creation with autoscaling enabled","text":"<p>To add CAS support for gen2 clusters, the cluster create sub-command of the arlon CLI has a <code>autoscaler</code> flag which deploys the capi-cluster-autoscaler helm chart on the management cluster.</p> <p>To create a gen2 workload with a cluster-autoscaler pod running, from the cluster template, run this command:</p> <p>Note: Use the dynamic profile that was created in the previous steps</p> <pre><code>arlon cluster create --cluster-name &lt;clusterName&gt; --repo-url &lt;repoUrl&gt; --repo-path &lt;pathToDirectory&gt; --profile dynamic-cas --autoscaler\n</code></pre>"},{"location":"gen2_Tutorial/#known-issues-and-limitations","title":"Known issues and limitations","text":"<p>Gen2 clusters are powerful because the cluster template can be arbitrarily complex and feature rich. Since they are fairly new and still evolving, gen2 clusters have several known limitations relative to gen1.</p> <ul> <li>You cannot customize/override any property of the cluster template on the fly when creating a workload cluster, which is an exact clone of the cluster template except for the names of its resources and their namespace. The work-around is to make a copy of the cluster template directory, push the new directory, make the desired changes, commit &amp; push the changes, and register the directory as a new cluster template.</li> <li>The clusters created directly from the cluster template are completely declarative whereas the clusters which are created using override property are not completely declarative.</li> <li>If a user passes a different repository for patch repo url from the repo where cluster template is present, argocd won't be able to detect if there are any changes in the cluster template repository but will deect all the chnages in patch repo url for the cluster.</li> <li>If you modify and commit a change to one or more properties of the cluster template that the underlying Cluster API provider deems as \"immutable\", new   workload clusters created from the cluster template will have the modified propert(ies), but ArgoCD will flag existing clusters as OutOfSync, since   the provider will continually reject attempts to apply the new property values. The existing clusters continue to function, despite appearing unhealthy   in the ArgoCD UI and CLI outputs.</li> </ul> <p>Examples of mutable properties in Cluster API resources:</p> <ul> <li>Number of replicas (modification will result in a scale-up / down)</li> <li>Kubernetes version (modification will result in an upgrade)</li> </ul> <p>Examples of immutable properties:</p> <ul> <li>Most fields of AWSMachineTemplate (instance type, labels, etc...)</li> </ul>"},{"location":"gen2_Tutorial/#for-more-information","title":"For more information","text":"<p>For more details on gen2 clusters, refer to the design document.</p>"},{"location":"gen2_Tutorial/#arlon-cli-version-information","title":"Arlon CLI Version information","text":"<p>The Arlon CLI has a version command which shows the current version of the CLI as shown below.</p> <pre><code>arlon version \nArlon CLI Version: 0.10.0\n</code></pre> <p>The CLI can be downloaded from GitHub releases with the latest release of the CLI found here. The official releases follow a semver versioning scheme. All patches to a minor version are non-breaking changes. For instance, the transition from <code>v0.9.9</code> to <code>v0.9.10</code> will not introduce any breaking changes. However, a change in either minor or the major versions doesn't guarantee this.</p> <p>To check if there are patches available for the current minor version of the CLI, Arlon has a <code>version check</code> command. If upgrade patch for a CLI minor version is available, the command will request to download the newer version. For example: <pre><code>arlon version check\nArlon CLI version 0.10.0 is outdated. New patch 0.10.1 available\n</code></pre> Otherwise, the CLI reports that it is up-to date. <pre><code>arlon version check\n\"Arlon CLI version 0.10.0 is up-to-date\n</code></pre></p>"},{"location":"gen2_overrides_proposal_1/","title":"Gen2 Cluster Overrides - Proposal 1","text":"<p>This is a design proposal doc for gen2 cluster overrides. Right now, according to our gen2 design, we can deploy multiple clusters with same specifications from one cluster template. But what if we want to deploy cluster with a different sshkeyname from the same manifest?. To allow deploying clusters with different specifications from the same cluster template we are introducing the concept of clusteroverrides. So, clusteroverrides is being able to deploy clusters with different specs using same manifest and overriding the specs which we want to change.</p> <p>We have 2 different approches to override in a cluster:</p> <ol> <li>Converting the git repo where cluster template is present to helm charts</li> <li>Overriding specifies fields using kustomize</li> </ol> <p>In the first approach, We first let user upload the cluster template in the repo and deploy the cluster from it and then convert it into helm chart, so that we will be able to override fields in the manifest. The downside of this approach is that we don't have a specific template for cluster template, a user can use any form of the template in which case we will not be able to convert the manifest to heml chart.</p> <p>So, continuing with the 2nd approach, in the kustomize approach, we create an overlay folder parallel to the cluster template folder which contains folders named with the cluster name. These cluster named folders contain the specific override files to the cluster. An example of the folder structure is as belows:</p>"},{"location":"gen2_overrides_proposal_1/#a-directory-layout","title":"A directory layout","text":"<pre><code>.\n\u251c\u2500\u2500 ClusterTemplate              # ClusterTemplate folder(Contains cluster template)\n\u251c\u2500\u2500 Overlays                     # Contains folders specific to each cluster created from cluster template\n    \u251c\u2500\u2500 Cluster1                 # Contains overrides corresponding to cluster1 \n    \u251c\u2500\u2500 Cluster2                 # Contains overrides corresponding to cluster2\n</code></pre>"},{"location":"gen2_overrides_proposal_1/#lets-consider-an-example-case-to-understand-the-kustomize-approach","title":"Let's consider an example case to understand the kustomize approach","text":"<ol> <li> <p>Let's consider three different clusters on AWS. The management cluster already exists.</p> </li> <li> <p>Two of these clusters will run in the AWS \u201cus-west-2\u201d region, while the third will run in the \u201cus-east-2\u201d region.</p> </li> <li> <p>One of the two \u201cus-west-2\u201d clusters will use larger instance types to accommodate more resource-intensive workloads.</p> </li> </ol> <p>Now, we need to get our gitrepo ready by pushing the cluster template manifest into a folder named clustertemplate and for overriding, we need to create a folder for each cluster with the cluster name and place them in overlays folder which is parallel to the clustertemplate folder</p>"},{"location":"gen2_overrides_proposal_1/#setting-up-the-directory-structure","title":"Setting up the Directory Structure","text":"<p>To accommodate this use case, we will need to use a directory structure that supports the use of kustomize overlays. Therefore, the directory structure would look like this for the project:</p> <pre><code>(parent)\n|- clustertemplate\n|- overlays\n    |- usw2-cluster1\n    |- usw2-cluster2\n    |- use2-cluster1\n</code></pre> <p>The clustertemplate directory will store the cluster template for the final Cluster API manifests, as well as a kustomization.yaml file that identifies these Cluster API manifests as resources for kustomize to use.</p> <p>The contents of kustomize file in clustertemplate folder is as follows:</p> <pre><code>resources:\n- clustertemplatemanifest.yaml\n</code></pre> <p>The kustomization.yaml states that the resources for cluster is in cluster templatemanifest.yaml file</p>"},{"location":"gen2_overrides_proposal_1/#what-consists-in-the-cluster-named-folders","title":"What consists in the cluster named folders?","text":"<p>The intriguing parts begin with the overlays. You will need to provide the cluster-specific patches kustomize will use to generate the YAML for each cluster in its own directory under the overlays directory.</p> <p>With the \"usw2-cluster1\" overlay, let's begin. You must first comprehend what modifications must be made to the basic configuration in order to develop the appropriate configuration for this specific cluster in order to grasp what will be required.</p> <p>We can use two methods for patches 1. JSON patches 2. YAML patches</p> <p>In JSON patches, we have to write a JSON file to replace fields in the manifests. So, we need to write a different file for each replace and that would become hectic. </p> <pre><code>Example of a JSON patch:\n[\n{ \"op\": \"replace\",\n    \"path\": \"/metadata/name\",\n    \"value\": \"usw2-cluster-1\" },\n{ \"op\": \"replace\",\n    \"path\": \"/spec/infrastructureRef/name\",\n    \"value\": \"usw2-cluster-1\" }\n]\n</code></pre> <p>So, let's discuss the YAML approach which will be much easier to handle the overrides</p> <pre><code>---\napiVersion: cluster.x-k8s.io/v1alpha2\nkind: Machine\nmetadata:\nname: .*\nlabels:\n    cluster.x-k8s.io/cluster-name: \"usw2-cluster1\"\n</code></pre> <p>This will add a cluster name field to label in the manifest which is an advantage over JSON approach. We can both add and replace fields in manifest unlike just just replace in JSON aproach.</p> <p>You would once more require a reference to the patch file in kustomization for this last example. Both the patch file itself and kustomization.yaml</p> <p>This kustomization.yaml file will be pointing to both the cluster template manifest and patch file basically working like a link between both the cluster and manifest. </p> <p>Using this particular approach the present cluster template approach will need to take a redesign as we will need to skip the name suffix method we using before to create a manifest for each cluster respectively with their own names.</p> <p>In this approach, Instead of the configurations.yaml(Needed for name suffix), we will have a folder for each cluster and argocd path pointing to the cluster folder. This will help us in skipping the name suffix method we were using before.</p> <p>We will be able to basically override any of the field in manifest without any limitations before creating a cluster using this approach.</p>"},{"location":"gen2_overrides_proposal_1/#uxuser-experience","title":"UX(User experience):","text":"<p>To provide a user the freedom to completely override any part of the cluster template manifest, we ask the user to point to a yaml file in which the fields have been overridden. </p> <p>This would be easier to user as well because he/she would generate the manifest file anyway. So, they need to make changes to the already generated and point it. </p> <p>But we should even take care of the point that the cluster template manifest in the git and the overriden manifest file are comparable. Example of a command:</p> <p><code>arlon cluster create &lt;cluster name&gt; --repo-url &lt;repo url&gt; --repo-path &lt;repo path&gt; --overrides &lt;path to overriden manifest file&gt;</code></p>"},{"location":"gen2_overrides_proposal_1/#limitations","title":"Limitations:","text":"<ul> <li>Manifests (clustertemplate and overlays) for the clustertemplate cluster as well as workload clusters reside in the same repository. This means those who create the workload cluster will need write access to the clustertemplate repository which might not be the case in enterprises.</li> <li> <p>So, if we consider having the manifests (clustertemplate and overlays) are in different repositories, they will need a link to each other and as of now, if we update the cluster template while having manifest in one repo and patches in another repo. Argocd will not be able to take up the updated changes in cluster template</p> </li> <li> <p>The main goal of gen2 clusters was to remove the dependency on git to store metadata and make the clusters completely declarative unlike gen1 clusters. But here, we re-introduce a dependency on Arlon API (library) and git (state in git with dir structure)</p> </li> <li> <p>Although we can make this approach declarative by introducing another controller (CRD), this would increase the whole complexity of the issue and arlon.</p> </li> <li> <p>Using this approach, we might not be able to prefix a name in the cluster template manifest which is an issue because, Some resources generate external resources, like AWS load balancer and we need to avoid naming conflict - hence name prefix (not sufficient) + name reference in gen2 is required</p> </li> </ul>"},{"location":"gen2_overrides_proposal_2/","title":"Gen2 overrides proposal 2","text":""},{"location":"gen2_overrides_proposal_2/#gen2-cluster-overrides-proposal-2","title":"Gen2 cluster overrides - Proposal 2","text":"<p>This is an update to proposal of Gen2 cluster overrides proposal design to add the ability to override the Gen 2 cluster. This update doc has a more clear version of the proposal.</p>"},{"location":"gen2_overrides_proposal_2/#broader-idea-of-the-proposal","title":"Broader idea of the proposal","text":"<ul> <li>Basically, in the cluster overrides we want to generate different clusters with patches from the same cluster template. So, considering this feature in enterprise scope, the user who is uploading the cluster template in git might be an admin and another employee who wants to create a cluster form the cluster template might not have access to the git repo where the cluster template is present. </li> <li>So, In this approach a user can use a different git repository to store the patches of a manifest and create the cluster from the cluster template which is in different repository.</li> <li>Let's consider an example where our manifest is a repo called arlon-bc and with the repo path bc1. So, these are the files which will be present in bc1 folder:</li> <li>Resouces file which is the cluster template</li> <li> <p>Kustomization.yaml</p> <p>Contents of the kustomization.yaml file are as follows:</p> <pre><code>resources:\n- capi-quickstart-eks.yaml\n</code></pre> <p>In this case capi-quickstart-eks.yaml is the name of the cluster template.</p> </li> <li> <p>Now, let's say that our patch file is in another repository. This repository should caontain a folder named with the cluster's name and the files inside the cluster named folder are:</p> </li> <li> <p>configurations.yaml         configurations.yaml file corresponds to name suffix addition to the yaml file.</p> </li> <li>kustomization.yaml         kustomization.yaml file contains fields for resouces, configurations and patches as shown in the example below:         <pre><code>---\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n- git::https://github.com/jayanth-tjvrr/arlon-bc//bc1\nconfigurations:\n- configurations.yaml\npatches:\n- target:\n    group: controlplane.cluster.x-k8s.io\n    version: v1beta1\n    kind: MachineDeployment\n    path: md-details.yaml\n</code></pre></li> <li> <p>patch files         These patch files contain the patches which we want to include for the cluster</p> </li> <li> <p>As we can observe in the above example, the resource field in kustomization.yaml is pointing to a different repository which contains our cluster template.</p> </li> <li> <p>This is how we first organize our repositories to deploy a cluster with patches</p> </li> <li>Once, we organize the clusters, while creating the argocd app, the path of the argocd app should point to the cluster named folder. The code will then run kustomize build in the patch file directory and that will produce our final manifest.</li> <li>A user can just print out the manifest without letting the code apply it. This makes this approach declarative. At the same time, a user can even opt to apply the manifest to argocd app from the code.</li> <li>This is a clear proposal of the approach which we will be following for Gen2 cluster overrides.</li> </ul>"},{"location":"gen2_overrides_proposal_2/#limitations","title":"Limitations:","text":"<ul> <li>Since, we are pointing to the repository where our patch file is present, the argocd won't be able to detect the changes in the cluster template repository. -</li> <li>This can be an added feature aw well in one way because whenever there is a change in the cluster template configuration, it won't be immediately picked up. This brings user the ability to promote cluster template changes sensibly through each of our environments.</li> <li>Clean up of the cluster named folders when a user deletes a cluster is one of the issue which is still being addressed and looked up on.</li> </ul>"},{"location":"gen2_overrides_proposal_2/#ux-user-experience","title":"UX (User experience):","text":"<p>A user can pass the patch file for the cluster as an argument while executing the <code>arlon cluster create ..</code> command with the --override flag. The command would look like:</p> <p><code>arlon cluster create &lt;cluster-name&gt; --repo-url &lt;repo url where patch files should be present&gt; --repo-path &lt;repo path to cluster named folder&gt; --override &lt;path to the patch file&gt;</code> </p>"},{"location":"gen2_overrides_proposal_3/","title":"Gen2 overrides proposal 3","text":""},{"location":"gen2_overrides_proposal_3/#gen2-cluster-overrides-final-proposal","title":"Gen2 cluster overrides - final proposal","text":"<p>This is an update to proposal of Gen2 cluster overrides proposal-2 design to add the ability to override the Gen 2 cluster. This update doc has the final version of clusterride feature.</p>"},{"location":"gen2_overrides_proposal_3/#design-of-overrides-feature-for-gen2-cluster","title":"Design of overrides feature for gen2 cluster","text":"<ul> <li>Basically, we want to construct various clusters with patches from the same cluster template in the cluster overrides. The person uploading the cluster template in git may be an administrator, and another employee who wants to construct a cluster from the cluster template may not have access to the git repository where the cluster template is located. This is because the capability is intended to be used in an enterprise setting.</li> <li>The cluster overrides feature is built on top of the existing cluster template design. So, there won't be any changes in the design of cluster template folder.</li> <li>Let's consider an example where our manifest is a repo called arlon-bc and with the repo path bc1. So, these are the files which will be present in bc1 folder:</li> <li>Resouces file which is the cluster template</li> <li>Kustomization.yaml</li> <li> <p>configurations.yaml</p> <p>Contents of the kustomization.yaml file are as follows:</p> <pre><code>resources:\n- capi-quickstart-eks.yaml\nconfigurations:\n- configurations.yaml\n</code></pre> <p>In this case capi-quickstart-eks.yaml is the name of the cluster template.</p> </li> <li> <p>Now, let's say that our patch file is in another repository. This repository should caontain a folder named with the cluster's name and the files inside the cluster named folder are:</p> </li> <li> <p>configurations.yaml         configurations.yaml file corresponds to name suffix addition to the yaml file.</p> </li> <li>kustomization.yaml         kustomization.yaml file contains fields for resouces, configurations and patches as shown in the example below:         <pre><code>---\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n- git::https://github.com/jayanth-tjvrr/arlon-bc//bc1\nconfigurations:\n- configurations.yaml\npatches:\n- target:\n    group: controlplane.cluster.x-k8s.io\n    version: v1beta1\n    kind: MachineDeployment\n    path: md-details.yaml\n</code></pre></li> <li>patch files         These patch files contain the patches which we want to include for the cluster</li> <li> <p>As we can observe in the above example, the resource field in kustomization.yaml is pointing to a different repository which contains our cluster template. Exampple of how a patch file looks like:     <code>---     apiVersion: cluster.x-k8s.io/v1beta1     kind: MachineDeployment      metadata:     name: .*     spec:     replicas: 2</code></p> </li> <li> <p>This is how we first organize our repositories to deploy a cluster with patches</p> </li> <li>When we structure the clusters, the path of the resulting argocd app should direct users to the cluster-specific folder. After that, the code will launch kustomize build in the patch file directory, which will result in the creation of our final manifest.</li> <li>The manifest can be printed out directly by the user without having to go through the code. This technique is declarative as a result. A user can choose to apply the manifest to the Argocd app directly from the code at the same time.</li> <li>The above patch file directory structure is only applicable to clusters which have patch files associated with them. Other clusters which are built from the cluster template directly with no patch files attached won't have any directory created anywhere in git.</li> <li>When a user deletes a cluster which has patch files associated with it. The patch files get cleaned up from the repository as well.</li> <li>This is a clear proposal of the approach which we will be following for Gen2 cluster overrides.</li> </ul>"},{"location":"gen2_overrides_proposal_3/#limitations","title":"Limitations:","text":"<ul> <li>Since, we are pointing to the repository where our patch file is present, the argocd won't be able to detect the changes in the cluster template repository. </li> <li>This can be an added feature aw well in one way because whenever there is a change in the cluster template configuration, it won't be immediately picked up. This brings user the ability to promote cluster template changes sensibly through each of our environments.</li> <li>The cluster created using overrides approach are not completely declarative.</li> </ul>"},{"location":"gen2_overrides_proposal_3/#ux-user-experience","title":"UX (User experience):","text":"<p>A user can pass the patch file for the cluster as an argument while executing the <code>arlon cluster create ..</code> command with the --override flag. The command would look like:</p> <p><code>arlon cluster create &lt;cluster-name&gt; --repo-url &lt;repo url where cluster template is present&gt; --repo-path &lt;repo path to the cluster template&gt; --override &lt;path to the patch file&gt; --patch-repo-url &lt;repo url where patch file should be stored&gt; --patch-repo-path &lt;repo path to store the patch file&gt;</code> </p> <p>The above command will create a cluster named folder in patch repo url which contains all the patch files and the argocd app created for the respective cluster will be pointing to the cluster named folder which has been created.</p> <p>To delete a cluster, a user can follow the same command as usual,</p> <p><code>arlon cluster delete &lt;cluster-name&gt;</code></p> <p>This command checks if the cluster is overriden and if it is overrides, then the code first deletes the associated cluster named folder and then it deletes the argocd app.</p>"},{"location":"gen2_profiles_proposal_1/","title":"Gen2 Profiles - Proposal 1","text":"<p>In this design proposal, gen2 profiles are completely built on native ArgoCD ApplicationSets and resource labels. There are no first-class Arlon objects.</p>"},{"location":"gen2_profiles_proposal_1/#object-model","title":"Object model","text":"<ul> <li> <p>Arlon Application: a thin wrapper around an ApplicationSet.   An ApplicationSet is an Arlon Application if it has the <code>managed-by=arlon</code> label.</p> </li> <li> <p>Profile name: any unique label value that appears in the <code>spec.generators[0].clusters.selector.matchExpressions.values[]</code>   array of at least one Arlon application.</p> </li> <li> <p>Cluster: any cluster registered in ArgoCD. Not limited to clusters created by Arlon.</p> </li> </ul> <p>Observations: - A profile can be associated with any number of applications. And an application can be associated with multiple profiles. - A cluster is said to be associated with a profile if it is labeled with <code>arlon.io/profile=&lt;profileName&gt;</code>. A cluster can be associated with at most one profile. A profile may be associated (attached to) any number of clusters.</p>"},{"location":"gen2_profiles_proposal_1/#usage","title":"Usage","text":""},{"location":"gen2_profiles_proposal_1/#managing-applications","title":"Managing applications","text":"<p><code>arlon app list</code> shows the current list of Arlon applications. It is initially empty.</p> <p>The prototype does not currently support direct Arlon application creation. (This is easy to add later as a new command) An Arlon app has to be created manually by one of these methods: - Create a new ApplicationSet from a YAML file with   - The <code>managed-by=arlon</code> label   - The spec as follows: <pre><code>spec:\n  generators:\n  - clusters:\n      selector:\n        matchExpressions:\n        - key: arlon.io/profile\n          operator: In\n          values: []\n</code></pre> - Modify an existing ApplicationSet with the above requirements</p>"},{"location":"gen2_profiles_proposal_1/#managing-profiles","title":"Managing profiles","text":"<p>Profiles are not first class objects. They only exist as labels referenced by applications and placed on clusters. If a particular profile label value is not referenced from any application, it does not exist.</p> <p><code>arlon ngprofile list</code> shows the current list of profiles, the applications associated with each profile, and the clusters currently using each profile. The list is constructed by scanning all applications and determining the unique set of labels referenced in the <code>matchExpressions.values[]</code> array of each.</p> <p>To create a profile that doesn't exist yet, it needs to be added to at least one application's label set. This is conceptually achieved by \"adding the app to the profile\":</p> <p><code>arlon app addtoprofile &lt;appName&gt; &lt;profileName&gt;</code></p> <p>Conversely, a profile label can be removed from an application by \"removing the app from the profile\":</p> <p><code>arlon app removefromprofile &lt;appname&gt; &lt;profileName&gt;</code></p> <p>Caution: this can cause the profile to cease to exist if that was the last app referencing it.</p>"},{"location":"gen2_profiles_proposal_1/#associating-profiles-with-clusters","title":"Associating profiles with clusters","text":"<p>A cluster can have at most one profile attached to it. To attach a profile to a cluster:</p> <p><code>arlon nprofile attach &lt;profilename&gt; &lt;clustername&gt;</code></p> <p>Similarly, to detach:</p> <p><code>arlon nprofile detach &lt;profilename&gt; &lt;clustername&gt;</code></p> <p>Internally, an attach operation simply labels the cluster (via ArgoCD API) with the <code>arlon.io/profile=&lt;profileName&gt;</code> key value pair.</p>"},{"location":"gen2_profiles_proposal_1/#discussion","title":"Discussion","text":"<p>Pros of the design: * Lightweight, elegant, simple * Fully declarative (no new resources introduced, relies entirely on existing ArgoCD resources) * Does not require \"workspace git repo\" since a profile has no compiled component.</p> <p>Cons: * Profiles are not first class objects. A profile can cease to exist if it   becomes unreferenced from any application. This can be confusing to users.   For the same reason, you can't create an empty profile and add apps to it later.   This can be alleviated by clearly documenting the fact that profiles are just label values.   Once the user understand this, everything will become clearer, and the simplicity of   the design can begin to outweigh its quirks. * A cluster can only have one gen2 profile attached to it. This is a result   of the limited expressiveness of the <code>matchExpressions</code> logic.   In contrast, any number of gen1 profiles can be attached to a cluster   (the current implementation only allows one, but could be enhanced to allow many) * It's impossible to specify per-cluster overrides for an application.   That's because an ApplicationSet can be deployed to multiple clusters if   they have a matching profile label.   (To be fair, neither gen1 profiles nor gen2 clusters support cluster overrides either, but for a different reason. This is tracked in a github issue) * Any limitations of ApplicationSets (for e.g. lack of Sync Wave support?) will apply to Arlon Apps using gen2 profiles. * The lightweight nature of this design may cause some to perceive Arlon's   contribution to be very minimal (it's a thin wrapper around ArgoCD constructs). * Relies on ApplicationSet, which is ArgoCD specific, making it harder to port Arlon   to other gitops tools in the future, e.g. Flux (Trilok mentioned this, though it's not a strong concern at this point, given how invested we already are in ArgoCD)</p>"},{"location":"gen2_profiles_proposal_1/#potential-solutions-to-the-profiles-are-not-firstclass-objects-issue","title":"Potential solutions to the profiles-are-not-firstclass-objects issue:","text":""},{"location":"gen2_profiles_proposal_1/#the-null-app","title":"The Null App","text":"<p>The Null App (NA) is an Arlon app (applicationset) that belongs to (is associated with) all profiles. Arlon ensures that the null app always exists and maintains the above invariant. When deployed to a cluster, the NA does not change the cluster state, so it's a no-OP. A possible implementation is to make the NA deploy the \"default\" namespace, which already exists in all (most?) clusters.</p> <p>Arlon CLI commands (and possibly APIs) will filter out the NA and automatically create and update it as necessary, so the user doesn't see it in practice.</p> <ul> <li>The NA gets all profile labels, meaning all profiles \"contain\" the null app.</li> <li>A user can now create an empty profile. Internally, it is added to the NA's label list.</li> <li>When a profile is attached to any cluster, that cluster automatically \"gets\" the NA (since it's in all profiles), in addition to any other apps associated with the profile.</li> <li>When an app is \"added\" to a profile, meaning the profile is added to the app's labels list, the profile may not previously exist, therefore the profile is also added to the null app's label list. Therefore, when an app is added to a profile, two apps are modified.</li> <li>When an app is \"removed\" from a profile, meaning the profile is removed from the app's labels list, no change is made to the null app, therefore the profile remains in the null app's label list. (Actually, this behavior must change to support inconsistent states, see \"declarative installation ...\" section below.</li> </ul>"},{"location":"gen2_profiles_proposal_1/#lifecycle-operations-on-profiles","title":"Lifecycle operations on profiles","text":"<ul> <li> <p>With the presence of the null app, profiles can appear to be first class objects with defined lifecycle operations.</p> </li> <li> <p>Creating an empty profile: a profile is \"created\" by adding its name to the null app's label list. If it already exists in the null app's list, the app is unmodified. If it already existed in another app's list, then that's fine too. That app is not modified either. At the end of this operation, which is idempotent, the profile is guaranteed to exist in at least one app.</p> </li> <li> <p>Deleting a profile: this deletes the profile from all apps in which it appears in the label list. The operation is idempotent. If the profile did not initially exist, a warning will be printed by no error occurs.</p> </li> </ul>"},{"location":"gen2_profiles_proposal_1/#issue-declarative-installation-and-inconsistent-states","title":"Issue: declarative installation and inconsistent states","text":"<p>A user may want to provision profiles and applications in a declarative way, meaning with manifests and \"kubectl apply -f\". Those manifests contain applicationsets that satisfy the \"arlon application\" requirement. The user does not know about the null app. Therefore the user's declared applicationsets (with the arlon requirements) will solely completely define the arlon applications and profiles. We assume that the user has no interest in declaratively create empty profiles, only profiles that have at least one associated application.</p> <p>Arlon must allow a partially inconsistent state, meaning, at any point in time, some profiles may not exist in the null app. This is fine, since the null app's only purpose is to maintain the existence of empty profiles. During an inconsistent state, profiles that exist in some apps but not in the null app are, by definition, existent, since they appear in at least one app. However, one enhancement is necessary on the \"remove app from profile\" operation:  - In addition to removing the profile from the app's label list, the operation must ensure the existence of the profile in null app, meaning add it if it's not already there. This will ensure that at the end of the operation, the profile still exists in the null app. If it no longer exists anywhere else, then by definition it is empty.</p>"},{"location":"gen2_profiles_proposal_1/#full-custom-resource","title":"Full Custom Resource","text":"<p>(Under construction)</p> <p>We could represent Gen2 profiles using a custom resource, either a new type, or by overloading the existing Profile CR already used by Gen1. The downside is an increase in implementation complexity, for e.g * where is the source of truth for app-to-profile associations? * what if an app refers to a profile label value not represented by any Profile CR?</p> <p>A new controller would be most likely need to be developed.</p>"},{"location":"gen2_profiles_proposal_2/","title":"Gen2 Profiles - Proposal 2","text":"<p>This is an update to the previous Proposal 1 for Gen2 Profiles design. The main change is the introduction of the new AppProfile custom resource, which elevates profiles to first-class objects. The rest of the design remains mostly unchanged, meaning Arlon apps are still based on ApplicationSets, and a cluster is associated with an AppProfile by labeling it, except the labeling is handled slightly differently (see Labeling Algorithm). AppProfiles are now the source of truth for profile-to-app mappings. A new controller was introduced to reconcile not only AppProfiles, but clusters and ApplicationSets as well since they are all inter linked.</p>"},{"location":"gen2_profiles_proposal_2/#object-model","title":"Object model","text":"<ul> <li> <p>Arlon Application (or App for short): a thin wrapper around an ApplicationSet.   An ApplicationSet is an Arlon Application if it has the <code>managed-by=arlon</code> label.</p> </li> <li> <p>App Profile: a uniquely named set of Arlon Applications. It is backed by a new custom resource and CRD.   (The resource is named AppProfile to distinguish it from the gen1 Profile resource. Even though   gen1 profiles will deprecated and eventually retired, the naming scheme avoids conflicts during the transition).</p> </li> <li> <p>Arlon Cluster: a gen2 cluster created by Arlon.</p> </li> <li>As a reminder, it is represented by between 2 and 3 ArgoCD Application resources:<ul> <li>The cluster application (named with the workload cluster name)</li> <li>The arlon application (named by appending the -arlon suffix to the cluster application's name)</li> <li>The optional profile application (named by appending -profile suffix to the cluster application's name)</li> </ul> </li> <li>The first application (the cluster application) is treated as the anchor for the entire set. When an Arlon Cluster     is associated with an AppProfile, the cluster application will be labeled with the AppProfile's name.</li> <li> <p>An Arlon Cluster that was successfully deployed always has an associated ArgoCD cluster (thanks to the ClusterRegistration mechanism).</p> </li> <li> <p>ArgoCD Cluster: the set of ArgoCD clusters is a superset of Arlon clusters.</p> </li> <li> <p>External Cluster: any ArgoCD cluster that was not created by Arlon.   So essentially <code>External Clusters Set = ArgoCD Clusters Set - Arlon Clusters Set</code>.   A user may want to associate an external cluster with an app profile.</p> </li> </ul> <p>Observations: - An app profile can be associated with (or \"contain\") any number of applications - And an app can be associated with multiple profiles. - A cluster is said to be associated with a profile if it is labeled with <code>arlon.io/profile=&lt;profileName&gt;</code>. - A cluster can be associated with at most one profile. A profile may be associated (attached to) any number of clusters.</p> <p>This table summarizes the actual resources backing the objects: | Object Type        | Actual Resource        | |--------------------|------------------------| | Arlon Application  | ArgoCD ApplicationSet  | | AppProfile         | AppProfile             | | Arlon Cluster      | ArgoCD Application     | | ArgoCD Cluster     | Kubernetes Secret      |</p>"},{"location":"gen2_profiles_proposal_2/#labeling-algorithm","title":"Labeling Algorithm","text":"<p>Just like in proposal 1, associating a cluster with an app profile is done by labeling the cluster with the profile's name, and ensuring that that name is included in the corresponding ApplicationSet's <code>matchExpressions</code> values list. But there are some differences: - For an Arlon cluster, which is anchored by an ArgoCD Application resource,   the user should label the Application resource, not the corresponding ArgoCD cluster.   The new AppProfile controller will propagate the label to the corresponding ArgoCD cluster.   This allows the user to deploy an Arlon cluster, create and populate a profile, and associate   the cluster to the profile all in one declarative \"apply\" operation. (A user can't label   an ArgoCD cluster that doesn't exist yet) - For non-Arlon clusters, generally referred as \"external\", the design allows those existing   ArgoCD clusters to be labeled directly, but this will be managed outside of the AppProfiles controller   and essentially the user's responsibility, and has limitations.</p>"},{"location":"gen2_profiles_proposal_2/#controller","title":"Controller","text":"<p>A new controller was developed to not only reconcile AppProfiles, but also clusters and ApplicationSets (those representing Arlon Applications) since they are now all inter linked through profiles. - The main controller logic resides in <code>pkg/appprofile/reconcile.go</code> and <code>controllers/appprofile_controller.go</code>. - Additionally, logic was added to reconcile ArgoCD applications (representing Arlon clusters) and   ArgoCD ApplicationSets (representing Arlon apps) with the relationships defined by AppProfiles:   - <code>controllers/application_controller.go</code>   - <code>controllers/applicationset_controller.go</code> - The new logic will be eventually merged into the main controller. In the prototype, it is available as a separate CLI command   to simplify testing: <code>arlon appprofile-controller</code>. The reconciliation algorithm is complex due to the number of interdependent resources. See Appendix A: Reconciliation Algorithm for details.</p>"},{"location":"gen2_profiles_proposal_2/#usage","title":"Usage","text":"<p>Since all objects direcly map to Kubernetes resources, the user can in theory manage everything with <code>kubectl [create/apply/edit/delete/lebel]</code>. However, the arlon CLI and API are still useful for some things as explained below.</p>"},{"location":"gen2_profiles_proposal_2/#creating-an-initial-arlon-application","title":"Creating an initial Arlon Application","text":"<p>Even though an Arlon app is directly represented by an ApplicationSet, the resource has strict requirements, so it's easiest to create it with the help of the future <code>arlon app create</code>, which can create the resource directly or dump its YAML to standard out. (This command is not yet implemented in the initial Proposal 2 prototype).</p> <p>The requirements on the ApplicationSet are: - Must have the <code>arlon-type=application</code> label. - The spec's <code>generators</code> section must use a single generator with a <code>matchExpressions</code> selector with   the <code>arlon.io/profile</code> key and <code>In</code> as operator.  (Note: the <code>values</code> field will later be   set the AppProfile controller, automatically) - The spec's <code>template</code> section must templatize the generated ArgoCD application names   based on the workload cluster name. The user is free to configure the template's   <code>spec</code> subsection to target any manifest or Helm chart in git.</p> <p>Example: <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  labels:\n    arlon-type: application\n  name: guestbook\n  namespace: argocd\nspec:\n  generators:\n  - clusters:\n      selector:\n        matchExpressions:\n        - key: arlon.io/profile\n          operator: In\n          values: []\n  template:\n    metadata:\n      name: '{{name}}-appset-guestbook'\n    spec:\n      destination:\n        namespace: default\n        server: '{{server}}'\n      project: default\n      source:\n        path: guestbook\n        repoURL: https://github.com/argoproj/argocd-example-apps/\n        targetRevision: HEAD\n      syncPolicy:\n        automated:\n          prune: true\n</code></pre></p>"},{"location":"gen2_profiles_proposal_2/#managing-application-profiles","title":"Managing Application Profiles","text":"<p>An AppProfile is a new custom resource with a simple spec: a list of Arlon Application names: <pre><code>apiVersion: core.arlon.io/v1\nkind: AppProfile\nmetadata:\n  name: engineering\n  namespace: arlon\nspec:\n  appNames:\n  - guestbook\n  - wordpress\n  - nginx\nstatus:\n  health: degraded\n  invalidAppNames:\n  - wordpress\n  - nginx\n</code></pre></p> <ul> <li>The resource also has a Status field that the controller updates.</li> <li>The <code>Status.health</code> subfield is either <code>healthy</code> or <code>degraded</code>. Degraded means that one or more specified apps don't exist.</li> <li>The <code>Status.invalidAppNames</code> indicates which app names are invalid, if any.</li> </ul> <p>The presence of the <code>Status</code> section is a result of the decision to validate the profile asynchronously and allow invalid app names to be specified. This generally results in a simpler design, and follows the Kubernetes philosophy of allowing a user to specify any <code>spec</code> and report status later, after reconciliation. The alternative would have been to validate the app names synchronously using a webhook. This alternative was not currently chosen due to the complexity of webhook devopment and testing, but may be reconsidered in the future if a good use case arises.</p> <ul> <li>Updating a profile's <code>Spec.appNames</code> can have immediate side effects:   corresponding ArgoCD Applications can be deployed or destroyed in any clusters labeled with the profile name,   as soon as the controller finishes reconciliation.</li> <li>To list profiles: <code>kubectl -n arlon get appprofiles</code></li> <li>To delete a profile: <code>kubectl -n arlon delete appprofile &lt;name&gt;</code>. As expected, this command can have immediate effects on impacted clusters.</li> </ul>"},{"location":"gen2_profiles_proposal_2/#associating-profiles-with-arlon-clusters","title":"Associating profiles with Arlon clusters","text":"<p>A cluster can have at most one profile attached to it. The labeling mechanism can be viewed as an internal Arlon implementation detail, and it will be hidden from users with <code>arlon</code> CLI providing commands to attach/detach profiles to/from clusters, but until that's implemented, the user can achieve the same effect by using Kubernetes labeling directly:</p> <ul> <li>To attach a profile to an Arlon cluster, simply label the corresponding anchor ArgoCD Application as follows:</li> <li><code>kubectl -n argocd label application --overwrite &lt;clusterName&gt; arlon.io/profile=&lt;profileName&gt;</code></li> <li>To detach, remove the label:</li> <li><code>kubectl -n argocd label application &lt;clusterName&gt; arlon.io/profile-</code></li> </ul> <p>In addition to those raw kubectl commands, we propose to add arlon CLI commands and APIs to slightly simplify the task and abstract out the labeling.</p>"},{"location":"gen2_profiles_proposal_2/#associating-profiles-with-external-clusters-experimental-not-tested-yet","title":"Associating profiles with external clusters (experimental, not tested yet)","text":"<p>An external cluster does not have an Arlon representation (meaning there is no \"anchor\" ArgoCD Application with the same name). It can still be associated with an AppProfile by labeling the raw ArgoCD cluster: - Unfortunately as of ArgoCD 2.4, there is no argocd CLI command to label a cluster - A user can still manually label the secret resource where ArgoCD stores the cluster's metadata and credentials. This requires some work,   as those secrets are named using an non-obvious convention. - The best thing Arlon should do is to provide dedicated CLI commands and APIs to simplify this task.</p>"},{"location":"gen2_profiles_proposal_2/#fully-declarative-initialization","title":"Fully declarative initialization","text":"<p>Now that all Arlon concepts are represented by declarative resources, it is now possible to provision a complete set of resources with a single <code>kubectl apply -f</code> on a file or folder of manifests. In particular, we can now develop a fully self-contained demo folder that contains all of these resources containing references to each other: - App Profiles - Apps - Arlon Clusters</p> <p>The user can then <code>kubectl apply -f</code> the whole folder and observe the automatic creation of the cluster(s), profiles, and apps.</p> <p>(Note: the workload clusters need a cluster template in git. This can be supplied by the Arlon repo itself. This does point to an issue that may become a problem later on: cluster templates don't have a representation today. There is no registration mechanism for them. The user is expected to \"know\" where his/her cluster templates reside, and is responsible for specifying their git location when creating new workload clusters)</p>"},{"location":"gen2_profiles_proposal_2/#application-overrides","title":"Application Overrides","text":"<p>Given that an Arlon Application is just a specialized ApplicationSet, it inherits an ApplicationSet's ability to specify a full ArgoCD Application spec, and this spec can contain overrides for Helm charts. - This does mean that every different permutation of override values requires a new ApplicationSet, and therefore Arlon Application. - This may be reasonable if we accept that an Arlon Application is not a true application, but rather, an intermediate object that points   to the true application source residing in Git. The intermediate object can be viewed as a customization or specialization of the   application source, so it's ok to have multiple intermediate objects, each holding a different set of customizations.</p>"},{"location":"gen2_profiles_proposal_2/#issues-limitations","title":"Issues &amp; Limitations","text":"<p>The remaining issues &amp; limitations raised in Proposal 1 still apply: - Only one profile per cluster - Inherits any limitations of ApplicationSets - Does not support ApplicationSets with other types of generators - Makes Arlon even more dependent on ArgoCD technology - There is no way to create a per-cluster application override, unless the user creates a unique and dedicated Arlon Application for   the cluster, places it in a dedicated profile, and applies the profile to the cluster.</p>"},{"location":"gen2_profiles_proposal_2/#appendix-a-reconciliation-algorithm","title":"Appendix A: Reconciliation Algorithm","text":"<p>The pseudocode looks something like:</p> <p></p>"},{"location":"gen2_profiles_proposal_3/","title":"Gen2 Profiles - Proposal 3","text":"<p>This is an update to Proposal 2 for Gen2 Profiles design to add the ability to associate (attach) multiple AppProfiles to a cluster.</p>"},{"location":"gen2_profiles_proposal_3/#summary-of-changes","title":"Summary of changes","text":"<ul> <li> <p>The arlon.io/profiles annotation replaces the arlon.io/profile label   in an Arlon cluster (represented by an ArgoCD Application resource with label   <code>arlon-type=cluster-app</code>). The annotation stores a comma separated list of   AppProfile names. An annotation was chosen instead of a label because   Kubernetes label values cannot contain the comma (<code>,</code>) character.</p> </li> <li> <p>Arlon applications are still implemented as ArgoCD ApplicationSets.   But the List generator replaces the Clusters generator used in Proposal 2.   The List generator allows the Arlon controller to generate the precise   list of clusters associated with an Arlon Application, which is computed from   the profile-to-application and cluster-to-profile mappings at any given time.   Every generated Application resource receives these two template parameters:</p> </li> <li><code>cluster_name</code>: The cluster's name</li> <li> <p><code>cluster_server</code>: The URL of the cluster's API server.   The initial manifest generated by <code>arlon app create</code> (see below) is configured   to set any generated Application resource's name to <code>&lt;cluster_name&gt;-app-&lt;app_name&gt;</code>.   The user is free to change it by editing the ApplicationSet resource directly</p> </li> <li> <p>New CLI commands and features</p> </li> <li><code>arlon app create appName repoUrl repoPath [flags]</code> creates an ApplicationSet      manifest that satisfies the requirements to serve as an initial Arlon Application.      In particular, it includes a <code>List</code> generator with an <code>Elements</code> list of zero items.      The Arlon AppProfile controller will update the list dynamically as needed.      The manifest is applied to the management cluster immediately unless the user      specifies the <code>--output-yaml</code> option, in which case the manifest can be saved,      edited, and applied later.</li> <li><code>arlon cluster list</code> now displays the app profile list in the <code>APP_PROFILES</code> column.</li> <li><code>arlon cluster setappprofiles &lt;clusterName&gt; &lt;commaSeparatedAppProfileNames</code> allows      a user to set the list of app profiles associated with a cluster. Setting the      list to an empty string removes all app profiles from the cluster.</li> <li>No CRUD operations are provided for Arlon AppProfile resources since they are simple     resources that are best manipulated and applied as manifests with <code>kubectl</code>.     The one exception is <code>arlon appprofile list</code>, which is a convenience and shows     the list of apps for each AppProfile, and the <code>Status.healthy</code> field.</li> </ul>"},{"location":"gen2_profiles_proposal_3/#testing","title":"Testing","text":"<p>The reconciliation algorithm is tested by a comprensive unit test located at <code>pkg/appprofile/appprofile_test.go</code></p>"},{"location":"installation/","title":"Installation","text":"<p>Arlon CLI downloads are provided on GitHub. The CLI is not a self-contained standalone executable though. It is required to point the CLI to a management cluster and set up the Arlon controller in this management cluster. It is also recommended to have the tools described here to be installed for a seamless experience.</p> <p>For a quickstart minimal demonstration setup, follow the instructions to set up a KIND based testbed with Arlon and ArgoCD running  here.</p> <p>Please follow the manual instructions in this section for a customised setup or refer the instructions for automated installation here.</p>"},{"location":"installation/#setting-up-tools","title":"Setting up tools","text":"<p>To leverage the complete features provided by ArgoCD and Arlon, it is recommended to have the following tools installed:</p> <ol> <li><code>git</code></li> <li>ArgoCD CLI</li> <li><code>kubectl</code></li> </ol> <p>Barring the <code>git</code> installation, the Arlon CLI has the ability to install <code>argocd</code> and <code>kubectl</code> CLIs on a user's machine for Linux and macOS based systems.  To install these tools, run <code>arlon install --tools-only</code> to download and place these executables in <code>~/.local/bin</code>. It however, falls to the user to add the  aforementioned directory to <code>$PATH</code> if not present. This command also verifies the presence of <code>git</code> on your <code>$PATH</code>.</p>"},{"location":"installation/#customised-setup","title":"Customised Setup","text":""},{"location":"installation/#management-cluster","title":"Management cluster","text":"<p>You can use any Kubernetes cluster that you have admin access to. Ensure:</p> <ul> <li><code>kubectl</code> is in your path</li> <li><code>KUBECONFIG</code> is pointing to the right file and the context set properly</li> </ul>"},{"location":"installation/#argocd","title":"ArgoCD","text":"<ul> <li>Follow steps 1-4 of the ArgoCD installation guide to install ArgoCD onto your management cluster.</li> <li>After this step, you should be logged in as <code>admin</code> and a config file was created at <code>${HOME}/.config/argocd/config</code></li> <li>Create your workspace repository in your git provider if necessary, then register it.   Example: <code>argocd repo add https://github.com/myname/arlon_workspace --username myname --password secret</code>.</li> <li>Note: type <code>argocd repo add --help</code> to see all available options.</li> <li>For Arlon developers, this is not your fork of the Arlon source code repository,   but a separate git repo where some artifacts like profiles created by Arlon will be stored.</li> <li>Highly recommended: configure a webhook   to immediately notify ArgoCD of changes to the repo. This will be especially useful   during the tutorial. Without a webhook, repo changes may take up to 3 minutes   to be detected, delaying cluster configuration updates.</li> <li>Create a local user named <code>arlon</code> with the <code>apiKey</code> capability.   This involves editing the <code>argocd-cm</code> ConfigMap using <code>kubectl</code>.</li> <li>Adjust the RBAC settings to grant admin permissions to the <code>arlon</code> user.   This involves editing the <code>argocd-rbac-cm</code> ConfigMap to add the entry   <code>g, arlon, role:admin</code> under the <code>policy.csv</code> section. Example:</li> </ul> <pre><code>apiVersion: v1\ndata:\npolicy.csv: |\ng, arlon, role:admin\nkind: ConfigMap\n[...]\n</code></pre> <ul> <li>Generate an account token: <code>argocd account generate-token --account arlon</code></li> <li>Make a temporary copy of this config-file in <code>/tmp/config</code> then   edit it to replace the value of <code>auth-token</code> with the token from   the previous step. Save changes. This file will be used to configure the Arlon   controller's ArgoCD credentials during the next steps.</li> </ul>"},{"location":"installation/#arlon-controller","title":"Arlon controller","text":"<ul> <li>Create the arlon namespace: <code>kubectl create ns arlon</code></li> <li>Create the ArgoCD credentials secret from the temporary config file:   <code>kubectl -n arlon create secret generic argocd-creds --from-file /tmp/config</code></li> <li>Delete the temporary config file</li> <li>Clone the arlon git repo and cd to its top directory</li> <li>Create the CRDs: <code>kubectl apply -f config/crd/bases/</code></li> <li>Deploy the controller: <code>kubectl apply -f deploy/manifests/</code></li> <li>Ensure the controller eventually enters the Running state: <code>watch kubectl -n arlon get pod</code></li> </ul>"},{"location":"installation/#arlon-cli","title":"Arlon CLI","text":"<p>Download the CLI for the latest release from GitHub. Currently, Linux and macOS operating systems are supported. Uncompress the tarball, rename it as <code>arlon</code> and add to your PATH</p> <p>Run <code>arlon verify</code> to check for prerequisites. Run <code>arlon install</code> to install any missing prerequisites.</p> <p>The following instructions are to manually build CLI from this code repository.</p>"},{"location":"installation/#building-the-cli","title":"Building the CLI","text":"<ul> <li>Clone this repository and pull the latest version of a branch (main by default)</li> <li>From the top directory, run <code>make build</code></li> <li>Optionally create a symlink from a directory   (e.g. <code>/usr/local/bin</code>) included in your ${PATH} to the <code>bin/arlon</code> binary   to make it easy to invoke the command.</li> </ul>"},{"location":"installation/#cluster-orchestration-api-providers","title":"Cluster orchestration API providers","text":"<p>Arlon currently supports Cluster API on AWS cloud. It also has experimental support for Crossplane on AWS. <code>cluster-api</code> cloud provider components on  a management cluster can be installed by following the official guide, as instructed here. In addition to this, the Arlon CLI also ships with an <code>install</code> command to facilitate, the installation of supported  infrastructure providers by mimicking the behaviour of <code>clusterctl</code> CLI used in the official setup instructions. The details  for which can be found here.</p>"},{"location":"installation/#cluster-api","title":"Cluster API","text":""},{"location":"installation/#manual-installation","title":"Manual Installation","text":"<p>Using the Cluster API Quickstart Guide as reference, complete these steps:</p> <ul> <li>Install <code>clusterctl</code></li> <li>Initialize the management cluster.   In particular, follow instructions for your specific cloud provider (AWS in this example)   Ensure <code>clusterctl init</code> completes successfully and produces the expected output.</li> </ul>"},{"location":"installation/#using-arlon-cli","title":"Using Arlon CLI","text":"<p>To install <code>cluster-api</code> components on the management cluster, the <code>install</code> command provides a  helpful wrapper around <code>clusterctl</code> CLI tool.</p> <p>To install a provider, all the pre-requisites must be met as mentioned here. After which, simply running <code>arlon install --capi-only --infrastructure aws,docker</code> will install the latest available version  of AWS and Docker provider components onto the management cluster.</p>"},{"location":"installation/#crossplane-experimental","title":"Crossplane (experimental)","text":"<p>Using the Upbound AWS Reference Platform Quickstart Guide as reference, complete these steps:</p> <ul> <li>Install UXP on your management cluster</li> <li>Install Crossplane kubectl extension</li> <li>Install the platform configuration</li> <li>Configure the cloud provider credentials</li> </ul> <p>You do not need to go any further, but you're welcome to try the Network Fabric example.</p> <p>FYI: we noticed the dozens/hundreds of CRDs that Crossplane installs in the management cluster can noticeably slow down kubectl, and you may see a warning that looks like:</p> <pre><code>I0222 17:31:14.112689   27922 request.go:668] Waited for 1.046146023s due to client-side throttling, not priority and fairness, request: GET:https://AA61XXXXXXXXXXX.gr7.us-west-2.eks.amazonaws.com/apis/servicediscovery.aws.crossplane.io/v1alpha1?timeout=32s\n</code></pre>"},{"location":"installation/#automatic-setup","title":"Automatic Setup","text":"<p>Arlon CLI provides an <code>init</code> command to install \"itself\" on a management cluster. This command performs a basic setup of <code>argocd</code>(if needed) and <code>arlon</code> controller. It makes the following assumptions while installing <code>arlon</code>:</p> <ul> <li> <p>For ArgoCD:</p> <ul> <li>If ArgoCD is present, it is present the in namespace <code>argocd</code> and the <code>admin</code> password is the same as in <code>argocd-initial-admin-secret</code> ConfigMap.</li> </ul> </li> <li> <p>For Arlon:</p> <ul> <li>assuming that the existence of <code>arlon</code> namespace means Arlon controller exists.</li> </ul> </li> </ul> <p>To install Arlon controller using the init command these pre-requisites need to be met:</p> <ol> <li>A valid kubeconfig pointing to the management cluster.</li> <li>Port 8080 should not be in use by other programs. Arlon init uses it to port-forward <code>argocd</code>.</li> <li>A hosted Git repository with at least a <code>README</code> file present and a valid GitHub token(detailed here) for:</li> <li>Adding a repository to ArgoCD.</li> <li>Avoiding rate limiting of GitHub API while fetching <code>cluster-api</code> related manifests.</li> <li>Pushing cluster template manifests to the workspace repository.</li> <li>Pre-requisites for supported CAPI infrastructure providers(AWS and Docker as of now) as described below.</li> </ol>"},{"location":"installation/#setting-up-the-workspace-repository","title":"Setting up the workspace repository","text":"<ol> <li>Create a GitHub repository if you don't already have it.</li> <li>Ensure that the repository at least has a README file because empty repository cannot be added to ArgoCD.</li> <li>Create a Personal Access Token for authentication, the token will need <code>repo:write</code> scope to push the cluster template example manifests.</li> <li>Set the <code>GITHUB_TOKEN</code> environment variable to the token create in the previous step.</li> </ol>"},{"location":"installation/#pre-requisites-for-cluster-api-providers","title":"Pre-requisites for cluster-api providers","text":"<p>This section outlines the requirements that need to be fulfilled for installing the <code>cluster-api</code> provider components that the <code>init</code> or the <code>install</code> command installs on the management cluster.</p>"},{"location":"installation/#docker","title":"Docker","text":"<p>There are no special requirements for docker provider, as it is largely used in an experimental setups.</p>"},{"location":"installation/#aws","title":"AWS","text":"<p>The following environment variables need to be set: - <code>AWS_SSH_KEY_NAME</code> (the SSH key name to use) - <code>AWS_REGION</code> (region where the cluster is deployed) - <code>AWS_ACCESS_KEY_ID</code> (access key id for the associated AWS account) - <code>AWS_SECRET_ACCESS_KEY</code> (secret access key for the associated AWS account) - <code>AWS_NODE_MACHINE_TYPE</code> (machine type for cluster nodes) - <code>AWS_CONTROL_PLANE_MACHINE_TYPE</code> (machine type for control plane) - <code>AWS_SESSION_TOKEN</code> (optional: only for MFA enabled accounts)</p>"},{"location":"installation/#starting-the-installation","title":"Starting the installation","text":"<p>Once the above requirements are met, start the installation process, simply running <code>arlon init -e --username &lt;GIT_USER&gt; --repoURL &lt;WORKSPACE_URL&gt; --password &lt;GIT_PASSWORD&gt; --examples -y</code>. This installs the controller, argocd(if not already present) <code>-e</code> flag adds cluster template manifests to the  for using the given credentials. To not add examples, just remove the <code>-e</code> flag. The <code>-y</code> flag refers to silent installation, which is useful for scripts. For an interactive installation, exclude the <code>-y</code> or <code>--no-confirm</code> flag. <p>This command does the following:</p> <ul> <li>Installs ArgoCD if not present.</li> <li>Installs Arlon if not present.</li> <li>Creates the Arlon user account in ArgoCD with <code>admin</code> rights.</li> <li>Installs <code>cluster-api</code> with the latest versions of <code>docker</code> and <code>aws</code> providers.</li> <li>Removes the <code>repoctx</code> file created by <code>arlon git register</code> if present.</li> <li>Registers a <code>default</code> alias against the provided <code>repoUrl</code>.</li> <li>Checks for pre-existing examples, and prompts to delete and proceed(if <code>-y</code> or <code>--no-confirm</code> flag is not set, else deletes the existing examples).</li> <li>Pushes cluster template manifests to the provided GitHub repository.</li> </ul> <p>The Arlon repository also hosts some examples in the <code>examples</code> directory. In particular, the <code>examples/declarative</code> directory is a set of ready to use manifests  which allows us to deploy a cluster with the new <code>app</code> and <code>app-profiles</code>. To run these examples, simply clone the source code and run these commands: <pre><code>cd &lt;ARLON_SOURCE_REPO_PATH&gt;\nkubectl apply -f examples/declarative\n</code></pre></p> <p>This creates an EKS cluster with managed machine pools in the <code>us-west-1</code> region, and attaches a few example \"apps\" to it in the form of two <code>app-profiles</code> namely <code>frontent</code> and <code>backend</code>. The <code>frontend</code> app-profile consists of the guestbook application and a sentinel non-existing app appropriately named <code>nonexistent-1</code>.  Similarly, the <code>backend</code> app-profile consists of the redis and another non-existent app called <code>nonexistent-2</code>.  The sentinel, <code>nonexistent</code> apps are simply present to demonstrate the <code>description</code> field for the health field and the <code>invalidAppNames</code> which lists down the apps which do not exist.</p>"},{"location":"tutorial/","title":"Tutorial (gen1)","text":"<p>This assumes that you plan to deploy workload clusters on AWS cloud, with Cluster API (\"CAPI\") as the cluster orchestration API provider. Also ensure you have set up a workspace repository, and it is registered as a git repo in ArgoCD. The tutorial will assume the existence of these environment variables:</p> <ul> <li><code>${ARLON_REPO}</code>: where the arlon repo is locally checked out</li> <li><code>${WORKSPACE_REPO}</code>: where the workspace repo is locally checked out</li> <li><code>${WORKSPACE_REPO_URL}</code>: the workspace repo's git URL. It typically looks like <code>https://github.com/${username}/${reponame}.git</code></li> <li><code>${CLOUD_REGION}</code>: the region where you want to deploy example clusters and workloads (e.g. us-west-2)</li> <li><code>${SSH_KEY_NAME}</code>: the name of a public ssh key name registered in your cloud account, to enable ssh to your cluster nodes</li> </ul> <p>Additionally, for examples assuming <code>arlon git register</code>, \"default\" and a \"prod\" git repo aliases will also be given.</p> <p>_Note: for the best experience, make sure your workspace repo is configured to send change notifications to ArgoCD via a webhook. See the Installation section for details.</p>"},{"location":"tutorial/#cluster-specs","title":"Cluster specs","text":"<p>We first create a few cluster specs with different combinations of API providers and cluster types (kubeadm vs EKS). One of the cluster specs is for an unconfigured API provider (Crossplane); this is for illustrative purposes, since we will not use it in this tutorial.</p> <pre><code>arlon clusterspec create capi-kubeadm-3node --api capi --cloud aws --type kubeadm --kubeversion v1.21.10 --nodecount 3 --nodetype t2.medium --tags devel,test --desc \"3 node kubeadm for dev/test\" --region ${CLOUD_REGION} --sshkey ${SSH_KEY_NAME}\narlon clusterspec create capi-eks --api capi --cloud aws --type eks --kubeversion v1.21.10 --nodecount 2 --nodetype t2.large --tags staging --desc \"2 node eks for general purpose\" --region ${CLOUD_REGION} --sshkey ${SSH_KEY_NAME}\narlon clusterspec create xplane-eks-3node --api xplane --cloud aws --type eks --kubeversion v1.21.10 --nodecount 4 --nodetype t2.small --tags experimental --desc \"4 node eks managed by crossplane\" --region ${CLOUD_REGION} --sshkey ${SSH_KEY_NAME}\n</code></pre> <p>Ensure you can now list the cluster specs:</p> <pre><code>$ arlon clusterspec list\nNAME                APIPROV  CLOUDPROV  TYPE     KUBEVERSION  NODETYPE   NODECNT  MSTNODECNT  SSHKEY  CAS    CASMIN  CASMAX  TAGS          DESCRIPTION\ncapi-eks            capi     aws        eks      v1.21.10     t2.large   2        3           leb     false  1       9       staging       2 node eks for general purpose\ncapi-kubeadm-3node  capi     aws        kubeadm  v1.21.10     t2.medium  3        3           leb     false  1       9       devel,test    3 node kubeadm for dev/test\nxplane-eks-3node    xplane   aws        eks      v1.21.10     t2.small   4        3           leb     false  1       9       experimental  4 node eks managed by crossplane\n</code></pre>"},{"location":"tutorial/#bundles","title":"Bundles","text":"<p>First create a static bundle containing raw YAML for the <code>guestbook</code> sample application from this example file:</p> <pre><code>cd ${ARLON_REPO}\narlon bundle create guestbook-static --tags applications --desc \"guestbook app\" --from-file examples/bundles/guestbook.yaml\n</code></pre> <p>(Note: the YAML is simply a concatenation of the files found in the ArgoCD Example Apps repo)</p> <p>To illustrate the difference between static and dynamic bundles, we create a dynamic version of the same application, this time using a reference to a git directory containing the YAML. We could point it directly to the copy in the ArgoCD Example Apps repo, but we'll want to make modifications to it, so we instead create a new directory to host our own copy in our workspace directory:</p> <pre><code>cd ${WORKSPACE_REPO}\nmkdir -p bundles/guestbook\ncp ${ARLON_REPO}/examples/bundles/guestbook.yaml bundles/guestbook\ngit add bundles/guestbook\ngit commit -m \"add guestbook\"\ngit push origin main\n</code></pre> <pre><code>arlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-url ${WORKSPACE_REPO_URL} --repo-path bundles/guestbook\n            # OR\n# using repository aliases\n# using the default alias\narlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-path bundles/guestbook\n  # using the prod alias\narlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-path bundles/guestbook --repo-alias prod\n</code></pre> <p>Next, we create a static bundle for another \"dummy\" application, an Ubuntu pod (OS version: \"Xenial\") that does nothing but print the date-time in an infinite sleep loop:</p> <pre><code>cd ${ARLON_REPO}\narlon bundle create xenial-static --tags applications --desc \"xenial pod\" --from-file examples/bundles/xenial.yaml\n</code></pre> <p>Finally, we create a bundle for the Calico CNI, which provides pod networking. Some types of clusters (e.g. kubeadm) require a CNI provider to be installed onto a newly created cluster, so encapsulating the provider as a bundle will give us a flexible way to install it. We download a known copy from the authoritative source and store it the workspace repo in order to create a dynamic bundle from it:</p> <pre><code>cd ${WORKSPACE_REPO}\nmkdir -p bundles/calico\ncurl https://docs.projectcalico.org/v3.21/manifests/calico.yaml -o bundles/calico/calico.yaml\ngit add bundles/calico\ngit commit -m \"add calico\"\ngit push origin main\n</code></pre> <pre><code>arlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-url ${WORKSPACE_REPO_URL} --repo-path bundles/calico\n            # OR\n# using repository aliases\n# using the default alias\narlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-path bundles/calico\n  # using the prod alias\narlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-path bundles/calico --repo-alias prod\n</code></pre> <p>List your bundles to verify they were correctly entered:</p> <pre><code>$ arlon bundle list\nNAME               TYPE     TAGS                 REPO-URL                                             REPO-PATH              DESCRIPTION\ncalico             dynamic  networking,cni       ${WORKSPACE_REPO_URL}                                bundles/calico         Calico CNI\nguestbook-dynamic  dynamic  applications         ${WORKSPACE_REPO_URL}                                bundles/guestbook      guestbook app (dynamic)\nguestbook-static   static   applications         (N/A)                                                (N/A)                  guestbook app\nxenial-static      static   applications         (N/A)                                                (N/A)                  ubuntu pod in infinite sleep loop\n</code></pre>"},{"location":"tutorial/#profiles","title":"Profiles","text":"<p>We can now create profiles to group bundles into useful, deployable sets. First, create a static profile containing bundles xenial-static and guestbook-static:</p> <pre><code>arlon profile create static-1 --static --bundles guestbook-static,xenial-static --desc \"static profile 1\" --tags examples\n</code></pre> <p>Secondly, create a dynamic version of the same profile. We'll store the compiled form of the profile in the <code>profiles/dynamic-1</code> directory of the workspace repo. We don't create it manually; instead, the arlon CLI will create it for us, and it will push the change to git:</p> <pre><code>arlon profile create dynamic-1 --repo-url ${WORKSPACE_REPO_URL} --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples\n            # OR\n# using repository aliases\n# using the default alias\narlon profile create dynamic-1 --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples\n  # using the prod alias\narlon profile create dynamic-1 --repo-alias prod --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples\n</code></pre> <p>Note: the <code>--repo-base-path profiles</code> option tells <code>arlon</code> to create the profile under a base directory <code>profiles/</code> (to be created if it doesn't exist). That is in fact the default value of that option, so it is not necessary to specify it in this case.</p> <p>To verify that the compiled profile was created correctly:</p> <pre><code>$ cd ${WORKSPACE_REPO}\n$ git pull\n$ tree profiles\nprofiles\n\u251c\u2500\u2500 dynamic-1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mgmt\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Chart.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 templates\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 guestbook-dynamic.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 placeholder_configmap.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 xenial.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 workload\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 xenial\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 xenial.yaml\n[...]\n</code></pre> <p>Since <code>xenial</code> is a static bundle, a copy of its YAML was stored in <code>workload/xenial/xenial.yaml</code>. This is not done for <code>guestbook-dynamic</code> because it is dynamic.</p> <p>Finally, we create another variant of the same profile, with the only difference being the addition of Calico bundle. It'll be used on clusters that need a CNI provider:</p> <pre><code>arlon profile create dynamic-2-calico --repo-url ${WORKSPACE_REPO_URL} --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples\n            # OR\n# using repository aliases\n# using the default alias\narlon profile create dynamic-2-calico --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples\n  # using the prod alias\narlon profile create dynamic-2-calico --repo-alias prod --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples\n</code></pre> <p>Listing the profiles should show:</p> <pre><code>$ arlon profile list\nNAME              TYPE     BUNDLES                                 REPO-URL               REPO-PATH                  TAGS         DESCRIPTION\ndynamic-1         dynamic  guestbook-static,xenial-static          ${WORKSPACE_REPO_URL}  profiles/dynamic-1         examples     dynamic test 1\ndynamic-2-calico  dynamic  calico,guestbook-static,xenial-static   ${WORKSPACE_REPO_URL}  profiles/dynamic-2-calico  examples     dynamic test 1\nstatic-1          static   guestbook-dynamic,xenial-static         (N/A)                  (N/A)                      examples     static profile 1\n</code></pre>"},{"location":"tutorial/#clusters-gen1","title":"Clusters (gen1)","text":"<p>We are now ready to deploy our first cluster. It will be of type EKS. Since EKS clusters come configured with pod networking out of the box, we choose a profile that does not include Calico: <code>dynamic-1</code>. When deploying a cluster, arlon creates in git a Helm chart containing the manifests for creating and bootstrapping the cluster. Arlon then creates an ArgoCD App referencing the chart, thereby relying on ArgoCD to orchestrate the whole process of deploying and configuring the cluster. The arlon <code>deploy</code> command accepts a git URL and path for this git location. Any git repo can be used (so long as it's registered with ArgoCD), but we'll use the workspace cluster for convenience:</p> <pre><code>arlon cluster deploy --repo-url ${WORKSPACE_REPO_URL} --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks\n            # OR\n# using repository aliases\n# using the default alias\narlon cluster deploy --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks\n  # using the prod alias\narlon cluster deploy --repo-alias prod --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks\n</code></pre> <p>The git directory hosting the cluster Helm chart is created as a subdirectory of a base path in the repo. The base path can be specified with <code>--base-path</code>, but we'll leave it unspecified in order to use the default value of <code>clusters</code>. Consequently, this example produces the directory <code>clusters/eks-1/</code> in the repo. To verify its presence:</p> <pre><code>$ cd ${WORKSPACE_REPO}\n$ git pull\n$ tree clusters/eks-1\nclusters/eks-1\n\u2514\u2500\u2500 mgmt\n    \u251c\u2500\u2500 charts\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 capi-aws-eks\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Chart.yaml\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 templates\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 cluster.yaml\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 capi-aws-kubeadm\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Chart.yaml\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 templates\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 cluster.yaml\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 xplane-aws-eks\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 Chart.yaml\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 templates\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 cluster.yaml\n    \u2502\u00a0\u00a0         \u2514\u2500\u2500 network.yaml\n    \u251c\u2500\u2500 Chart.yaml\n    \u251c\u2500\u2500 templates\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 clusterregistration.yaml\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 ns.yaml\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 profile.yaml\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 rbac.yaml\n    \u2514\u2500\u2500 values.yaml\n</code></pre> <p>The chart contains several subcharts under <code>mgmt/charts/</code>, one for each supported type of cluster. Only one of them will be enabled, in this case <code>capi-aws-eks</code> (Cluster API on AWS with type EKS).</p> <p>At this point, the cluster is provisioning and can be seen in arlon and AWS EKS:</p> <pre><code>$ arlon cluster list\nNAME       CLUSTERSPEC  PROFILE  \neks-1      capi-eks     dynamic-1\n\n$ aws eks list-clusters\n{\n\"clusters\": [\n\"eks-1_eks-1-control-plane\",\n    ]\n}\n</code></pre> <p>Eventually, it will also be seen as a registered cluster in argocd, but this won't be visible for a while, because the cluster is not registered until its control plane (the Kubernetes API) is ready:</p> <pre><code>$ argocd cluster list\nSERVER                                                                    NAME        VERSION  STATUS      MESSAGE\nhttps://9F07DC211252C6F7686F90FA5B8B8447.gr7.us-west-2.eks.amazonaws.com  eks-1       1.18+    Successful  \nhttps://kubernetes.default.svc                                            in-cluster  1.20+    Successful  </code></pre> <p>To monitor the progress of the cluster deployment, check the status of the ArgoCD app of the same name:</p> <pre><code>$ argocd app list\nNAME                         CLUSTER                         NAMESPACE  PROJECT  STATUS  HEALTH   SYNCPOLICY  CONDITIONS  REPO                   PATH                                          TARGET\neks-1                        https://kubernetes.default.svc  default    default  Synced  Healthy  Auto-Prune  &lt;none&gt;      ${WORKSPACE_REPO_URL}  clusters/eks-1/mgmt                           main\neks-1-guestbook-static                                       default    default  Synced  Healthy  Auto-Prune  &lt;none&gt;      ${WORKSPACE_REPO_URL}  profiles/dynamic-1/workload/guestbook-static  HEAD\neks-1-profile-dynamic-1      https://kubernetes.default.svc  argocd     default  Synced  Healthy  Auto-Prune  &lt;none&gt;      ${WORKSPACE_REPO_URL}  profiles/dynamic-1/mgmt                       HEAD\neks-1-xenial                                                 default    default  Synced  Healthy  Auto-Prune  &lt;none&gt;      ${WORKSPACE_REPO_URL}  profiles/dynamic-1/workload/xenial            HEAD\n</code></pre> <p>The top-level app <code>eks-1</code> is the root of all argocd apps that make up the cluster and its configuration contents. The next level app <code>eks-1-profile-dynamic-1</code> represents the profile, and its children apps <code>eks-1-guestbook-static</code> and <code>eks-1-xenial</code> correspond to the bundles.</p> <p>Note: The overall tree-like organization of the apps and their health status can be visually observed in the ArgoCD web user interface._</p> <p>The cluster is fully deployed once those apps are all <code>Synced</code> and <code>Healthy</code>. An EKS cluster typically takes 10-15 minutes to finish deploying.</p>"},{"location":"tutorial/#behavioral-differences-between-static-and-dynamic-bundles-profiles","title":"Behavioral differences between static and dynamic bundles &amp; profiles","text":""},{"location":"tutorial/#static-bundle","title":"Static bundle","text":"<p>A change to a static bundle does not affect existing clusters using that bundle (through a profile). To illustrate this, bring up the ArgoCD UI and open the detailed view of the <code>eks-1-guestbook-static</code> application, which applies the <code>guestbook-static</code> bundle to the <code>eks-1</code> cluster. Note that there is only one <code>guestbook-ui</code> pod.</p> <p>Next, update the <code>guestbook-static</code> bundle to have 3 replicas of the pod:</p> <pre><code>arlon bundle update guestbook-static --from-file examples/bundles/guestbook-3replicas.yaml\n</code></pre> <p>Note that the UI continues to show one pod. Only new clusters consuming this bundle will have the 3 replicas.</p>"},{"location":"tutorial/#dynamic-profile","title":"Dynamic profile","text":"<p>Before discussing dynamic bundles, we take a small detour to introduce dynamic profiles, since this will help understand the relationship between profiles and bundles. To illustrate how a profile can be updated, we remove <code>guestbook-static</code> bundle from <code>dynamic-1</code> by specifying a new bundle set:</p> <pre><code>arlon profile update dynamic-1 --bundles xenial\n</code></pre> <p>Since the old bundle set was <code>guestbook-static,xenial-static</code>, that command resulted in the removal of <code>guestbook-static</code> from the profile. In the UI, observe the <code>eks-1-profile-dynamic-1</code> app going through Sync and Progressing phases, eventually reaching the healthy (green) state. And most importantly, the <code>eks-1-guestbook-static</code> app is gone. The reason this real-time change to the cluster was possible is that the <code>dynamic-1</code> profile is dynamic, meaning any change to its composition results in arlon updating the corresponding compiled Helm chart in git. ArgoCD detects this git change and propagates the app / configuration updates to the cluster.</p> <p>If the profile were of the static type, a change in its composition (the set of bundles) would not have affected existing clusters using that profile. It would only affect new clusters created with the profile.</p>"},{"location":"tutorial/#dynamic-bundle","title":"Dynamic bundle","text":"<p>To illustrate the defining characteristic of a dynamic bundle, we first add <code>guestbook-dynamic</code> to <code>dynamic-1</code>:</p> <pre><code>arlon profile update dynamic-1 --bundles xenial,guestbook-dynamic\n</code></pre> <p>Observe the re-appearance of the guestbook application, which is managed by the <code>eks-1-guestbook-dynamic</code> ArgoCD app. A detailed view of the app shows 1 guestbook-ui pod. Remember that a dynamic bundle's manifest content is stored in git. Use these commands to change the number of pod replicas to 3:</p> <pre><code>cd ${WORKSPACE_REPO}\ngit pull # to get all latest changes pushed by arlon\nvim bundles/guestbook/guestbook.yaml # edit to change deployment's replicas to 3\ngit commit -am \"increase guestbook replicas\"\ngit push origin main\n</code></pre> <p>Observe the number of pods increasing to 3 in the UI. Any existing cluster consuming this dynamic bundle will be updated similarly, regardless of whether the bundle is consumed via a dynamic or static profile.</p>"},{"location":"tutorial/#static-profile","title":"Static profile","text":"<p>Finally, a profile can be static. It means that it has no corresponding \"compiled\" component (a Helm chart) living in git. When a cluster is deployed using a static profile, the set of bundles (whether static or dynamic) it receives is determined by the bundle set defined by the profile at deployment time, and will not change in the future, even if the profile is updated to a new set at a later time.</p>"},{"location":"tutorial/#cluster-updates-and-upgrades","title":"Cluster updates and upgrades","text":"<p>The <code>arlon cluster update [flags]</code> command allows you to make changes to an existing cluster. The clusterspec, profile, or both can change, provided that the following rules and guidelines are followed.</p>"},{"location":"tutorial/#clusterspec","title":"Clusterspec","text":"<p>There are two scenarios. In the first, the clusterspec name associated with the cluster hasn't changed, meaning the cluster is using the same clusterspec. However, some properties of the clusterspec's properties have changed since the cluster was deployed or last updated, using <code>arlon clusterspec update</code> Arlon supports updating the cluster to use updated values of the following properties:</p> <ul> <li>kubernetesVersion</li> <li>nodeCount</li> <li>nodeType</li> </ul> <p>Note: Updating the cluster is not allowed if other properties of its clusterspec (e.g. cluster orchestration API provider, cloud, cluster type, region, pod CIDR block, etc...) have changed, however new clusters can always be created/deployed using the changed clusterspec.</p> <p>A change in <code>kubernetesVersion</code> will result in a cluster upgrade/downgrade. There are some restrictions and caveats you need to be aware of:</p> <ul> <li>The specific Kubernetes version must be supported by the particular implementation and release of the underlying cluster orchestration API provider cloud, and cluster type.</li> <li>In general, the control plane will be upgraded first</li> <li>Existing nodes are not typically not upgraded to the new Kubernetes version.   Only new nodes (added as part of manual <code>nodeCount</code> change or autoscaling)</li> </ul> <p>In the second scenario, as part of an update operation, you may choose to associate the cluster with a different clusterspec altogether. The rule governing the allowed property changes remains the same: the cluster update operation is allowed if, relative to the previously associated clusterspec, the new clusterspec's properties differ only in the values listed above.</p>"},{"location":"tutorial/#profile","title":"Profile","text":"<p>You can specify a completely different profile when updating a cluster. All bundles previously used will be removed from the cluster, and new ones specified by the new profile will be applied. This is regardless of whether the old and new profiles are static or dynamic.</p>"},{"location":"tutorial/#examples","title":"Examples","text":"<p>These sequence of commands updates a clusterspec to a newer Kubernetes version and a higher node count, then upgrades the cluster to the newer specifications:</p> <pre><code>arlon clusterspec update capi-eks --nodecount 3 --kubeversion v1.19.15\narlon cluster update eks-1\n</code></pre> <p>Note that the 2nd command didn't need any flags because the clusterspec used is the same as before.</p> <p>This example updates a cluster to use a new profile <code>my-new-profile</code>:</p> <pre><code>arlon cluster update eks-1 --profile my-new-profile\n</code></pre>"},{"location":"tutorial/#enabling-cluster-autoscaler-in-the-workload-cluster","title":"Enabling Cluster Autoscaler in the workload cluster:","text":""},{"location":"tutorial/#bundle-creation","title":"Bundle creation:","text":"<p>Register a dynamic bundle pointing to the bundles/capi-cluster-autoscaler in the Arlon repo.</p> <p>To enable the cluster-autoscaler bundle, add one more parameter during cluster creation: <code>srcType</code>. This is the ArgoCD-defined application source type (Helm, Kustomize, Directory). In addition to this, the <code>repo-revision</code> parameter should also be set to a stable arlon release branch ( in this case v0.10 ).</p> <p>This example creates a bundle pointing to the bundles/capi-cluster-autoscaler in Arlon repo</p> <pre><code>arlon bundle create cas-bundle --tags cas,devel,test --desc \"CAS Bundle\" --repo-url https://github.com/arlonproj/arlon.git --repo-path bundles/capi-cluster-autoscaler --srctype helm --repo-revision v0.10.0\n</code></pre>"},{"location":"tutorial/#profile-creation","title":"Profile creation:","text":"<p>Create a profile that contains this capi-cluster-autoscaler bundle. </p> <pre><code>arlon profile create dynamic-cas --repo-url ${WORKSPACE_REPO_URL} --repo-base-path profiles --bundles cas-bundle --desc \"dynamic cas profile\" --tags examples\n</code></pre>"},{"location":"tutorial/#clusterspec-creation","title":"Clusterspec creation:","text":"<p>Create a clusterspec with CAPI as ApiProvider and autoscaling enabled.In addition to this, the ClusterAutoscaler(Min|Max)Nodes properties are used to set 2 annotations on MachineDeployment required by the cluster autoscaler for CAPI.</p> <pre><code>arlon clusterspec create cas-spec --api capi --cloud aws --type eks --kubeversion v1.21.10 --nodecount 2 --nodetype t2.medium --tags devel,test --desc \"dev/test\"  --region ${REGION} --sshkey ${SSH_KEY_NAME} --casenabled\n</code></pre>"},{"location":"tutorial/#cluster-creation","title":"Cluster creation:","text":"<p>Deploy a cluster from this cluster spec and profile created in the previous steps.</p> <p><pre><code>arlon cluster deploy --repo-url ${WORKSPACE_REPO_URL} --cluster-name cas-cluster --profile dynamic-cas --cluster-spec cas-spec\n</code></pre> Consequently, this example produces the directory <code>clusters/cas-cluster/</code> in the repo. This will contain the capi-autoscaler subchart and manifests <code>mgmt/charts/</code>. To verify its contents:</p> <pre><code>$ cd ${WORKSPACE_REPO_URL}\n$ tree clusters/cas-cluster\nclusters/cas-cluster\n\u2514\u2500\u2500 mgmt\n    \u251c\u2500\u2500 Chart.yaml\n    \u251c\u2500\u2500 charts\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 capi-aws-eks\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Chart.yaml\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 templates\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 cluster.yaml\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 capi-aws-kubeadm\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Chart.yaml\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 templates\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 cluster.yaml\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 capi-cluster-autoscaler\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Chart.yaml\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 templates\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 callhomeconfig.yaml\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 rbac.yaml\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 xplane-aws-eks\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 Chart.yaml\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 templates\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 cluster.yaml\n    \u2502\u00a0\u00a0         \u2514\u2500\u2500 network.yaml\n    \u251c\u2500\u2500 templates\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 clusterregistration.yaml\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 ns.yaml\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 profile.yaml\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 rbac.yaml\n    \u2514\u2500\u2500 values.yaml\n</code></pre> <p>At this point, the cluster is provisioning. To monitor the progress of the cluster deployment, check the status of the ArgoCD app of the same name. Eventually, the ArgoCD apps will be synced and healthy.</p> <pre><code>$ argocd app list\n\nNAME                           CLUSTER                         NAMESPACE  PROJECT  STATUS     HEALTH   SYNCPOLICY  CONDITIONS  REPO                                            PATH                             TARGET\ncas-cluster                    https://kubernetes.default.svc  default    default Synced  Healthy  Auto-Prune  &lt;none&gt;  ${WORKSPACE_REPO_URL}   clusters/cas-cluster/mgmt        main\ncas-cluster-cas-bundle           cas-cluster                 default    default  Synced     Healthy  Auto-Prune  &lt;none&gt;   https://github.com/arlonproj/arlon.git   bundles/capi-cluster-autoscaler  HEAD\ncas-cluster-profile-dynamic-cas  https://kubernetes.default.svc  argocd     default  Synced     Healthy  Auto-Prune  &lt;none&gt;      ${WORKSPACE_REPO_URL} profiles/dynamic-cas/mgmt\n</code></pre>"}]}