{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 \u00b6 What Is Arlon? \u00b6 Arlon is a declarative, gitops based fleet management tool for Kubernetes clusters. It allows administrators to: Deploy and upgrade a large number of workload clusters Secure clusters by installing and configuring policies Install a set of applications / add-ons on the clusters all in a structured, predictable manner. Arlon makes Kubernetes cluster fleet management secure, version controlled, auditable and easy to perform at scale. Arlon takes advantage of multiple declarative cluster management API providers for the actual cluster orchestration. The first two supported API providers are Cluster API and Crossplane. Arlon uses ArgoCD as the underlying Kubernetes manifest deployment and enforcement engine. A workload cluster is composed of the following constructs: Cluster spec : a description of the infrastructure and external settings of a cluster, e.g. Kubernetes version, cloud provider, cluster type, node instance type. Profile : a grouping of configuration bundles which will be installed into the cluster Configuration bundle : a unit of configuration which contains (or references) one or more Kubernetes manifests. A bundle can encapsulate anything that can be deployed onto a cluster: an RBAC ruleset, an add-on, an application, etc... Arlon Benefits \u00b6 Improves time to market by enabling better velocity for developers through infrastructure management that is more fluid and agile. Define, store, change and enforce your cluster infrastructure & application add-ons at scale. Reduces the risk of unexpected infrastructure downtime and outages, or unexpected security misconfiguration, with consistent management of infrastructure and security policies. Allows IT and Platform Ops admins to operate large scale of clusters, infrastructure & add-ons with significantly reduced team size & operational overhead, using GitOps. Contents \u00b6 Concepts Installation Tutorial (gen-1) Tutorial (gen-2) Architecture","title":"Overview"},{"location":"#introduction","text":"","title":"Introduction"},{"location":"#_1","text":"","title":""},{"location":"#what-is-arlon","text":"Arlon is a declarative, gitops based fleet management tool for Kubernetes clusters. It allows administrators to: Deploy and upgrade a large number of workload clusters Secure clusters by installing and configuring policies Install a set of applications / add-ons on the clusters all in a structured, predictable manner. Arlon makes Kubernetes cluster fleet management secure, version controlled, auditable and easy to perform at scale. Arlon takes advantage of multiple declarative cluster management API providers for the actual cluster orchestration. The first two supported API providers are Cluster API and Crossplane. Arlon uses ArgoCD as the underlying Kubernetes manifest deployment and enforcement engine. A workload cluster is composed of the following constructs: Cluster spec : a description of the infrastructure and external settings of a cluster, e.g. Kubernetes version, cloud provider, cluster type, node instance type. Profile : a grouping of configuration bundles which will be installed into the cluster Configuration bundle : a unit of configuration which contains (or references) one or more Kubernetes manifests. A bundle can encapsulate anything that can be deployed onto a cluster: an RBAC ruleset, an add-on, an application, etc...","title":"What Is Arlon?"},{"location":"#arlon-benefits","text":"Improves time to market by enabling better velocity for developers through infrastructure management that is more fluid and agile. Define, store, change and enforce your cluster infrastructure & application add-ons at scale. Reduces the risk of unexpected infrastructure downtime and outages, or unexpected security misconfiguration, with consistent management of infrastructure and security policies. Allows IT and Platform Ops admins to operate large scale of clusters, infrastructure & add-ons with significantly reduced team size & operational overhead, using GitOps.","title":"Arlon Benefits"},{"location":"#contents","text":"Concepts Installation Tutorial (gen-1) Tutorial (gen-2) Architecture","title":"Contents"},{"location":"appprofiles/","text":"Application Profiles (new in v0.10.0) \u00b6 The Application Profiles feature, also known as Gen2 Profiles, is an addition to Arlon v0.10.0 that provides a new way to describe, group, and deploy manifests to workload clusters. The new feature introduces these concepts: * Arlon Application (or just \"App\") * Application Profile (a.k.a. AppProfile) * Targeting application profiles to workload clusters via annotation The feature provides an alternative to the Bundle and Profile concepts (\"the old way\") of earlier versions of Arlon. Specifically, the Arlon Application can be viewed as a replacement for Bundles, and AppProfile is a substitute for Profile. In release v0.10.0, the \"old way\" continues to be supported, but is deprecated, meaning it will likely be retired in an upcoming release. Arlon Application (a.k.a. \"Arlon App\") \u00b6 An Arlon Application is similar to a Dynamic Bundle from earlier releases. It specifies a source of one or more manifests stored in git in any \"tool\" format supported by ArgoCD Internally, Arlon represents an App as a specialized ArgoCD ApplicationSet resource. This allows you to specify the manifest source in the spec.template.spec section, while Arlon takes care of targeting the deployment to the correct workload cluster(s) by automatically manipulating the spec.generators section. ApplicationSets managed by Arlon to represent apps are distinguished from other ApplicationSets via the arlon-type=application label. The ApplicationSet's Generators list must contain a single generator of List type. The Arlon AppProfile controller will modify this list in real-time to deploy to application to the right workload clusters (or no cluster at all). While it is possible for you create and edit an ApplicationSet resource manifest satisfying the requirements to be an Arlon App from scratch, Arlon makes this easier with the arlon app create --output-yaml command, which outputs an initial compliant manifest that you can save to a file and edit to your liking before applying to the management cluster to actually create the app. (Without the `--output-yaml flag, the command will apply the resource for you). Here's an example of an initial Arlon Application manifest: apiVersion : argoproj.io/v1alpha1 kind : ApplicationSet metadata : labels : arlon-type : application managed-by : arlon name : myconfigmap namespace : argocd spec : generators : - list : {} template : metadata : name : '{{cluster_name}}-app-myconfigmap' spec : destination : namespace : default server : '{{cluster_server}}' project : default source : path : apps/my-cfgmap-1 repoURL : https://github.com/bcle/fleet-infra.git targetRevision : HEAD syncPolicy : automated : prune : true The List generator that the AppProfile controller maintains supplies two variables for template substitution: - cluster_name : the name of the target workload cluster - cluster_server : the URL+FQDN of the workload cluster's Kubernetes API endpoint Notice how the initial manifest takes advantage of those variables to set - spec.template.metadata.name to {{cluster-name}}-app-myconfigmap to ensure that any actual ArgoCD Application resources deployed from the ApplicationSet are uniquely named, by prefixing the cluster name. - spec.template.spec.destination.server to {{cluster_server}} to target the correct workload cluster Arlon apps can be listed in two ways. The first is to use the arlon app list command. One advantage is that it's simple to use and also displays additional information about the app, such as which AppProfiles are currently associated with the app. Example: $ arlon app list NAME REPO PATH REVISION APP_PROFILES myconfigmap https://github.com/bcle/fleet-infra.git apps/my-cfgmap-1 HEAD [ marketing ] The second way is to use pure kubectl to list ApplicationSets with a particular label: $ kubectl -n argocd get applicationset NAME AGE myconfigmap 21d Similarly, an Arlon app can be deleted in two ways: - arlon app delete <appName> - kubectl -n argocd delete applicationset <appName> AppProfile \u00b6 An AppProfile is simply a grouping (or set) of Arlon Apps. Unlike an Arlon Application (which is represented by an ApplicationSet resource), an AppProfile is represented by an Arlon-native custom resource. An AppProfile specifies the apps it is associated with via the appNames list. It is legal for appNames to contain names of Arlon Apps that don't exist yet. To indicate whether some app names are currently invalid, the AppProfile controller will update the resource's status section as follows: - If all specified app names refer to valid Arlon apps, status.health is set to healthy . - If one or more specified app names refer to non-existent Arlon apps, then status.health is set to degraded , and status.invalidAppNames lists the invalid names. Here is an example of an AppProfile manifest that includes 3 apps, one of which does not exist: apiVersion : core.arlon.io/v1 kind : AppProfile metadata : name : marketing namespace : arlon spec : appNames : - myconfigmap - wordpress - nonexistent-app status : health : degraded invalidAppNames : - nonexistent-app Note: AppProfile resources reside in the arlon namespace. This is in contrast to Arlon App resources, which reside in the argocd namespace since they are actually ArgoCD ApplicationSet resources. Since AppProfiles are defined by their own custom resource and are fairly straightforward, their lifecycle can be managed entirely using kubectl . That said, Arlon provides the arlon appprofile list to display useful information about current AppProfiles. Example: $ arlon appprofile list NAME APPS HEALTH INVALID_APPS engineering [wordpress] healthy [] marketing [myconfigmap wordpress] degraded [nonexistent-app] As the example illustrates, it is totally legal for two or more AppProfiles to include the same app(s). When those AppProfiles are targeted to a workload cluster, the cluster receives the union of these app sets. Targeting AppProfiles to Workload Clusters \u00b6 Apps are actually deployed to workload clusters by annotating the desired cluster(s) with arlon.io/profiles=<comma separated list of AppProfiles> . This is supported only on Arlon Gen2 clusters, which themselves are represented as ArgoCD Application resources. Example: suppose we have a Gen2 workload cluster named clust1 . Since the cluster is represented as an ArgoCD Application resource, targeting the engineering and marketing app profiles to it is achieved with: kubectl -n argocd annotate --overwrite application clust1 arlon.io/profiles=engineering,marketing The set of app profiles for a cluster can always be updated that way, in real time. Behind the scenes, Arlon's AppProfile controller updates the spec.generators section of each affected app to target the cluster. The set of apps targeting a cluster is the union of all the valid appNames from every app profile listed in the annotation. By default, every ArgoCD Application Resource generated by the app's ApplicationSet is named with the pattern <clustername>-app-<appname> . In the running example, these two ArgoCD application resources will be generated: - clust1-app-wordpress - clust1-app-myconfigmap To detach all profiles from a cluster, simply remove the annotation: kubectl -n argocd annotate application clust1 arlon.io/profiles- The last command will automatically destroy any ArgoCD application resources generated for that cluster (assuming they originate from Arlon apps). Note: users are free to create and maintain their own ApplicationSets not managed by Arlon. Those will work side-by-side with the ones managed by Arlon, which is unaware of those other ApplicationSets. This allows the user to take advantage of other types of ApplicationSet generators .","title":"Application Profiles (new in v0.10.0)"},{"location":"appprofiles/#application-profiles-new-in-v0100","text":"The Application Profiles feature, also known as Gen2 Profiles, is an addition to Arlon v0.10.0 that provides a new way to describe, group, and deploy manifests to workload clusters. The new feature introduces these concepts: * Arlon Application (or just \"App\") * Application Profile (a.k.a. AppProfile) * Targeting application profiles to workload clusters via annotation The feature provides an alternative to the Bundle and Profile concepts (\"the old way\") of earlier versions of Arlon. Specifically, the Arlon Application can be viewed as a replacement for Bundles, and AppProfile is a substitute for Profile. In release v0.10.0, the \"old way\" continues to be supported, but is deprecated, meaning it will likely be retired in an upcoming release.","title":"Application Profiles (new in v0.10.0)"},{"location":"appprofiles/#arlon-application-aka-arlon-app","text":"An Arlon Application is similar to a Dynamic Bundle from earlier releases. It specifies a source of one or more manifests stored in git in any \"tool\" format supported by ArgoCD Internally, Arlon represents an App as a specialized ArgoCD ApplicationSet resource. This allows you to specify the manifest source in the spec.template.spec section, while Arlon takes care of targeting the deployment to the correct workload cluster(s) by automatically manipulating the spec.generators section. ApplicationSets managed by Arlon to represent apps are distinguished from other ApplicationSets via the arlon-type=application label. The ApplicationSet's Generators list must contain a single generator of List type. The Arlon AppProfile controller will modify this list in real-time to deploy to application to the right workload clusters (or no cluster at all). While it is possible for you create and edit an ApplicationSet resource manifest satisfying the requirements to be an Arlon App from scratch, Arlon makes this easier with the arlon app create --output-yaml command, which outputs an initial compliant manifest that you can save to a file and edit to your liking before applying to the management cluster to actually create the app. (Without the `--output-yaml flag, the command will apply the resource for you). Here's an example of an initial Arlon Application manifest: apiVersion : argoproj.io/v1alpha1 kind : ApplicationSet metadata : labels : arlon-type : application managed-by : arlon name : myconfigmap namespace : argocd spec : generators : - list : {} template : metadata : name : '{{cluster_name}}-app-myconfigmap' spec : destination : namespace : default server : '{{cluster_server}}' project : default source : path : apps/my-cfgmap-1 repoURL : https://github.com/bcle/fleet-infra.git targetRevision : HEAD syncPolicy : automated : prune : true The List generator that the AppProfile controller maintains supplies two variables for template substitution: - cluster_name : the name of the target workload cluster - cluster_server : the URL+FQDN of the workload cluster's Kubernetes API endpoint Notice how the initial manifest takes advantage of those variables to set - spec.template.metadata.name to {{cluster-name}}-app-myconfigmap to ensure that any actual ArgoCD Application resources deployed from the ApplicationSet are uniquely named, by prefixing the cluster name. - spec.template.spec.destination.server to {{cluster_server}} to target the correct workload cluster Arlon apps can be listed in two ways. The first is to use the arlon app list command. One advantage is that it's simple to use and also displays additional information about the app, such as which AppProfiles are currently associated with the app. Example: $ arlon app list NAME REPO PATH REVISION APP_PROFILES myconfigmap https://github.com/bcle/fleet-infra.git apps/my-cfgmap-1 HEAD [ marketing ] The second way is to use pure kubectl to list ApplicationSets with a particular label: $ kubectl -n argocd get applicationset NAME AGE myconfigmap 21d Similarly, an Arlon app can be deleted in two ways: - arlon app delete <appName> - kubectl -n argocd delete applicationset <appName>","title":"Arlon Application (a.k.a. \"Arlon App\")"},{"location":"appprofiles/#appprofile","text":"An AppProfile is simply a grouping (or set) of Arlon Apps. Unlike an Arlon Application (which is represented by an ApplicationSet resource), an AppProfile is represented by an Arlon-native custom resource. An AppProfile specifies the apps it is associated with via the appNames list. It is legal for appNames to contain names of Arlon Apps that don't exist yet. To indicate whether some app names are currently invalid, the AppProfile controller will update the resource's status section as follows: - If all specified app names refer to valid Arlon apps, status.health is set to healthy . - If one or more specified app names refer to non-existent Arlon apps, then status.health is set to degraded , and status.invalidAppNames lists the invalid names. Here is an example of an AppProfile manifest that includes 3 apps, one of which does not exist: apiVersion : core.arlon.io/v1 kind : AppProfile metadata : name : marketing namespace : arlon spec : appNames : - myconfigmap - wordpress - nonexistent-app status : health : degraded invalidAppNames : - nonexistent-app Note: AppProfile resources reside in the arlon namespace. This is in contrast to Arlon App resources, which reside in the argocd namespace since they are actually ArgoCD ApplicationSet resources. Since AppProfiles are defined by their own custom resource and are fairly straightforward, their lifecycle can be managed entirely using kubectl . That said, Arlon provides the arlon appprofile list to display useful information about current AppProfiles. Example: $ arlon appprofile list NAME APPS HEALTH INVALID_APPS engineering [wordpress] healthy [] marketing [myconfigmap wordpress] degraded [nonexistent-app] As the example illustrates, it is totally legal for two or more AppProfiles to include the same app(s). When those AppProfiles are targeted to a workload cluster, the cluster receives the union of these app sets.","title":"AppProfile"},{"location":"appprofiles/#targeting-appprofiles-to-workload-clusters","text":"Apps are actually deployed to workload clusters by annotating the desired cluster(s) with arlon.io/profiles=<comma separated list of AppProfiles> . This is supported only on Arlon Gen2 clusters, which themselves are represented as ArgoCD Application resources. Example: suppose we have a Gen2 workload cluster named clust1 . Since the cluster is represented as an ArgoCD Application resource, targeting the engineering and marketing app profiles to it is achieved with: kubectl -n argocd annotate --overwrite application clust1 arlon.io/profiles=engineering,marketing The set of app profiles for a cluster can always be updated that way, in real time. Behind the scenes, Arlon's AppProfile controller updates the spec.generators section of each affected app to target the cluster. The set of apps targeting a cluster is the union of all the valid appNames from every app profile listed in the annotation. By default, every ArgoCD Application Resource generated by the app's ApplicationSet is named with the pattern <clustername>-app-<appname> . In the running example, these two ArgoCD application resources will be generated: - clust1-app-wordpress - clust1-app-myconfigmap To detach all profiles from a cluster, simply remove the annotation: kubectl -n argocd annotate application clust1 arlon.io/profiles- The last command will automatically destroy any ArgoCD application resources generated for that cluster (assuming they originate from Arlon apps). Note: users are free to create and maintain their own ApplicationSets not managed by Arlon. Those will work side-by-side with the ones managed by Arlon, which is unaware of those other ApplicationSets. This allows the user to take advantage of other types of ApplicationSet generators .","title":"Targeting AppProfiles to Workload Clusters"},{"location":"architecture/","text":"Architecture \u00b6 Arlon is composed of a controller, a library, and a CLI that exposes the library's functions as commands. In the future, an API server may be built from the library as well. Arlon adds CRDs (custom resource definitions) for several custom resources such as ClusterRegistration and Profile. Management cluster \u00b6 The management cluster is a Kubernetes cluster hosting all the components needed by Arlon, including: The ArgoCD server The Arlon \"database\" (implemented as Kubernetes secrets and configmaps) The Arlon controller Cluster management API providers: Cluster API or Crossplane Custom resources (CRs) that drive the involved providers and controllers Custom resource definitions (CRDs) for all the involved CRs The user is responsible for supplying the management cluster, and to have access to a kubeconfig granting administrator permissions on the cluster. Controller \u00b6 The Arlon controller observes and responds to changes in clusterregistration custom resources. The Arlon library creates a clusterregistration at the beginning of workload cluster creation, causing the controller to wait for the cluster's kubeconfig to become available, at which point it registers the cluster with ArgoCD to enable manifests described by bundles to be deployed to the cluster. Library \u00b6 The Arlon library is a Go module that contains the functions that communicate with the Management Cluster to manipulate the Arlon state (bundles, profiles, clusterspecs) and transforms them into git directory structures to drive ArgoCD's gitops engine. Initially, the library is exposed via a CLI utility. In the future, it may also be embodied into a server an exposed via a network API. Workspace repository \u00b6 As mentioned earlier, Arlon creates and maintains directory structures in a git repository to drive ArgoCD sync operations. The user is responsible for supplying this workspace repository (and base paths) hosting those structures. Arlon relies on ArgoCD for repository registration, therefore the user should register the workspace registry in ArgoCD before referencing it from Arlon data types. Starting from release v0.9.0, Arlon now includes two commands to help with managing various git repository URLs. With these commands in place, the --repo-url flag in commands requiring a hosted git repository is no longer needed. A more detailed explanation is given in the next section . Repo Aliases \u00b6 A repo(repository) alias allows an Arlon user to register a GitHub repository with ArgoCD and store a local configuration file on their system that can be referenced by the CLI to then determine a repository URL and fetch its credentials when needed. All commands that require a repository, support a --repo-url flag also support a repo-alias flag to specify an alias instead of an alias, such commands will consider the \"default\" alias to be used when no --repo-alias and no --repo-url flags are given. There are two subcommands i.e., arlon git register and arlon git unregister which allow for a basic form of git repository context management. When arlon git register is run it requires a repo URL, the username, the access token and an optional alias(which defaults to \u201cdefault\u201d)- if a \u201cdefault\u201d alias already exists, the repo isn\u2019t registered with argocd and the alias creation fails saying that the default alias already exists otherwise, the repo is registered with argocd . Lastly we also write this repository information to the local configuration file. This contains two pieces of information for each repository- it\u2019s URL and the alias. The structure of the file is as shown: { \"default\" : { \"url\" : \"\" , \"alias\" : \"default\" }, \"repos\" : [ { \"url\" : \"\" , \"alias\" : \"default\" }, { \"url\" : \"\" , \"alias\" : \"not_default\" }, {} ] } On running arlon git unregister ALIAS , it removes that entry from the configuration file. However, it does NOT remove the repository from argocd . When the \"default\" alias is deleted, we also clear the \"default\" entry from the JSON file. Examples \u00b6 Given below are some examples for registering and unregistering a repository. Registering Repositories \u00b6 Registering a repository requires the repository link, the GitHub username( --user ), and a personal access token( --password ). When the --password flag isn't provided at the command line, the CLI will prompt for a password(this is the recommended approach). arlon git register https://github.com/GhUser/manifests --user GhUser arlon git register https://github.com/GhUser/prod-manifests --user GhUser --alias prod For non-interactive registrations, the --password flag can be used. export GH_PAT = \"...\" arlon git register https://github.com/GhUser/manifests --user GhUser --password $GH_PAT arlon git register https://github.com/GhUser/prod-manifests --user GhUser --alias prod --password $GH_PAT Unregistering Repositories \u00b6 Unregistering an alias only requires a positional argument: the repository alias. # unregister the default alias locally arlon git unregister default # unregister some other alias locally arlon git unregister prod","title":"Architecture"},{"location":"architecture/#architecture","text":"Arlon is composed of a controller, a library, and a CLI that exposes the library's functions as commands. In the future, an API server may be built from the library as well. Arlon adds CRDs (custom resource definitions) for several custom resources such as ClusterRegistration and Profile.","title":"Architecture"},{"location":"architecture/#management-cluster","text":"The management cluster is a Kubernetes cluster hosting all the components needed by Arlon, including: The ArgoCD server The Arlon \"database\" (implemented as Kubernetes secrets and configmaps) The Arlon controller Cluster management API providers: Cluster API or Crossplane Custom resources (CRs) that drive the involved providers and controllers Custom resource definitions (CRDs) for all the involved CRs The user is responsible for supplying the management cluster, and to have access to a kubeconfig granting administrator permissions on the cluster.","title":"Management cluster"},{"location":"architecture/#controller","text":"The Arlon controller observes and responds to changes in clusterregistration custom resources. The Arlon library creates a clusterregistration at the beginning of workload cluster creation, causing the controller to wait for the cluster's kubeconfig to become available, at which point it registers the cluster with ArgoCD to enable manifests described by bundles to be deployed to the cluster.","title":"Controller"},{"location":"architecture/#library","text":"The Arlon library is a Go module that contains the functions that communicate with the Management Cluster to manipulate the Arlon state (bundles, profiles, clusterspecs) and transforms them into git directory structures to drive ArgoCD's gitops engine. Initially, the library is exposed via a CLI utility. In the future, it may also be embodied into a server an exposed via a network API.","title":"Library"},{"location":"architecture/#workspace-repository","text":"As mentioned earlier, Arlon creates and maintains directory structures in a git repository to drive ArgoCD sync operations. The user is responsible for supplying this workspace repository (and base paths) hosting those structures. Arlon relies on ArgoCD for repository registration, therefore the user should register the workspace registry in ArgoCD before referencing it from Arlon data types. Starting from release v0.9.0, Arlon now includes two commands to help with managing various git repository URLs. With these commands in place, the --repo-url flag in commands requiring a hosted git repository is no longer needed. A more detailed explanation is given in the next section .","title":"Workspace repository"},{"location":"architecture/#repo-aliases","text":"A repo(repository) alias allows an Arlon user to register a GitHub repository with ArgoCD and store a local configuration file on their system that can be referenced by the CLI to then determine a repository URL and fetch its credentials when needed. All commands that require a repository, support a --repo-url flag also support a repo-alias flag to specify an alias instead of an alias, such commands will consider the \"default\" alias to be used when no --repo-alias and no --repo-url flags are given. There are two subcommands i.e., arlon git register and arlon git unregister which allow for a basic form of git repository context management. When arlon git register is run it requires a repo URL, the username, the access token and an optional alias(which defaults to \u201cdefault\u201d)- if a \u201cdefault\u201d alias already exists, the repo isn\u2019t registered with argocd and the alias creation fails saying that the default alias already exists otherwise, the repo is registered with argocd . Lastly we also write this repository information to the local configuration file. This contains two pieces of information for each repository- it\u2019s URL and the alias. The structure of the file is as shown: { \"default\" : { \"url\" : \"\" , \"alias\" : \"default\" }, \"repos\" : [ { \"url\" : \"\" , \"alias\" : \"default\" }, { \"url\" : \"\" , \"alias\" : \"not_default\" }, {} ] } On running arlon git unregister ALIAS , it removes that entry from the configuration file. However, it does NOT remove the repository from argocd . When the \"default\" alias is deleted, we also clear the \"default\" entry from the JSON file.","title":"Repo Aliases"},{"location":"architecture/#examples","text":"Given below are some examples for registering and unregistering a repository.","title":"Examples"},{"location":"architecture/#registering-repositories","text":"Registering a repository requires the repository link, the GitHub username( --user ), and a personal access token( --password ). When the --password flag isn't provided at the command line, the CLI will prompt for a password(this is the recommended approach). arlon git register https://github.com/GhUser/manifests --user GhUser arlon git register https://github.com/GhUser/prod-manifests --user GhUser --alias prod For non-interactive registrations, the --password flag can be used. export GH_PAT = \"...\" arlon git register https://github.com/GhUser/manifests --user GhUser --password $GH_PAT arlon git register https://github.com/GhUser/prod-manifests --user GhUser --alias prod --password $GH_PAT","title":"Registering Repositories"},{"location":"architecture/#unregistering-repositories","text":"Unregistering an alias only requires a positional argument: the repository alias. # unregister the default alias locally arlon git unregister default # unregister some other alias locally arlon git unregister prod","title":"Unregistering Repositories"},{"location":"clustertemplate/","text":"Next-gen Cluster Provisioning using Cluster templates \u00b6 This document describes a new way of provisioning workload clusters in Arlon. The most significant change is the Cluster Template construct, which replaces the older ClusterSpec from gen1 clusters. To distinguish them from the older gen1 clusters, the ones deployed from a cluster template are called next-gen clusters or gen2 clusters. Goals \u00b6 Allow users to deploy arbitrarily complex clusters using the full Cluster API feature set. Fully declarative and gitops compatible: a cluster deployment should be composed of one or more self-sufficient manifests that the user can choose to either apply directly (via kubectl) or store in git for later-stage deployment by a gitops tool (mainly ArgoCD). Support Linked Mode update: an update to the the cluster template should automatically propagate to all workload clusters deployed from it. Profile support \u00b6 While profiles are also being re-architected, the first implementation of next-gen clusters fully integrates with current-generation profiles, which are expressed as Profile custom resources and compiled into a set of intermediate files in a git repository. Profiles are optional, and a next-gen cluster can be created without a profile. One can be attached later. Architecture diagram \u00b6 This example shows a cluster template named capi-quickstart used to deploy two workload clusters cluster-a and cluster-b . Additionally, cluster-a is given profile xxx , while cluster-b is given profile yyy . Cluster Template \u00b6 A cluster template serves as a base for creating new workload clusters. The workload clusters are all exact copies of the cluster template, meaning that they acquire all unmodified resources of the cluster template, except for: resource names, which are prefixed during the cluster creation process to make them unique to avoid conflicts the namespace, which is set to a new namespace unique to the workload cluster Preparation \u00b6 To create a cluster template, a user first creates a single YAML file containing the desired Cluster API cluster and all related resources (e.g. MachineDeployments, etc...), using whatever tool the user chooses (e.g. clusterctl generate cluster ). The user is responsible for the correctness of the file and resources within. Arlon will not check for errors. For example, the specified Kubernetes version must be supported by the Cluster API providers currently installed in the management cluster. If it isn't, resulting clusters will fail and enter a perpetual OutOfSync state. The user then commits and pushes the manifest file to a dedicated directory in a git repository. The name of the cluster resource does not matter, it will be used as a suffix during workload cluster creation. The directory should be unique to the file, and not contain any other files. If not already registered, the git repository should also be registered in ArgoCD with the proper credentials for read/write access. To check whether the git directory is a compliant Arlon cluster template, the user runs: arlon clustertemplate validategit --repo-url <repoUrl> --repo-path <pathToDirectory> [ --repo-revision revision ] Note: if --repo-revision is not specified, it defaults to main. The command produces an error the first time because the git directory has not yet been \"prepped\". To \"prep\" the directory to become a compliant Arlon cluster template, the user runs: arlon clustertemplate preparegit --repo-url <repoUrl> --repo-path <pathToDirectory> [ --repo-revision revision ] This pushes a commit to the repo with these changes: A kustomization.yaml file is added to the directory to make the manifest customizable by Kustomize. A configurations.yaml file is added to configure the namereference Kustomize plugin which ensures reference fields are correctly set when pointing to resource names that ArgoCD will modify using the Kustomize nameprefix mechanism. The content of the file is sourced from this Scott Lowe blog article . All namespace properties in the cluster manifest are removed to allow Kustomize to override the namespace of all resources. If prep is successful, another invocation of arlon clustertemplate validategit should succeed as well. Workload clusters \u00b6 Creation \u00b6 Use arlon cluster create to create a next-gen workload cluster from a cluster template ( this is different from arlon cluster deploy for creating current-generation clusters ). The command creates between 2 and 3 (depending on whether a profile is used) ArgoCD application resources that together make up the cluster and its contents. The general usage is: arlon cluster create --cluster-name <clusterName> --repo-url <repoUrl> --repo-path <pathToDirectory> [ --output-yaml ] [ --profile <profileName> ] [ --repo-revision <repoRevision> ] The command supports two modes of operation: With --output-yaml : output a list of YAML resources that you can inspect, save to a file, or pipe to kubectl apply -f Without --output-yaml : create the application resources directly in the management cluster currently referenced by your KUBECONFIG and context. The --profile flag is optional; a cluster can be created with no profile. Composition \u00b6 A workload cluster is composed of 2 to 3 ArgoCD application resources, which are named based on the name of the cluster template and the workload cluster. For illustration purposes, the following discussion assumes that the cluster template is named capi-quickstart , the workload cluster is named cluster-a , and the optional profile is named xxx . Cluster app \u00b6 The cluster-a application is the cluster app for the cluster. It is responsible for deploying the cluster template resources, meaning the Cluster API manifests. It is named directly from the workload cluster name. The application's spec uses a ApplicationSourceKustomize that points to the cluster template's git directory. The spec ensures that all deployed resources are configured to: Reside in the cluster-a namespace, which is deployed by the arlon app (see below). This achieved by setting app.Spec.Destination.Namespace to the workload cluster's name ( this only works if the resources do not specify an explicit namespace; this requirement is taken care of by the \"prep\" step on the cluster template*). Be named cluster-a-capi-quickstart , meaning the workload cluster name followed by the cluster template name. This is achieved by setting app.Spec.Source.Kustomize.NamePrefix to the workload cluster name plus a hyphen. Arlon app \u00b6 The cluster-a-arlon application is the arlon app for the cluster. It is resposible for deploying: The cluster-a namespace, which holds most resources related to this workload cluster, such as the Cluster API manifests deployed by the cluster app. Resources required to register the workload cluster with argocd when available: ClusterRegistration and associated RBAC rules. Additional resources (service account, more RBAC rules) for Cluster Autoscaler if enabled. The application spec's ApplicationSource points to the existing Arlon Helm chart located here by default: Repo: https://github.com/arlonproj/arlon.git Revision: private/leb/gen2 (IMPORTANT: NEEDS TO CHANGE TO STABLE BRANCH OR TAG) Path: pkg/cluster/manifests This is the same Helm chart that current-generation clusters are deployed from, using arlon cluster deploy . When used for the arlon app for a next-gen cluster, the Helm parameters are configured to only deploy the Arlon resources, with the subchart for cluster resources disabled, since those resources will be deployed by the cluster app. Important issue : as described above, the application source resides in the public Arlon repo. To avoid breaking user's deployed clusters, the source must be stable and not change! This probably means a particular Arlon release should point the source to a stable tag (not even a branch?) As an alternative, during Arlon setup, allow the user to copy the Helm chart into a private repo, and point the source there. Profile app (optional) \u00b6 A next-gen cluster can be assigned a current-gen dynamic profile, in which case Arlon creates a profile app named <clusterName>-profile-<profileName> , or cluster-a-profile-xxx in the running example. This is similar to the profile app created when attaching a profile app to an external cluster. The application source points to the git location of the dynamic profile. Teardown \u00b6 Since a next-gen cluster is composed of multiple ArgoCD applications, destroying the cluster requires deleting all of its applications. To facilitate this, the 2 or 3 applications created by arlon cluster create are automatically labeled with arlon-cluster=<clusterName> . The user has two options for destroying a next-gen cluster: The easiest way: arlon cluster delete <clusterName> . This command automatically detects a next-gen cluster and cleans up all related applications. A more manual way: kubectl delete application -l arlon-cluster=<clusterName> Update Semantics \u00b6 A cluster template lives in git and is shared by all workload clusters created from it. This is sometimes referred to as Linked Mode . Any git update to the cluster can affect the associated workload clusters, therefore such updates must be planned and managed with care; there is a real risk of such an update breaking existing clusters. By default, a workload's cluster cluster app is configured with auto-sync, meaning ArgoCD will immediately apply any changes in the cluster template to the deployed Cluster API cluster resources. In general, a cluster template does not need to be \"prepped\" again after a modification to its main manifest file (the one containing the Cluster API resources). So the user is free to edit the manifest directly, commit/push the changes, and expect to see immediate changes to already-deployed clusters created from that cluster template. Unsupported changes \u00b6 The controllers for Cluster API and its providers disallow changes to some fields belonging to already-deployed resources. For example, changing the cluster template name ( medata.Name of the Cluster resource) will have disastrous consequences on already-deployed clusters, causing many resources to enter the OutOfSync state and never recover because ArgoCD fails to apply the changes (they are rejected by the controllers). Consequently, a user should never change the name of a cluster template. Besides the cluster name, other fields cannot change (this has been observed anecdotally, we don't yet have an exhaustive list). Changing the Kubernetes version of the control plane or data plane is supported, so long as the new version is supported by the relevant providers. If accepted, such a change will result in a rolling update of the corresponding plane. Specific to AWS: the AWSMachineTemplate.spec is immutable and a CAPI webhook disallows such updates. The user is advised to not make such modifications to a cluster template manifest. In the event that such an event does happen, the user is advised to not manually sync in those changes via argocd . If a new cluster with a different AWSMachineTemplate.spec is desired, the recommended approach is to make a copy of the manifests in the workspace repository and then issue an arlon cluster create command which would then consume this manifest.","title":"Cluster template"},{"location":"clustertemplate/#next-gen-cluster-provisioning-using-cluster-templates","text":"This document describes a new way of provisioning workload clusters in Arlon. The most significant change is the Cluster Template construct, which replaces the older ClusterSpec from gen1 clusters. To distinguish them from the older gen1 clusters, the ones deployed from a cluster template are called next-gen clusters or gen2 clusters.","title":"Next-gen Cluster Provisioning using Cluster templates"},{"location":"clustertemplate/#goals","text":"Allow users to deploy arbitrarily complex clusters using the full Cluster API feature set. Fully declarative and gitops compatible: a cluster deployment should be composed of one or more self-sufficient manifests that the user can choose to either apply directly (via kubectl) or store in git for later-stage deployment by a gitops tool (mainly ArgoCD). Support Linked Mode update: an update to the the cluster template should automatically propagate to all workload clusters deployed from it.","title":"Goals"},{"location":"clustertemplate/#profile-support","text":"While profiles are also being re-architected, the first implementation of next-gen clusters fully integrates with current-generation profiles, which are expressed as Profile custom resources and compiled into a set of intermediate files in a git repository. Profiles are optional, and a next-gen cluster can be created without a profile. One can be attached later.","title":"Profile support"},{"location":"clustertemplate/#architecture-diagram","text":"This example shows a cluster template named capi-quickstart used to deploy two workload clusters cluster-a and cluster-b . Additionally, cluster-a is given profile xxx , while cluster-b is given profile yyy .","title":"Architecture diagram"},{"location":"clustertemplate/#cluster-template","text":"A cluster template serves as a base for creating new workload clusters. The workload clusters are all exact copies of the cluster template, meaning that they acquire all unmodified resources of the cluster template, except for: resource names, which are prefixed during the cluster creation process to make them unique to avoid conflicts the namespace, which is set to a new namespace unique to the workload cluster","title":"Cluster Template"},{"location":"clustertemplate/#preparation","text":"To create a cluster template, a user first creates a single YAML file containing the desired Cluster API cluster and all related resources (e.g. MachineDeployments, etc...), using whatever tool the user chooses (e.g. clusterctl generate cluster ). The user is responsible for the correctness of the file and resources within. Arlon will not check for errors. For example, the specified Kubernetes version must be supported by the Cluster API providers currently installed in the management cluster. If it isn't, resulting clusters will fail and enter a perpetual OutOfSync state. The user then commits and pushes the manifest file to a dedicated directory in a git repository. The name of the cluster resource does not matter, it will be used as a suffix during workload cluster creation. The directory should be unique to the file, and not contain any other files. If not already registered, the git repository should also be registered in ArgoCD with the proper credentials for read/write access. To check whether the git directory is a compliant Arlon cluster template, the user runs: arlon clustertemplate validategit --repo-url <repoUrl> --repo-path <pathToDirectory> [ --repo-revision revision ] Note: if --repo-revision is not specified, it defaults to main. The command produces an error the first time because the git directory has not yet been \"prepped\". To \"prep\" the directory to become a compliant Arlon cluster template, the user runs: arlon clustertemplate preparegit --repo-url <repoUrl> --repo-path <pathToDirectory> [ --repo-revision revision ] This pushes a commit to the repo with these changes: A kustomization.yaml file is added to the directory to make the manifest customizable by Kustomize. A configurations.yaml file is added to configure the namereference Kustomize plugin which ensures reference fields are correctly set when pointing to resource names that ArgoCD will modify using the Kustomize nameprefix mechanism. The content of the file is sourced from this Scott Lowe blog article . All namespace properties in the cluster manifest are removed to allow Kustomize to override the namespace of all resources. If prep is successful, another invocation of arlon clustertemplate validategit should succeed as well.","title":"Preparation"},{"location":"clustertemplate/#workload-clusters","text":"","title":"Workload clusters"},{"location":"clustertemplate/#creation","text":"Use arlon cluster create to create a next-gen workload cluster from a cluster template ( this is different from arlon cluster deploy for creating current-generation clusters ). The command creates between 2 and 3 (depending on whether a profile is used) ArgoCD application resources that together make up the cluster and its contents. The general usage is: arlon cluster create --cluster-name <clusterName> --repo-url <repoUrl> --repo-path <pathToDirectory> [ --output-yaml ] [ --profile <profileName> ] [ --repo-revision <repoRevision> ] The command supports two modes of operation: With --output-yaml : output a list of YAML resources that you can inspect, save to a file, or pipe to kubectl apply -f Without --output-yaml : create the application resources directly in the management cluster currently referenced by your KUBECONFIG and context. The --profile flag is optional; a cluster can be created with no profile.","title":"Creation"},{"location":"clustertemplate/#composition","text":"A workload cluster is composed of 2 to 3 ArgoCD application resources, which are named based on the name of the cluster template and the workload cluster. For illustration purposes, the following discussion assumes that the cluster template is named capi-quickstart , the workload cluster is named cluster-a , and the optional profile is named xxx .","title":"Composition"},{"location":"clustertemplate/#cluster-app","text":"The cluster-a application is the cluster app for the cluster. It is responsible for deploying the cluster template resources, meaning the Cluster API manifests. It is named directly from the workload cluster name. The application's spec uses a ApplicationSourceKustomize that points to the cluster template's git directory. The spec ensures that all deployed resources are configured to: Reside in the cluster-a namespace, which is deployed by the arlon app (see below). This achieved by setting app.Spec.Destination.Namespace to the workload cluster's name ( this only works if the resources do not specify an explicit namespace; this requirement is taken care of by the \"prep\" step on the cluster template*). Be named cluster-a-capi-quickstart , meaning the workload cluster name followed by the cluster template name. This is achieved by setting app.Spec.Source.Kustomize.NamePrefix to the workload cluster name plus a hyphen.","title":"Cluster app"},{"location":"clustertemplate/#arlon-app","text":"The cluster-a-arlon application is the arlon app for the cluster. It is resposible for deploying: The cluster-a namespace, which holds most resources related to this workload cluster, such as the Cluster API manifests deployed by the cluster app. Resources required to register the workload cluster with argocd when available: ClusterRegistration and associated RBAC rules. Additional resources (service account, more RBAC rules) for Cluster Autoscaler if enabled. The application spec's ApplicationSource points to the existing Arlon Helm chart located here by default: Repo: https://github.com/arlonproj/arlon.git Revision: private/leb/gen2 (IMPORTANT: NEEDS TO CHANGE TO STABLE BRANCH OR TAG) Path: pkg/cluster/manifests This is the same Helm chart that current-generation clusters are deployed from, using arlon cluster deploy . When used for the arlon app for a next-gen cluster, the Helm parameters are configured to only deploy the Arlon resources, with the subchart for cluster resources disabled, since those resources will be deployed by the cluster app. Important issue : as described above, the application source resides in the public Arlon repo. To avoid breaking user's deployed clusters, the source must be stable and not change! This probably means a particular Arlon release should point the source to a stable tag (not even a branch?) As an alternative, during Arlon setup, allow the user to copy the Helm chart into a private repo, and point the source there.","title":"Arlon app"},{"location":"clustertemplate/#profile-app-optional","text":"A next-gen cluster can be assigned a current-gen dynamic profile, in which case Arlon creates a profile app named <clusterName>-profile-<profileName> , or cluster-a-profile-xxx in the running example. This is similar to the profile app created when attaching a profile app to an external cluster. The application source points to the git location of the dynamic profile.","title":"Profile app (optional)"},{"location":"clustertemplate/#teardown","text":"Since a next-gen cluster is composed of multiple ArgoCD applications, destroying the cluster requires deleting all of its applications. To facilitate this, the 2 or 3 applications created by arlon cluster create are automatically labeled with arlon-cluster=<clusterName> . The user has two options for destroying a next-gen cluster: The easiest way: arlon cluster delete <clusterName> . This command automatically detects a next-gen cluster and cleans up all related applications. A more manual way: kubectl delete application -l arlon-cluster=<clusterName>","title":"Teardown"},{"location":"clustertemplate/#update-semantics","text":"A cluster template lives in git and is shared by all workload clusters created from it. This is sometimes referred to as Linked Mode . Any git update to the cluster can affect the associated workload clusters, therefore such updates must be planned and managed with care; there is a real risk of such an update breaking existing clusters. By default, a workload's cluster cluster app is configured with auto-sync, meaning ArgoCD will immediately apply any changes in the cluster template to the deployed Cluster API cluster resources. In general, a cluster template does not need to be \"prepped\" again after a modification to its main manifest file (the one containing the Cluster API resources). So the user is free to edit the manifest directly, commit/push the changes, and expect to see immediate changes to already-deployed clusters created from that cluster template.","title":"Update Semantics"},{"location":"clustertemplate/#unsupported-changes","text":"The controllers for Cluster API and its providers disallow changes to some fields belonging to already-deployed resources. For example, changing the cluster template name ( medata.Name of the Cluster resource) will have disastrous consequences on already-deployed clusters, causing many resources to enter the OutOfSync state and never recover because ArgoCD fails to apply the changes (they are rejected by the controllers). Consequently, a user should never change the name of a cluster template. Besides the cluster name, other fields cannot change (this has been observed anecdotally, we don't yet have an exhaustive list). Changing the Kubernetes version of the control plane or data plane is supported, so long as the new version is supported by the relevant providers. If accepted, such a change will result in a rolling update of the corresponding plane. Specific to AWS: the AWSMachineTemplate.spec is immutable and a CAPI webhook disallows such updates. The user is advised to not make such modifications to a cluster template manifest. In the event that such an event does happen, the user is advised to not manually sync in those changes via argocd . If a new cluster with a different AWSMachineTemplate.spec is desired, the recommended approach is to make a copy of the manifests in the workspace repository and then issue an arlon cluster create command which would then consume this manifest.","title":"Unsupported changes"},{"location":"concepts/","text":"Concepts \u00b6 Management cluster \u00b6 This Kubernetes cluster hosts the following components: ArgoCD Arlon Cluster management stacks e.g. Cluster API and/or Crossplane The Arlon state and controllers reside in the arlon namespace. Configuration bundle \u00b6 A configuration bundle (or just \"bundle\") is grouping of data files that produce a set of Kubernetes manifests via a tool . This closely follows ArgoCD's definition of tool types . Consequently, the list of supported bundle types mirrors ArgoCD's supported set of manifest-producing tools. Each bundle is defined using a Kubernetes ConfigMap resource in the arlon namespace. Static bundle \u00b6 A static bundle embeds the manifest's YAML data itself (\"static bundle\"). A cluster consuming a static bundle will always have a snapshot copy of the bundle at the time the cluster was created, and is not affected by subsequent changes to the bundle's manifest data. Dynamic bundle \u00b6 A dynamic bundle contains a reference to the manifest data stored in git. A dynamic bundle is distinguished by having these fields set to non-empty values: git URL of the repo Directory path within the repo The git URL must be registered in ArgoCD as a valid repository. The content of the specified directory can contain manifests in any of the tool formats supported by ArgoCD, including plain YAML, Helm and Kustomize. When the user updates a dynamic bundle in git, all clusters consuming that bundle (through a profile specified at cluster creation time) will acquire the change. Other properties \u00b6 A bundle can also have a comma-separated list of tags, and a description. Tags can be useful for classifying bundles, for e.g. by type (\"addon\", \"cni\", \"rbac\", \"app\"). Profile \u00b6 A profile expresses a desired configuration for a Kubernetes cluster. It is just a set of references to bundles (static, dynamic, or a combination). A profile can be static or dynamic. Static profile \u00b6 When a cluster consumes a static profile at creation time, the set of bundles for the cluster is fixed at that time and does not change over time even when the static bundle is updated. (Note: the contents of some of those bundles referenced by the static profile may however change over time if they are dynamic). A static profile is stored as an item in the Arlon database (specifically, as a CR in the Management Cluster). Dynamic profile \u00b6 A dynamic profile, on the other hand, has two components: the specification stored in the Arlon database, and a compiled component living in the workspace repository at a path specified by the user. (Note: this repository is usually the workspace repo, but it technically doesn't have to be, as long as it's a valid repo registered in ArgoCD) The compiled component is essentially a Helm chart of multiple ArgoCD app resources, each one pointing to a bundle. Arlon automatically creates and maintains the compiled component. When a user updates the composition of a dynamic profile, meaning redefines its bundle set, the Arlon library updates the compiled component to point to the bundles specified in the new set. Any cluster consuming that dynamic profile will be affected by the change, meaning it may lose or acquire new bundles in real time. Cluster \u00b6 An Arlon cluster, also known as workload cluster, is a Kubernetes cluster that Arlon creates and manages via a git directory structure stored in the workspace repository. (Under construction) Cluster spec \u00b6 A cluster spec contains desired settings when creating a new cluster. They currently include: API Provider: the cluster orchestration technology. Supported values are CAPI (Cluster API) and xplane (Crossplane) Cloud Provider: the infrastructure cloud provider. The currently supported values is aws , with gcp and azure support coming later. Type: the cluster type. Some API providers support more than one type. On aws cloud, Cluster API supports kubeadm and eks , whereas Crossplane only supports eks . The (worker) node instance type The initial (worker) node count The Kubernetes version Cluster Template \u00b6 To know more about cluster template (Arlon gen2 clusters), read it here","title":"Concepts"},{"location":"concepts/#concepts","text":"","title":"Concepts"},{"location":"concepts/#management-cluster","text":"This Kubernetes cluster hosts the following components: ArgoCD Arlon Cluster management stacks e.g. Cluster API and/or Crossplane The Arlon state and controllers reside in the arlon namespace.","title":"Management cluster"},{"location":"concepts/#configuration-bundle","text":"A configuration bundle (or just \"bundle\") is grouping of data files that produce a set of Kubernetes manifests via a tool . This closely follows ArgoCD's definition of tool types . Consequently, the list of supported bundle types mirrors ArgoCD's supported set of manifest-producing tools. Each bundle is defined using a Kubernetes ConfigMap resource in the arlon namespace.","title":"Configuration bundle"},{"location":"concepts/#static-bundle","text":"A static bundle embeds the manifest's YAML data itself (\"static bundle\"). A cluster consuming a static bundle will always have a snapshot copy of the bundle at the time the cluster was created, and is not affected by subsequent changes to the bundle's manifest data.","title":"Static bundle"},{"location":"concepts/#dynamic-bundle","text":"A dynamic bundle contains a reference to the manifest data stored in git. A dynamic bundle is distinguished by having these fields set to non-empty values: git URL of the repo Directory path within the repo The git URL must be registered in ArgoCD as a valid repository. The content of the specified directory can contain manifests in any of the tool formats supported by ArgoCD, including plain YAML, Helm and Kustomize. When the user updates a dynamic bundle in git, all clusters consuming that bundle (through a profile specified at cluster creation time) will acquire the change.","title":"Dynamic bundle"},{"location":"concepts/#other-properties","text":"A bundle can also have a comma-separated list of tags, and a description. Tags can be useful for classifying bundles, for e.g. by type (\"addon\", \"cni\", \"rbac\", \"app\").","title":"Other properties"},{"location":"concepts/#profile","text":"A profile expresses a desired configuration for a Kubernetes cluster. It is just a set of references to bundles (static, dynamic, or a combination). A profile can be static or dynamic.","title":"Profile"},{"location":"concepts/#static-profile","text":"When a cluster consumes a static profile at creation time, the set of bundles for the cluster is fixed at that time and does not change over time even when the static bundle is updated. (Note: the contents of some of those bundles referenced by the static profile may however change over time if they are dynamic). A static profile is stored as an item in the Arlon database (specifically, as a CR in the Management Cluster).","title":"Static profile"},{"location":"concepts/#dynamic-profile","text":"A dynamic profile, on the other hand, has two components: the specification stored in the Arlon database, and a compiled component living in the workspace repository at a path specified by the user. (Note: this repository is usually the workspace repo, but it technically doesn't have to be, as long as it's a valid repo registered in ArgoCD) The compiled component is essentially a Helm chart of multiple ArgoCD app resources, each one pointing to a bundle. Arlon automatically creates and maintains the compiled component. When a user updates the composition of a dynamic profile, meaning redefines its bundle set, the Arlon library updates the compiled component to point to the bundles specified in the new set. Any cluster consuming that dynamic profile will be affected by the change, meaning it may lose or acquire new bundles in real time.","title":"Dynamic profile"},{"location":"concepts/#cluster","text":"An Arlon cluster, also known as workload cluster, is a Kubernetes cluster that Arlon creates and manages via a git directory structure stored in the workspace repository. (Under construction)","title":"Cluster"},{"location":"concepts/#cluster-spec","text":"A cluster spec contains desired settings when creating a new cluster. They currently include: API Provider: the cluster orchestration technology. Supported values are CAPI (Cluster API) and xplane (Crossplane) Cloud Provider: the infrastructure cloud provider. The currently supported values is aws , with gcp and azure support coming later. Type: the cluster type. Some API providers support more than one type. On aws cloud, Cluster API supports kubeadm and eks , whereas Crossplane only supports eks . The (worker) node instance type The initial (worker) node count The Kubernetes version","title":"Cluster spec"},{"location":"concepts/#cluster-template","text":"To know more about cluster template (Arlon gen2 clusters), read it here","title":"Cluster Template"},{"location":"contributing/","text":"How to contribute to Arlon \u00b6 Team Arlon welcomes and encourages everyone to participate in its development via pull requests on GitHub. We prefer to take in pull requests to our active development branch i.e. the main branch. To report a bug or request a feature, we rely on GitHub issues. There are a number of points to keep in mind when submitting a feature request, reporting a bug or contributing in the development of Arlon. Before making a feature request, or reporting a bug please browse through the existing open issues to be sure that it hasn't been already tracked. If a feature request is subsumed by some other open issue, please add your valuable feedback as a comment to the issue. If a bug discovered by you is already being tracked, please provide additional information as you see fit(steps to reproduce, particulars of the environment, version information etc.) as a comment. Before submitting code for a new feature(or a complex, untracked bugfix) please create a new issue. This issue needs to undergo a review process which may involve a discussion on the same GitHub issue to discuss possible approaches and motivation for the said proposal. Please reach out to us on Slack for discussions, help, questions and the roadmap. Code changes \u00b6 Open a pull request (PR) on GitHub following the typical GitHub workflow here . Most of the new code changes are merged to the main branch except backports, bookkeeping changes, library upgrades and some bugs that manifest only a particular version. Before contributing new code, contributors are encouraged to either write unit tests, e2e tests or perform some form of manual validation as a sanity-check. Please adhere to standard good practices for Golang and do ensure that the code is properly formatted and vet succeeds, for which we have fmt and vet targets respectively. Since Arlon is a growing project, various areas require improvements- improving code coverage with unit tests, e2e tests , documentation, CI/CD pipelines using GitHub Actions are a few to name, we highly encourage to contribute to those areas to start with. The e2e test documentation is an excellent starting point to grasp the workings of our e2e test setup. Issues / Bug reports \u00b6 We track issues on GitHub . You are encouraged to browse through these, add relevant feedback, create new issues or participate in the development. If you are interested in a particular issue or feature request, please leave a comment to reach out to the team. In particular, the issues labeled as help wanted are a great starting point for adding code changes to the project. Documentation \u00b6 The documentation for Arlon is hosted on Read the Docs and comprises of contents from the \"docs\" directory of Arlon source. For making changes to the documentation, please follow the below steps: Fork the Arlon repository on GitHub and make the desired changes. Prerequisites Ensure that python3 , pip3 is installed. Optionally, create a venv by running python3 -m venv ./venv to create a virtual environment if you don't have one. From the root of the Arlon repository, run pip3 install -r docs/requirements.txt to install mkdocs and other pre-requisites. To test your local changes, run mkdocs serve from the repository root. This starts a local server to host the documentation website where you can preview the changes. To publish the changes, just push the changes to your fork repository and open a PR (pull request). Once your PR is accepted by one of the maintainers/ owners of Arlon project, the Arlon website will be updated.","title":"Contributing"},{"location":"contributing/#how-to-contribute-to-arlon","text":"Team Arlon welcomes and encourages everyone to participate in its development via pull requests on GitHub. We prefer to take in pull requests to our active development branch i.e. the main branch. To report a bug or request a feature, we rely on GitHub issues. There are a number of points to keep in mind when submitting a feature request, reporting a bug or contributing in the development of Arlon. Before making a feature request, or reporting a bug please browse through the existing open issues to be sure that it hasn't been already tracked. If a feature request is subsumed by some other open issue, please add your valuable feedback as a comment to the issue. If a bug discovered by you is already being tracked, please provide additional information as you see fit(steps to reproduce, particulars of the environment, version information etc.) as a comment. Before submitting code for a new feature(or a complex, untracked bugfix) please create a new issue. This issue needs to undergo a review process which may involve a discussion on the same GitHub issue to discuss possible approaches and motivation for the said proposal. Please reach out to us on Slack for discussions, help, questions and the roadmap.","title":"How to contribute to Arlon"},{"location":"contributing/#code-changes","text":"Open a pull request (PR) on GitHub following the typical GitHub workflow here . Most of the new code changes are merged to the main branch except backports, bookkeeping changes, library upgrades and some bugs that manifest only a particular version. Before contributing new code, contributors are encouraged to either write unit tests, e2e tests or perform some form of manual validation as a sanity-check. Please adhere to standard good practices for Golang and do ensure that the code is properly formatted and vet succeeds, for which we have fmt and vet targets respectively. Since Arlon is a growing project, various areas require improvements- improving code coverage with unit tests, e2e tests , documentation, CI/CD pipelines using GitHub Actions are a few to name, we highly encourage to contribute to those areas to start with. The e2e test documentation is an excellent starting point to grasp the workings of our e2e test setup.","title":"Code changes"},{"location":"contributing/#issues-bug-reports","text":"We track issues on GitHub . You are encouraged to browse through these, add relevant feedback, create new issues or participate in the development. If you are interested in a particular issue or feature request, please leave a comment to reach out to the team. In particular, the issues labeled as help wanted are a great starting point for adding code changes to the project.","title":"Issues / Bug reports"},{"location":"contributing/#documentation","text":"The documentation for Arlon is hosted on Read the Docs and comprises of contents from the \"docs\" directory of Arlon source. For making changes to the documentation, please follow the below steps: Fork the Arlon repository on GitHub and make the desired changes. Prerequisites Ensure that python3 , pip3 is installed. Optionally, create a venv by running python3 -m venv ./venv to create a virtual environment if you don't have one. From the root of the Arlon repository, run pip3 install -r docs/requirements.txt to install mkdocs and other pre-requisites. To test your local changes, run mkdocs serve from the repository root. This starts a local server to host the documentation website where you can preview the changes. To publish the changes, just push the changes to your fork repository and open a PR (pull request). Once your PR is accepted by one of the maintainers/ owners of Arlon project, the Arlon website will be updated.","title":"Documentation"},{"location":"design/","text":"Arlon Design and Concepts \u00b6 Management cluster \u00b6 This Kubernetes cluster hosts the following components: ArgoCD Arlon Cluster management stacks e.g. Cluster API and/or Crossplane The Arlon state and controllers reside in the arlon namespace. Configuration bundle \u00b6 A configuration bundle (or just \"bundle\") is grouping of data files that produce a set of Kubernetes manifests via a tool . This closely follows ArgoCD's definition of tool types . Consequently, the list of supported bundle types mirrors ArgoCD's supported set of manifest-producing tools. Each bundle is defined using a Kubernetes ConfigMap resource in the arlo namespace. Additionally, a bundle can embed the data itself (\"static bundle\"), or contain a reference to the data (\"dynamic bundle\"). A reference can be a URL, GitHub location, or Helm repo location. The current list of supported bundle types is: manifest_inline: a single manifest yaml file embedded in the resource manifest_ref: a reference to a single manifest yaml file dir_inline: an embedded tarball that expands to a directory of YAML files helm_inline: an embedded Helm chart package helm_ref: an external reference to a Helm chart Bundle purpose \u00b6 Bundles can specify an optional purpose to help classify and organize them. In the future, Arlon may order bundle installation by purpose order (for e.g. install bundles with purpose= networking before others) but that is not the case today. The currently suggested purpose values are: networking add-on data-service application Profile \u00b6 A profile expresses a desired configuration for a Kubernetes cluster. It is composed of An optional clusterspec. If specified, it allows the profile to be used to create new clusters. If absent, the profile can only be applied to existing clusters. A list of bundles specifying the configuration to apply onto the cluster once it is operational An optional list of values.yaml settings for any Helm Chart type bundle in the bundle list Cluster \u00b6 Cluster Specification/ Metadata \u00b6 A Cluster Specification contains desired settings when creating a new cluster. These settings are the values that define the shape and the configurations of the cluster. Currently, there is a difference in the cluster specification for gen1 and gen2 clusters. The main difference in these cluster specifications is that gen2 Cluster Specification allow users to deploy arbitrarily complex clusters using the full Cluster API feature set.This is also closer to the gitops and declarative style of cluster creation and gives users more control over the cluster that they deploy. gen1 \u00b6 A clusterspec contains desired settings when creating a new cluster. For gen1 clusters, this Cluster Specification is called ClusterSpec . Clusterspec currently includes: Stack: the cluster provisioning stack, for e.g. cluster-api or crossplane Provider: the specific cluster management provider under that stack, if applicable. Example: for cluster-api , the possible values are eks and kubeadm Other settings that specify the \"shape\" of the cluster, such as the size of the control plane and the initial number of nodes of the data plane. The pod networking technology (under discussion: this may be moved to a bundle because most if not all CNI providers can be installed as manifests) gen2 \u00b6 for gen2 clusters, the Cluster Specification is called the cluster template, which is described in detail here . A cluster template manifest consists of: A predefined list of Cluster API objects: Cluster, Machines, Machine Deployments, etc. to be deployed in the current namespace The specific infrastructure provider to be used (e.g aws).\u00df Kubernetes version Cluster templates/ flavors that need to be used for creating the cluster manifest (e.g eks, eks-managedmachinepool) Cluster Preparation \u00b6 Once these cluster specifications are created successfully, the next step is to prepare the cluster for deployment. gen1 \u00b6 Once the clusterspec is created for a gen-1 cluster, there is no need to prepare a workspace repository to create a new cluster. gen2 \u00b6 Once the cluster template manifest is created, the next step is to preare the workspace repository directory in which this cluster template manifest is present. This is explained in detail here Cluster Creation \u00b6 Now, all the prerequisites for creating a cluster are completed and the cluster can be created/deployed. Cluster Chart \u00b6 The cluster chart is a Helm chart that creates (and optionally applies) the manifests necessary to create a cluster and deploy desired configurations and applications to it as a part of cluster creation, the following resources are created: The profile's Cluster Specification, bundle list and other settings are used to generate values for the cluster chart, and the chart is deployed as a Helm release into the arlon namespace in the management cluster. Here is a summary of the kinds of resources generated and deployed by the chart: A unique namespace with a name based on the cluster's name. All subsequent resources below are created inside that namespace. The stack-specific resources to create the cluster (for e.g. Cluster API resources) A ClusterRegistration to automatically register the cluster with ArgoCD A GitRepoDir to automatically create a git repo and/or directory to host a copy of the expanded bundles. Every bundle referenced by the profile is copied/unpacked into its own subdirectory. One ArgoCD Application resource for each bundle. gen1 \u00b6 Cluster deployment is explained here gen2 \u00b6 cluster template creation is explained here","title":"Design"},{"location":"design/#arlon-design-and-concepts","text":"","title":"Arlon Design and Concepts"},{"location":"design/#management-cluster","text":"This Kubernetes cluster hosts the following components: ArgoCD Arlon Cluster management stacks e.g. Cluster API and/or Crossplane The Arlon state and controllers reside in the arlon namespace.","title":"Management cluster"},{"location":"design/#configuration-bundle","text":"A configuration bundle (or just \"bundle\") is grouping of data files that produce a set of Kubernetes manifests via a tool . This closely follows ArgoCD's definition of tool types . Consequently, the list of supported bundle types mirrors ArgoCD's supported set of manifest-producing tools. Each bundle is defined using a Kubernetes ConfigMap resource in the arlo namespace. Additionally, a bundle can embed the data itself (\"static bundle\"), or contain a reference to the data (\"dynamic bundle\"). A reference can be a URL, GitHub location, or Helm repo location. The current list of supported bundle types is: manifest_inline: a single manifest yaml file embedded in the resource manifest_ref: a reference to a single manifest yaml file dir_inline: an embedded tarball that expands to a directory of YAML files helm_inline: an embedded Helm chart package helm_ref: an external reference to a Helm chart","title":"Configuration bundle"},{"location":"design/#bundle-purpose","text":"Bundles can specify an optional purpose to help classify and organize them. In the future, Arlon may order bundle installation by purpose order (for e.g. install bundles with purpose= networking before others) but that is not the case today. The currently suggested purpose values are: networking add-on data-service application","title":"Bundle purpose"},{"location":"design/#profile","text":"A profile expresses a desired configuration for a Kubernetes cluster. It is composed of An optional clusterspec. If specified, it allows the profile to be used to create new clusters. If absent, the profile can only be applied to existing clusters. A list of bundles specifying the configuration to apply onto the cluster once it is operational An optional list of values.yaml settings for any Helm Chart type bundle in the bundle list","title":"Profile"},{"location":"design/#cluster","text":"","title":"Cluster"},{"location":"design/#cluster-specification-metadata","text":"A Cluster Specification contains desired settings when creating a new cluster. These settings are the values that define the shape and the configurations of the cluster. Currently, there is a difference in the cluster specification for gen1 and gen2 clusters. The main difference in these cluster specifications is that gen2 Cluster Specification allow users to deploy arbitrarily complex clusters using the full Cluster API feature set.This is also closer to the gitops and declarative style of cluster creation and gives users more control over the cluster that they deploy.","title":"Cluster Specification/ Metadata"},{"location":"design/#gen1","text":"A clusterspec contains desired settings when creating a new cluster. For gen1 clusters, this Cluster Specification is called ClusterSpec . Clusterspec currently includes: Stack: the cluster provisioning stack, for e.g. cluster-api or crossplane Provider: the specific cluster management provider under that stack, if applicable. Example: for cluster-api , the possible values are eks and kubeadm Other settings that specify the \"shape\" of the cluster, such as the size of the control plane and the initial number of nodes of the data plane. The pod networking technology (under discussion: this may be moved to a bundle because most if not all CNI providers can be installed as manifests)","title":"gen1"},{"location":"design/#gen2","text":"for gen2 clusters, the Cluster Specification is called the cluster template, which is described in detail here . A cluster template manifest consists of: A predefined list of Cluster API objects: Cluster, Machines, Machine Deployments, etc. to be deployed in the current namespace The specific infrastructure provider to be used (e.g aws).\u00df Kubernetes version Cluster templates/ flavors that need to be used for creating the cluster manifest (e.g eks, eks-managedmachinepool)","title":"gen2"},{"location":"design/#cluster-preparation","text":"Once these cluster specifications are created successfully, the next step is to prepare the cluster for deployment.","title":"Cluster Preparation"},{"location":"design/#gen1_1","text":"Once the clusterspec is created for a gen-1 cluster, there is no need to prepare a workspace repository to create a new cluster.","title":"gen1"},{"location":"design/#gen2_1","text":"Once the cluster template manifest is created, the next step is to preare the workspace repository directory in which this cluster template manifest is present. This is explained in detail here","title":"gen2"},{"location":"design/#cluster-creation","text":"Now, all the prerequisites for creating a cluster are completed and the cluster can be created/deployed.","title":"Cluster Creation"},{"location":"design/#cluster-chart","text":"The cluster chart is a Helm chart that creates (and optionally applies) the manifests necessary to create a cluster and deploy desired configurations and applications to it as a part of cluster creation, the following resources are created: The profile's Cluster Specification, bundle list and other settings are used to generate values for the cluster chart, and the chart is deployed as a Helm release into the arlon namespace in the management cluster. Here is a summary of the kinds of resources generated and deployed by the chart: A unique namespace with a name based on the cluster's name. All subsequent resources below are created inside that namespace. The stack-specific resources to create the cluster (for e.g. Cluster API resources) A ClusterRegistration to automatically register the cluster with ArgoCD A GitRepoDir to automatically create a git repo and/or directory to host a copy of the expanded bundles. Every bundle referenced by the profile is copied/unpacked into its own subdirectory. One ArgoCD Application resource for each bundle.","title":"Cluster Chart"},{"location":"design/#gen1_2","text":"Cluster deployment is explained here","title":"gen1"},{"location":"design/#gen2_2","text":"cluster template creation is explained here","title":"gen2"},{"location":"dev_setup/","text":"TODO (Under construction) \u00b6","title":"Setup"},{"location":"dev_setup/#todo-under-construction","text":"","title":"TODO (Under construction)"},{"location":"docs_help/","text":"Help with Arlon Documentation \u00b6 Prerequisites \u00b6 Documentation for Arlon is written in Markdown format, and generated using the mkdocs site generator. The theme mkdocs-material has been used, and docs from the main branch are published to a GitHub pages site. Install Python 3.x (3.8 or later recommended. 3.11 has been tested) Install the pip python package manager for Python 3. Install the Python modules mentioned in docs/requirements.txt Run mkdocs serve to test any local changes to docs, and commit them using git workflow Help with MkDocs \u00b6 Welcome to Arlon documentation with MkDocs For full documentation, visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout \u00b6 mkdocs.yml # The configuration file. # Contains the navigation structure docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Help with MkDocs"},{"location":"docs_help/#help-with-arlon-documentation","text":"","title":"Help with Arlon Documentation"},{"location":"docs_help/#prerequisites","text":"Documentation for Arlon is written in Markdown format, and generated using the mkdocs site generator. The theme mkdocs-material has been used, and docs from the main branch are published to a GitHub pages site. Install Python 3.x (3.8 or later recommended. 3.11 has been tested) Install the pip python package manager for Python 3. Install the Python modules mentioned in docs/requirements.txt Run mkdocs serve to test any local changes to docs, and commit them using git workflow","title":"Prerequisites"},{"location":"docs_help/#help-with-mkdocs","text":"Welcome to Arlon documentation with MkDocs For full documentation, visit mkdocs.org .","title":"Help with MkDocs"},{"location":"docs_help/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"docs_help/#project-layout","text":"mkdocs.yml # The configuration file. # Contains the navigation structure docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"e2e_testing/","text":"Arlon e2e tests \u00b6 Setup \u00b6 First, we run the testing/e2e_setup.sh script, which helps us setup a kind management cluster, a git-server gitea based workspace repository and installs services like argocd, arlon, capi in the management cluster. It also installs other required tools like kubectl, docker, kind, kuttl, clusterctl, helm and gitea. In addition to this, the script also creates a capi-eks cluster manifest which serves as the cluster template manifest. This cluster is created as a part of the e2e tests and is pushed to the workspace repository created in the previous step. This script also adds a xenial bundle manifest to the workspace repository which is required for creating a xenial bundle and a corresponding profile which is consumed by the test. Any prerequisites for any e2e test must be added to this script or to a seperate script as a part of the setup for the e2e test. These scripts must be executed as a part of the e2e test setup before any additional arlon commands can be executed. e2e integration tests using KUTTL \u00b6 We are using KUTTL to write e2e integration tests for arlon. KUTTL is a declarative integration testing harness for testing operators, KUDO, Helm charts, and any other Kubernetes applications or controllers. The KUTTL test CLI organizes tests into suites: A \"test step\" defines a set of Kubernetes manifests to apply and a state to assert on (wait for or expect). A \"test assert\" are the part of a test step that define the state to wait for Kubernetes to reach All the e2e tests are placed in /testing/e2e in the respecitive directory for the given test. For example, the test 00-deploy is used to deploy a cluster and all the files related to this test are placed in /testing/e2e/00-deploy . To add a new e2e test here, create a new directory with the correct step index. The tests in /testing/e2e run in the order of the index specified. Each filename in the test case directory should start with an index (in this example 00) that indicates which test step the file is a part of. The first filename in the test case will begin with index 00, the second will have index 01 and so on. e.g In /testing/e2e/00-deploy we have 00-prepare.yaml , 01-validate.yaml etc. Do note that the tests will run in the order of the index specified. This is the case for both the test case directory and the individual test files within these directories. Files that do not start with a step index are ignored and can be used for documentation or other test data. As a part of the test step, we can run commands and execute scripts. Here, for arlon e2e tests, we execute the e2e_setup script as a part of the test step and then run the arlon commands specific to the test case. Once we have created a test step case, we can also create a test assert for a given filename. The assert's filename should be the test step index followed by -assert.yaml e.g. In /testing/e2e/00-deploy we have 02-assert.yaml , which will run after 02-deploy.yaml test step. In a test assert step, we look for the desired resource state. This is the state of the resource after the test step has finished. As a part of the test assert, we can add a timeout incase the resource that we are waiting on takes a long time to reach the desired state. All the test steps and test asserts run in order and each must be successful for the test case to be considered successful. If any test step or test assertion fails then the test will fail. Testbed Teardown \u00b6 Currently, as a part of the /testing/e2e_setup_teardown.sh script, we delete the kind management cluster, the cloned workspace repository, bundles, profiles and cluster manifests present in this repository. This script runs at the end of every e2e test run regardless of the success or failure of the test for cleaning up any resources that might have been created as a part of the test. Any additional resource that needs to be cleaned up post the e2e test should be added to this script. e2e tests integration with Github Actions \u00b6 Currently, we are running the arlon e2e tests on an ubuntu VM using Github Actions. To invoke the arlon e2e tests, we are adding a new build target make test-e2e This command will execute the e2e test job using Github Actions.","title":"End to End Tests"},{"location":"e2e_testing/#arlon-e2e-tests","text":"","title":"Arlon e2e tests"},{"location":"e2e_testing/#setup","text":"First, we run the testing/e2e_setup.sh script, which helps us setup a kind management cluster, a git-server gitea based workspace repository and installs services like argocd, arlon, capi in the management cluster. It also installs other required tools like kubectl, docker, kind, kuttl, clusterctl, helm and gitea. In addition to this, the script also creates a capi-eks cluster manifest which serves as the cluster template manifest. This cluster is created as a part of the e2e tests and is pushed to the workspace repository created in the previous step. This script also adds a xenial bundle manifest to the workspace repository which is required for creating a xenial bundle and a corresponding profile which is consumed by the test. Any prerequisites for any e2e test must be added to this script or to a seperate script as a part of the setup for the e2e test. These scripts must be executed as a part of the e2e test setup before any additional arlon commands can be executed.","title":"Setup"},{"location":"e2e_testing/#e2e-integration-tests-using-kuttl","text":"We are using KUTTL to write e2e integration tests for arlon. KUTTL is a declarative integration testing harness for testing operators, KUDO, Helm charts, and any other Kubernetes applications or controllers. The KUTTL test CLI organizes tests into suites: A \"test step\" defines a set of Kubernetes manifests to apply and a state to assert on (wait for or expect). A \"test assert\" are the part of a test step that define the state to wait for Kubernetes to reach All the e2e tests are placed in /testing/e2e in the respecitive directory for the given test. For example, the test 00-deploy is used to deploy a cluster and all the files related to this test are placed in /testing/e2e/00-deploy . To add a new e2e test here, create a new directory with the correct step index. The tests in /testing/e2e run in the order of the index specified. Each filename in the test case directory should start with an index (in this example 00) that indicates which test step the file is a part of. The first filename in the test case will begin with index 00, the second will have index 01 and so on. e.g In /testing/e2e/00-deploy we have 00-prepare.yaml , 01-validate.yaml etc. Do note that the tests will run in the order of the index specified. This is the case for both the test case directory and the individual test files within these directories. Files that do not start with a step index are ignored and can be used for documentation or other test data. As a part of the test step, we can run commands and execute scripts. Here, for arlon e2e tests, we execute the e2e_setup script as a part of the test step and then run the arlon commands specific to the test case. Once we have created a test step case, we can also create a test assert for a given filename. The assert's filename should be the test step index followed by -assert.yaml e.g. In /testing/e2e/00-deploy we have 02-assert.yaml , which will run after 02-deploy.yaml test step. In a test assert step, we look for the desired resource state. This is the state of the resource after the test step has finished. As a part of the test assert, we can add a timeout incase the resource that we are waiting on takes a long time to reach the desired state. All the test steps and test asserts run in order and each must be successful for the test case to be considered successful. If any test step or test assertion fails then the test will fail.","title":"e2e integration tests using KUTTL"},{"location":"e2e_testing/#testbed-teardown","text":"Currently, as a part of the /testing/e2e_setup_teardown.sh script, we delete the kind management cluster, the cloned workspace repository, bundles, profiles and cluster manifests present in this repository. This script runs at the end of every e2e test run regardless of the success or failure of the test for cleaning up any resources that might have been created as a part of the test. Any additional resource that needs to be cleaned up post the e2e test should be added to this script.","title":"Testbed Teardown"},{"location":"e2e_testing/#e2e-tests-integration-with-github-actions","text":"Currently, we are running the arlon e2e tests on an ubuntu VM using Github Actions. To invoke the arlon e2e tests, we are adding a new build target make test-e2e This command will execute the e2e test job using Github Actions.","title":"e2e tests integration with Github Actions"},{"location":"gen2_Tutorial/","text":"Next-Generation (gen2) Clusters - New in version 0.9.x \u00b6 Gen1 clusters are limited in capability by the Helm chart used to deploy the infrastructure resources. Advanced Cluster API configurations, such as those using multiple MachinePools, each with different instance types, is not supported. Gen2 clusters solve this problem by allowing you to create workload clusters from a cluster template that you design and provide in the form of a manifest file stored in a git directory. The manifest typically contains multiple related resources that together define an arbitrarily complex cluster. If you make subsequent changes to the cluster template, workload clusters originally created from it will automatically acquire the changes. In earlier versions of arlon (before v0.10), cluster templates were called base clusters . Some parts of the code still refer to base cluster manifests. This should be considered as a synonym for cluster template manifests. Note: Cluster Templates only support dynamic profiles. Creating Cluster-API cluster manifest \u00b6 Note: The CAPA version used here is v2.0 and the manifests created here are in accordance with this version. Refer the compatibility matrix for Cluster API provider and CAPA versions for supported versions. Before deploying a EKS cluster, make sure to setup the AWS Environment as stated in the quickstart giude for CAPI MachineDeployment \u00b6 Here is an example of a manifest file that we can use to create a cluster template . This manifest file helps in deploying an EKS cluster with 'machine deployment' component from the cluster API (CAPI). This file has been generated by the following command clusterctl generate cluster capi-quickstart --flavor eks \\ --kubernetes-version v1.24.0 \\ --control-plane-machine-count = 3 \\ --worker-machine-count = 3 \\ > capi-quickstart.yaml # YAML apiVersion : cluster.x-k8s.io/v1beta1 kind : Cluster metadata : name : capi-quickstart namespace : default spec : clusterNetwork : pods : cidrBlocks : - 192.168.0.0/16 controlPlaneRef : apiVersion : controlplane.cluster.x-k8s.io/v1beta2 kind : AWSManagedControlPlane name : capi-quickstart-control-plane infrastructureRef : apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSManagedCluster name : capi-quickstart --- apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSManagedCluster metadata : name : capi-quickstart spec : {} --- apiVersion : controlplane.cluster.x-k8s.io/v1beta2 kind : AWSManagedControlPlane metadata : name : capi-quickstart-control-plane namespace : default spec : region : { REGION } sshKeyName : { SSH_KEYNAME } version : v1.24.0 --- apiVersion : cluster.x-k8s.io/v1beta1 kind : MachineDeployment metadata : name : capi-quickstart-md-0 namespace : default spec : clusterName : capi-quickstart replicas : 3 selector : matchLabels : null template : spec : bootstrap : configRef : apiVersion : bootstrap.cluster.x-k8s.io/v1beta2 kind : EKSConfigTemplate name : capi-quickstart-md-0 clusterName : capi-quickstart infrastructureRef : apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSMachineTemplate name : capi-quickstart-md-0 version : v1.24.0 --- apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSMachineTemplate metadata : name : capi-quickstart-md-0 namespace : default spec : template : spec : iamInstanceProfile : nodes.cluster-api-provider-aws.sigs.k8s.io instanceType : { INSTANCE_TYPE } sshKeyName : { SSH_KEYNAME } --- apiVersion : bootstrap.cluster.x-k8s.io/v1beta2 kind : EKSConfigTemplate metadata : name : capi-quickstart-md-0 namespace : default spec : template : {} AWSManagedMachinePool \u00b6 Initialize the environment for AWSManagedMachinePool as stated here Before deploying an EKS cluster, make sure that the MachinePool feature gate is enabled. To do so, run this command: kubectl describe deployment capa-controller-manager -n capa-system In the output, in the feature gates section of the deployment, MachinePool must be set to true. > kubectl describe deployment capa-controller-manager -n capa-system .......... .......... --featuregates = EKS = true,EKSEnableIAM = false,EKSAllowAddRoles = false,EKSFargate = true,MachinePool = true,EventBridgeInstanceState = false, AutoControllerIdentityCreator = true,BootstrapFormatIgnition = false,ExternalResourceGC = false .......... .......... This manifest file helps in deploying an EKS cluster with 'AWSManagedMachinePool' component from the cluster API (CAPI). This file has been generated by the following command clusterctl generate cluster awsmanaged-cluster --kubernetes-version v1.22.0 --flavor eks-managedmachinepool > manifest.yaml # YAML apiVersion : cluster.x-k8s.io/v1beta1 kind : Cluster metadata : name : awsmanaged-cluster namespace : default spec : clusterNetwork : pods : cidrBlocks : - 192.168.0.0/16 controlPlaneRef : apiVersion : controlplane.cluster.x-k8s.io/v1beta2 kind : AWSManagedControlPlane name : awsmanaged-cluster-control-plane infrastructureRef : apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSManagedCluster name : awsmanaged-cluster --- apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSManagedCluster metadata : name : awsmanaged-cluster namespace : default spec : {} --- apiVersion : controlplane.cluster.x-k8s.io/v1beta2 kind : AWSManagedControlPlane metadata : name : awsmanaged-cluster-control-plane namespace : default spec : region : { REGION } sshKeyName : { SSH_KEYNAME } version : v1.22.0 --- apiVersion : cluster.x-k8s.io/v1beta1 kind : MachinePool metadata : name : awsmanaged-cluster-pool-0 namespace : default spec : clusterName : awsmanaged-cluster replicas : 1 template : spec : bootstrap : dataSecretName : \"\" clusterName : awsmanaged-cluster infrastructureRef : apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSManagedMachinePool name : awsmanaged-cluster-pool-0 --- apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSManagedMachinePool metadata : name : awsmanaged-cluster-pool-0 namespace : default spec : {} AWSMachinePool \u00b6 An AWSMachinePool corresponds to an AWS AutoScaling Groups, which provides the cloud provider specific resource for orchestrating a group of EC2 machines. Initialize the environment for AWSMachinePool as stated here Before deploying an EKS cluster, make sure that the AWSMachinePool feature gate is enabled. To do so, run this command: kubectl describe deployment capa-controller-manager -n capa-system In the output, in the feature gates section of the deployment, MachinePool must be set to true. > kubectl describe deployment capa-controller-manager -n capa-system .......... .......... --featuregates = EKS = true,EKSEnableIAM = false,EKSAllowAddRoles = false,EKSFargate = true,MachinePool = true,EventBridgeInstanceState = false, AutoControllerIdentityCreator = true,BootstrapFormatIgnition = false,ExternalResourceGC = false .......... .......... This manifest file helps in deploying an EKS cluster with 'AWSManagedMachinePool' component from the cluster API (CAPI). This file has been generated by the following command clusterctl generate cluster awsmanaged-cluster --kubernetes-version v1.22.0 --flavor eks-machinepool > manifest.yaml # YAML apiVersion : cluster.x-k8s.io/v1beta1 kind : Cluster metadata : name : awsmanaged-cluster namespace : default spec : clusterNetwork : pods : cidrBlocks : - 192.168.0.0/16 controlPlaneRef : apiVersion : controlplane.cluster.x-k8s.io/v1beta2 kind : AWSManagedControlPlane name : awsmanaged-cluster-control-plane infrastructureRef : apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSManagedCluster name : awsmanaged-cluster --- apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSManagedCluster metadata : name : awsmanaged-cluster namespace : default spec : {} --- apiVersion : controlplane.cluster.x-k8s.io/v1beta2 kind : AWSManagedControlPlane metadata : name : awsmanaged-cluster-control-plane namespace : default spec : region : { REGION } sshKeyName : { SSH_KEYNAME } version : v1.22.0 --- apiVersion : cluster.x-k8s.io/v1beta1 kind : MachinePool metadata : name : awsmanaged-cluster-mp-0 namespace : default spec : clusterName : awsmanaged-cluster replicas : 1 template : spec : bootstrap : configRef : apiVersion : bootstrap.cluster.x-k8s.io/v1beta2 kind : EKSConfig name : awsmanaged-cluster-mp-0 clusterName : awsmanaged-cluster infrastructureRef : apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSMachinePool name : awsmanaged-cluster-mp-0 version : v1.22.0 --- apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSMachinePool metadata : name : awsmanaged-cluster-mp-0 namespace : default spec : awsLaunchTemplate : iamInstanceProfile : nodes.cluster-api-provider-aws.sigs.k8s.io instanceType : { INSTANCE_TYPE } sshKeyName : { SSH_KEYNAME } maxSize : 10 minSize : 1 --- apiVersion : bootstrap.cluster.x-k8s.io/v1beta2 kind : EKSConfig metadata : name : awsmanaged-cluster-mp-0 namespace : default spec : {} gen2 cluster creation using Arlon \u00b6 This manifest file needs to be pushed to the workspace repository before the manifest directory is prepped and then validated. Before a manifest directory can be used as a cluster template, it must first be \"prepared\" or \"prepped\" by Arlon. The \"prep\" phase makes minor changes to the directory and manifest to help Arlon deploy multiple copies of the cluster without naming conflicts. manifest directory preparation \u00b6 To prepare a git directory to serve as cluster template, use the clustertemplate preparegit command: arlon clustertemplate preparegit --repo-url <repoUrl> --repo-path <pathToDirectory> [ --repo-revision revision ] # OR # using repository aliases # using the default alias arlon clustertemplate preparegit --repo-path <pathToDirectory> [ --repo-revision revision ] # using the prod alias arlon clustertemplate preparegit --repo-alias prod --repo-path <pathToDirectory> [ --repo-revision revision ] manifest directory validation \u00b6 Post the successful preparation of the cluster template manifest directory using clustertemplate preparegit , the cluster template manifest directory needs to be validated before the cluster template is created. To determine if a git directory is eligible to serve as cluster template, run the clustertemplate validategit command: arlon clustertemplate validategit --repo-url <repoUrl> --repo-path <pathToDirectory> [ --repo-revision revision ] # OR # using repository aliases # using the default alias arlon clustertemplate validategit --repo-path <pathToDirectory> [ --repo-revision revision ] # using the prod alias arlon clustertemplate validategit --repo-alias prod --repo-path <pathToDirectory> [ --repo-revision revision ] gen2 cluster creation \u00b6 Note: Cluster templates only support dynamic profiles. To create a gen2 workload cluster from the cluster template: arlon cluster create --cluster-name <clusterName> --repo-url <repoUrl> --repo-path <pathToDirectory> [ --output-yaml ] [ --profile <profileName> ] [ --repo-revision <repoRevision> ] # OR # using repository aliases # using the default alias arlon cluster create --cluster-name <clusterName> --repo-path <pathToDirectory> [ --output-yaml ] [ --profile <profileName> ] [ --repo-revision <repoRevision> ] # using the prod alias arlon cluster create --cluster-name <clusterName> --repo-alias prod --repo-path <pathToDirectory> [ --output-yaml ] [ --profile <profileName> ] [ --repo-revision <repoRevision> ] gen2 cluster creation with overrides \u00b6 We call the concept of constructing various clusters with patches from the same base manifest as cluster overrides. The cluster overrides feature is built on top of the existing base cluster design. So, A user can create a cluster from the base manifest using the same command as in the above step(gen2 cluster creation). Now, to create a cluster with overrides in the base manifest, a user should have the corresponding patch files in a single yaml file in local. Here is an example of a patch file where we want to override replicas count to 2 and change the sshkeyname: --- apiVersion: cluster.x-k8s.io/v1beta1 kind: MachineDeployment metadata: name: capi-quickstart-eks-md-0 spec: replicas: 2 --- apiVersion: controlplane.cluster.x-k8s.io/v1beta1 kind: AWSManagedControlPlane metadata: name: capi-quickstart-eks-control-plane spec: sshKeyName: random --- ``` Note: The metadata field in the patch file should be same as of the metadata field in the resources file. Refer to this [ document ]( https://blog.scottlowe.org/2019/11/12/using-kustomize-with-cluster-api-manifests/ ) to know more about patch files Command to create a gen2 workload cluster form the cluster template manifest with overrides to the manifest is: ``` shell arlon cluster create <cluster-name> --repo-url <repo url where cluster template is present> --repo-path <repo path to the cluster template> --overrides-path <path to the patch file> --patch-repo-url <repo url where patch file should be stored> --patch-repo-path <repo path to store the patch files> ```` Runnning the above command will create a cluster named folder in patch repo path of patch repo url which contains the patch files, kustomization.yaml and configurations.yaml which are used to create the cluster app. Note that the patch file repo url can be different or same from the cluster template repo url acoording to the requirement of the user. A user can use a different repo url for string patch files for the cluster. ## gen2 cluster update To update the profiles of a gen2 workload cluster: ``` shell # To add a new profile to the existing cluster arlon cluster ngupdate <clustername> --profile <profilename> # To delete an existing profile from the existing cluster arlon cluster ngupdate <clustername> --delete-profile A gen2 cluster can be created without any profile app associated with the cluster. So, the above commands can be used to add a new profile to the existing cluster which will create profile app in argocd along with bundle apps associated with the profile. An existing profile can be deleted from the cluster as well using the above command. Executing this command will delete the profile app and all the bundles associated with the profile in argocd. gen2 cluster deletion \u00b6 To destroy a gen2 workload cluster: arlon cluster delete <clusterName> Arlon creates between 2 and 3 ArgoCD application resources to compose a gen2 cluster (the 3rd application, called \"profile app\", is used when an optional profile is specified at cluster creation time). When you destroy a gen2 cluster, Arlon will find all related ArgoCD applications and clean them up. If the cluster which which is being deleted is a cluster created using patch files, the controller first cleans the git repo where the respective patch files of the cluster are present and then it destroys all the related ArgoCD applications and clean them up. Enabling Cluster Autoscaler in the workload cluster \u00b6 To create a gen2 cluster with autoscaler, we need: bundle pointing to the bundle/capi-cluster-autoscaler in the arlon repository. dynamic profile that contains the above bundle. a cluster template manifest(that makes use of MachineDeployment and not MachinePools) which has the CAPI annotations for min and max nodes set ( as a part of preparegit or manually add it ). run arlon cluster create with the repo-path pointing to the cluster template manifest described in the step above, set the profile to be the one created in step 2 and pass the autoscaler flag. Bundle creation \u00b6 Register a dynamic bundle pointing to the bundles/capi-cluster-autoscaler in the Arlon repo. To enable the cluster-autoscaler bundle, add one more parameter during cluster creation: srcType . This is the ArgoCD-defined application source type (Helm, Kustomize, Directory). In addition to this, the repo-revision parameter should also be set to a stable arlon release branch ( in this case v0.10 ). This example creates a bundle pointing to the bundles/capi-cluster-autoscaler in Arlon repo arlon bundle create cas-bundle --tags cas,devel,test --desc \"CAS Bundle\" --repo-url https://github.com/arlonproj/arlon.git --repo-path bundles/capi-cluster-autoscaler --srctype helm --repo-revision v0.10 Profile creation \u00b6 Create a profile that contains this capi-cluster-autoscaler bundle. arlon profile create dynamic-cas --repo-url <repoUrl> --repo-base-path profiles --bundles cas-bundle --desc \"dynamic cas profile\" --tags examples manifest directory preparation \u00b6 Two additional properties cas-max and cas-min are used to set 2 annotations for Max/Min nodes on MachineDeployment required by the cluster autoscaler for CAPI as a part of the manifest directory preparation. These are the annotations required by the MachineDeployment for autoscaling. Note: These are the default values for the cas-min and cas-max properties annotations: cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: '1' cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: '9' These annotations are added during the preparegit step. If these annotations are already present in the manifest file, then they will not be added again as a part of preparegit Preparing the git directory to serve as the cluster template: arlon clustertemplate preparegit --repo-path <pathToDirectory> --cas-min 1 --cas-max 9 --repo-url <repoUrl> manifest directory validation \u00b6 To determine if a git directory is eligible to serve as cluster template, run the clustertemplate validategit command: arlon clustertemplate validategit --repo-path <pathToDirectory> --repo-url <repoUrl> gen2 cluster creation with autoscaling enabled \u00b6 To add CAS support for gen2 clusters, the cluster create sub-command of the arlon CLI has a autoscaler flag which deploys the capi-cluster-autoscaler helm chart on the management cluster. To create a gen2 workload with a cluster-autoscaler pod running, from the cluster template, run this command: Note: Use the dynamic profile that was created in the previous steps arlon cluster create --cluster-name <clusterName> --repo-url <repoUrl> --repo-path <pathToDirectory> --profile dynamic-cas --autoscaler Known issues and limitations \u00b6 Gen2 clusters are powerful because the cluster template can be arbitrarily complex and feature rich. Since they are fairly new and still evolving, gen2 clusters have several known limitations relative to gen1. You cannot customize/override any property of the cluster template on the fly when creating a workload cluster, which is an exact clone of the cluster template except for the names of its resources and their namespace. The work-around is to make a copy of the cluster template directory, push the new directory, make the desired changes, commit & push the changes, and register the directory as a new cluster template. The clusters created directly from the cluster template are completely declarative whereas the clusters which are created using override property are not completely declarative. If a user passes a different repository for patch repo url from the repo where cluster template is present, argocd won't be able to detect if there are any changes in the cluster template repository but will deect all the chnages in patch repo url for the cluster. If you modify and commit a change to one or more properties of the cluster template that the underlying Cluster API provider deems as \"immutable\", new workload clusters created from the cluster template will have the modified propert(ies), but ArgoCD will flag existing clusters as OutOfSync, since the provider will continually reject attempts to apply the new property values. The existing clusters continue to function, despite appearing unhealthy in the ArgoCD UI and CLI outputs. Examples of mutable properties in Cluster API resources: Number of replicas (modification will result in a scale-up / down) Kubernetes version (modification will result in an upgrade) Examples of immutable properties: Most fields of AWSMachineTemplate (instance type, labels, etc...) For more information \u00b6 For more details on gen2 clusters, refer to the design document .","title":"Tutorial"},{"location":"gen2_Tutorial/#next-generation-gen2-clusters-new-in-version-09x","text":"Gen1 clusters are limited in capability by the Helm chart used to deploy the infrastructure resources. Advanced Cluster API configurations, such as those using multiple MachinePools, each with different instance types, is not supported. Gen2 clusters solve this problem by allowing you to create workload clusters from a cluster template that you design and provide in the form of a manifest file stored in a git directory. The manifest typically contains multiple related resources that together define an arbitrarily complex cluster. If you make subsequent changes to the cluster template, workload clusters originally created from it will automatically acquire the changes. In earlier versions of arlon (before v0.10), cluster templates were called base clusters . Some parts of the code still refer to base cluster manifests. This should be considered as a synonym for cluster template manifests. Note: Cluster Templates only support dynamic profiles.","title":"Next-Generation (gen2) Clusters - New in version 0.9.x"},{"location":"gen2_Tutorial/#creating-cluster-api-cluster-manifest","text":"Note: The CAPA version used here is v2.0 and the manifests created here are in accordance with this version. Refer the compatibility matrix for Cluster API provider and CAPA versions for supported versions. Before deploying a EKS cluster, make sure to setup the AWS Environment as stated in the quickstart giude for CAPI","title":"Creating Cluster-API cluster manifest"},{"location":"gen2_Tutorial/#machinedeployment","text":"Here is an example of a manifest file that we can use to create a cluster template . This manifest file helps in deploying an EKS cluster with 'machine deployment' component from the cluster API (CAPI). This file has been generated by the following command clusterctl generate cluster capi-quickstart --flavor eks \\ --kubernetes-version v1.24.0 \\ --control-plane-machine-count = 3 \\ --worker-machine-count = 3 \\ > capi-quickstart.yaml # YAML apiVersion : cluster.x-k8s.io/v1beta1 kind : Cluster metadata : name : capi-quickstart namespace : default spec : clusterNetwork : pods : cidrBlocks : - 192.168.0.0/16 controlPlaneRef : apiVersion : controlplane.cluster.x-k8s.io/v1beta2 kind : AWSManagedControlPlane name : capi-quickstart-control-plane infrastructureRef : apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSManagedCluster name : capi-quickstart --- apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSManagedCluster metadata : name : capi-quickstart spec : {} --- apiVersion : controlplane.cluster.x-k8s.io/v1beta2 kind : AWSManagedControlPlane metadata : name : capi-quickstart-control-plane namespace : default spec : region : { REGION } sshKeyName : { SSH_KEYNAME } version : v1.24.0 --- apiVersion : cluster.x-k8s.io/v1beta1 kind : MachineDeployment metadata : name : capi-quickstart-md-0 namespace : default spec : clusterName : capi-quickstart replicas : 3 selector : matchLabels : null template : spec : bootstrap : configRef : apiVersion : bootstrap.cluster.x-k8s.io/v1beta2 kind : EKSConfigTemplate name : capi-quickstart-md-0 clusterName : capi-quickstart infrastructureRef : apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSMachineTemplate name : capi-quickstart-md-0 version : v1.24.0 --- apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSMachineTemplate metadata : name : capi-quickstart-md-0 namespace : default spec : template : spec : iamInstanceProfile : nodes.cluster-api-provider-aws.sigs.k8s.io instanceType : { INSTANCE_TYPE } sshKeyName : { SSH_KEYNAME } --- apiVersion : bootstrap.cluster.x-k8s.io/v1beta2 kind : EKSConfigTemplate metadata : name : capi-quickstart-md-0 namespace : default spec : template : {}","title":"MachineDeployment"},{"location":"gen2_Tutorial/#awsmanagedmachinepool","text":"Initialize the environment for AWSManagedMachinePool as stated here Before deploying an EKS cluster, make sure that the MachinePool feature gate is enabled. To do so, run this command: kubectl describe deployment capa-controller-manager -n capa-system In the output, in the feature gates section of the deployment, MachinePool must be set to true. > kubectl describe deployment capa-controller-manager -n capa-system .......... .......... --featuregates = EKS = true,EKSEnableIAM = false,EKSAllowAddRoles = false,EKSFargate = true,MachinePool = true,EventBridgeInstanceState = false, AutoControllerIdentityCreator = true,BootstrapFormatIgnition = false,ExternalResourceGC = false .......... .......... This manifest file helps in deploying an EKS cluster with 'AWSManagedMachinePool' component from the cluster API (CAPI). This file has been generated by the following command clusterctl generate cluster awsmanaged-cluster --kubernetes-version v1.22.0 --flavor eks-managedmachinepool > manifest.yaml # YAML apiVersion : cluster.x-k8s.io/v1beta1 kind : Cluster metadata : name : awsmanaged-cluster namespace : default spec : clusterNetwork : pods : cidrBlocks : - 192.168.0.0/16 controlPlaneRef : apiVersion : controlplane.cluster.x-k8s.io/v1beta2 kind : AWSManagedControlPlane name : awsmanaged-cluster-control-plane infrastructureRef : apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSManagedCluster name : awsmanaged-cluster --- apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSManagedCluster metadata : name : awsmanaged-cluster namespace : default spec : {} --- apiVersion : controlplane.cluster.x-k8s.io/v1beta2 kind : AWSManagedControlPlane metadata : name : awsmanaged-cluster-control-plane namespace : default spec : region : { REGION } sshKeyName : { SSH_KEYNAME } version : v1.22.0 --- apiVersion : cluster.x-k8s.io/v1beta1 kind : MachinePool metadata : name : awsmanaged-cluster-pool-0 namespace : default spec : clusterName : awsmanaged-cluster replicas : 1 template : spec : bootstrap : dataSecretName : \"\" clusterName : awsmanaged-cluster infrastructureRef : apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSManagedMachinePool name : awsmanaged-cluster-pool-0 --- apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSManagedMachinePool metadata : name : awsmanaged-cluster-pool-0 namespace : default spec : {}","title":"AWSManagedMachinePool"},{"location":"gen2_Tutorial/#awsmachinepool","text":"An AWSMachinePool corresponds to an AWS AutoScaling Groups, which provides the cloud provider specific resource for orchestrating a group of EC2 machines. Initialize the environment for AWSMachinePool as stated here Before deploying an EKS cluster, make sure that the AWSMachinePool feature gate is enabled. To do so, run this command: kubectl describe deployment capa-controller-manager -n capa-system In the output, in the feature gates section of the deployment, MachinePool must be set to true. > kubectl describe deployment capa-controller-manager -n capa-system .......... .......... --featuregates = EKS = true,EKSEnableIAM = false,EKSAllowAddRoles = false,EKSFargate = true,MachinePool = true,EventBridgeInstanceState = false, AutoControllerIdentityCreator = true,BootstrapFormatIgnition = false,ExternalResourceGC = false .......... .......... This manifest file helps in deploying an EKS cluster with 'AWSManagedMachinePool' component from the cluster API (CAPI). This file has been generated by the following command clusterctl generate cluster awsmanaged-cluster --kubernetes-version v1.22.0 --flavor eks-machinepool > manifest.yaml # YAML apiVersion : cluster.x-k8s.io/v1beta1 kind : Cluster metadata : name : awsmanaged-cluster namespace : default spec : clusterNetwork : pods : cidrBlocks : - 192.168.0.0/16 controlPlaneRef : apiVersion : controlplane.cluster.x-k8s.io/v1beta2 kind : AWSManagedControlPlane name : awsmanaged-cluster-control-plane infrastructureRef : apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSManagedCluster name : awsmanaged-cluster --- apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSManagedCluster metadata : name : awsmanaged-cluster namespace : default spec : {} --- apiVersion : controlplane.cluster.x-k8s.io/v1beta2 kind : AWSManagedControlPlane metadata : name : awsmanaged-cluster-control-plane namespace : default spec : region : { REGION } sshKeyName : { SSH_KEYNAME } version : v1.22.0 --- apiVersion : cluster.x-k8s.io/v1beta1 kind : MachinePool metadata : name : awsmanaged-cluster-mp-0 namespace : default spec : clusterName : awsmanaged-cluster replicas : 1 template : spec : bootstrap : configRef : apiVersion : bootstrap.cluster.x-k8s.io/v1beta2 kind : EKSConfig name : awsmanaged-cluster-mp-0 clusterName : awsmanaged-cluster infrastructureRef : apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSMachinePool name : awsmanaged-cluster-mp-0 version : v1.22.0 --- apiVersion : infrastructure.cluster.x-k8s.io/v1beta2 kind : AWSMachinePool metadata : name : awsmanaged-cluster-mp-0 namespace : default spec : awsLaunchTemplate : iamInstanceProfile : nodes.cluster-api-provider-aws.sigs.k8s.io instanceType : { INSTANCE_TYPE } sshKeyName : { SSH_KEYNAME } maxSize : 10 minSize : 1 --- apiVersion : bootstrap.cluster.x-k8s.io/v1beta2 kind : EKSConfig metadata : name : awsmanaged-cluster-mp-0 namespace : default spec : {}","title":"AWSMachinePool"},{"location":"gen2_Tutorial/#gen2-cluster-creation-using-arlon","text":"This manifest file needs to be pushed to the workspace repository before the manifest directory is prepped and then validated. Before a manifest directory can be used as a cluster template, it must first be \"prepared\" or \"prepped\" by Arlon. The \"prep\" phase makes minor changes to the directory and manifest to help Arlon deploy multiple copies of the cluster without naming conflicts.","title":"gen2 cluster creation using Arlon"},{"location":"gen2_Tutorial/#manifest-directory-preparation","text":"To prepare a git directory to serve as cluster template, use the clustertemplate preparegit command: arlon clustertemplate preparegit --repo-url <repoUrl> --repo-path <pathToDirectory> [ --repo-revision revision ] # OR # using repository aliases # using the default alias arlon clustertemplate preparegit --repo-path <pathToDirectory> [ --repo-revision revision ] # using the prod alias arlon clustertemplate preparegit --repo-alias prod --repo-path <pathToDirectory> [ --repo-revision revision ]","title":"manifest directory preparation"},{"location":"gen2_Tutorial/#manifest-directory-validation","text":"Post the successful preparation of the cluster template manifest directory using clustertemplate preparegit , the cluster template manifest directory needs to be validated before the cluster template is created. To determine if a git directory is eligible to serve as cluster template, run the clustertemplate validategit command: arlon clustertemplate validategit --repo-url <repoUrl> --repo-path <pathToDirectory> [ --repo-revision revision ] # OR # using repository aliases # using the default alias arlon clustertemplate validategit --repo-path <pathToDirectory> [ --repo-revision revision ] # using the prod alias arlon clustertemplate validategit --repo-alias prod --repo-path <pathToDirectory> [ --repo-revision revision ]","title":"manifest directory validation"},{"location":"gen2_Tutorial/#gen2-cluster-creation","text":"Note: Cluster templates only support dynamic profiles. To create a gen2 workload cluster from the cluster template: arlon cluster create --cluster-name <clusterName> --repo-url <repoUrl> --repo-path <pathToDirectory> [ --output-yaml ] [ --profile <profileName> ] [ --repo-revision <repoRevision> ] # OR # using repository aliases # using the default alias arlon cluster create --cluster-name <clusterName> --repo-path <pathToDirectory> [ --output-yaml ] [ --profile <profileName> ] [ --repo-revision <repoRevision> ] # using the prod alias arlon cluster create --cluster-name <clusterName> --repo-alias prod --repo-path <pathToDirectory> [ --output-yaml ] [ --profile <profileName> ] [ --repo-revision <repoRevision> ]","title":"gen2 cluster creation"},{"location":"gen2_Tutorial/#gen2-cluster-creation-with-overrides","text":"We call the concept of constructing various clusters with patches from the same base manifest as cluster overrides. The cluster overrides feature is built on top of the existing base cluster design. So, A user can create a cluster from the base manifest using the same command as in the above step(gen2 cluster creation). Now, to create a cluster with overrides in the base manifest, a user should have the corresponding patch files in a single yaml file in local. Here is an example of a patch file where we want to override replicas count to 2 and change the sshkeyname: --- apiVersion: cluster.x-k8s.io/v1beta1 kind: MachineDeployment metadata: name: capi-quickstart-eks-md-0 spec: replicas: 2 --- apiVersion: controlplane.cluster.x-k8s.io/v1beta1 kind: AWSManagedControlPlane metadata: name: capi-quickstart-eks-control-plane spec: sshKeyName: random --- ``` Note: The metadata field in the patch file should be same as of the metadata field in the resources file. Refer to this [ document ]( https://blog.scottlowe.org/2019/11/12/using-kustomize-with-cluster-api-manifests/ ) to know more about patch files Command to create a gen2 workload cluster form the cluster template manifest with overrides to the manifest is: ``` shell arlon cluster create <cluster-name> --repo-url <repo url where cluster template is present> --repo-path <repo path to the cluster template> --overrides-path <path to the patch file> --patch-repo-url <repo url where patch file should be stored> --patch-repo-path <repo path to store the patch files> ```` Runnning the above command will create a cluster named folder in patch repo path of patch repo url which contains the patch files, kustomization.yaml and configurations.yaml which are used to create the cluster app. Note that the patch file repo url can be different or same from the cluster template repo url acoording to the requirement of the user. A user can use a different repo url for string patch files for the cluster. ## gen2 cluster update To update the profiles of a gen2 workload cluster: ``` shell # To add a new profile to the existing cluster arlon cluster ngupdate <clustername> --profile <profilename> # To delete an existing profile from the existing cluster arlon cluster ngupdate <clustername> --delete-profile A gen2 cluster can be created without any profile app associated with the cluster. So, the above commands can be used to add a new profile to the existing cluster which will create profile app in argocd along with bundle apps associated with the profile. An existing profile can be deleted from the cluster as well using the above command. Executing this command will delete the profile app and all the bundles associated with the profile in argocd.","title":"gen2 cluster creation with overrides"},{"location":"gen2_Tutorial/#gen2-cluster-deletion","text":"To destroy a gen2 workload cluster: arlon cluster delete <clusterName> Arlon creates between 2 and 3 ArgoCD application resources to compose a gen2 cluster (the 3rd application, called \"profile app\", is used when an optional profile is specified at cluster creation time). When you destroy a gen2 cluster, Arlon will find all related ArgoCD applications and clean them up. If the cluster which which is being deleted is a cluster created using patch files, the controller first cleans the git repo where the respective patch files of the cluster are present and then it destroys all the related ArgoCD applications and clean them up.","title":"gen2 cluster deletion"},{"location":"gen2_Tutorial/#enabling-cluster-autoscaler-in-the-workload-cluster","text":"To create a gen2 cluster with autoscaler, we need: bundle pointing to the bundle/capi-cluster-autoscaler in the arlon repository. dynamic profile that contains the above bundle. a cluster template manifest(that makes use of MachineDeployment and not MachinePools) which has the CAPI annotations for min and max nodes set ( as a part of preparegit or manually add it ). run arlon cluster create with the repo-path pointing to the cluster template manifest described in the step above, set the profile to be the one created in step 2 and pass the autoscaler flag.","title":"Enabling Cluster Autoscaler in the workload cluster"},{"location":"gen2_Tutorial/#bundle-creation","text":"Register a dynamic bundle pointing to the bundles/capi-cluster-autoscaler in the Arlon repo. To enable the cluster-autoscaler bundle, add one more parameter during cluster creation: srcType . This is the ArgoCD-defined application source type (Helm, Kustomize, Directory). In addition to this, the repo-revision parameter should also be set to a stable arlon release branch ( in this case v0.10 ). This example creates a bundle pointing to the bundles/capi-cluster-autoscaler in Arlon repo arlon bundle create cas-bundle --tags cas,devel,test --desc \"CAS Bundle\" --repo-url https://github.com/arlonproj/arlon.git --repo-path bundles/capi-cluster-autoscaler --srctype helm --repo-revision v0.10","title":"Bundle creation"},{"location":"gen2_Tutorial/#profile-creation","text":"Create a profile that contains this capi-cluster-autoscaler bundle. arlon profile create dynamic-cas --repo-url <repoUrl> --repo-base-path profiles --bundles cas-bundle --desc \"dynamic cas profile\" --tags examples","title":"Profile creation"},{"location":"gen2_Tutorial/#manifest-directory-preparation_1","text":"Two additional properties cas-max and cas-min are used to set 2 annotations for Max/Min nodes on MachineDeployment required by the cluster autoscaler for CAPI as a part of the manifest directory preparation. These are the annotations required by the MachineDeployment for autoscaling. Note: These are the default values for the cas-min and cas-max properties annotations: cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: '1' cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: '9' These annotations are added during the preparegit step. If these annotations are already present in the manifest file, then they will not be added again as a part of preparegit Preparing the git directory to serve as the cluster template: arlon clustertemplate preparegit --repo-path <pathToDirectory> --cas-min 1 --cas-max 9 --repo-url <repoUrl>","title":"manifest directory preparation"},{"location":"gen2_Tutorial/#manifest-directory-validation_1","text":"To determine if a git directory is eligible to serve as cluster template, run the clustertemplate validategit command: arlon clustertemplate validategit --repo-path <pathToDirectory> --repo-url <repoUrl>","title":"manifest directory validation"},{"location":"gen2_Tutorial/#gen2-cluster-creation-with-autoscaling-enabled","text":"To add CAS support for gen2 clusters, the cluster create sub-command of the arlon CLI has a autoscaler flag which deploys the capi-cluster-autoscaler helm chart on the management cluster. To create a gen2 workload with a cluster-autoscaler pod running, from the cluster template, run this command: Note: Use the dynamic profile that was created in the previous steps arlon cluster create --cluster-name <clusterName> --repo-url <repoUrl> --repo-path <pathToDirectory> --profile dynamic-cas --autoscaler","title":"gen2 cluster creation with autoscaling enabled"},{"location":"gen2_Tutorial/#known-issues-and-limitations","text":"Gen2 clusters are powerful because the cluster template can be arbitrarily complex and feature rich. Since they are fairly new and still evolving, gen2 clusters have several known limitations relative to gen1. You cannot customize/override any property of the cluster template on the fly when creating a workload cluster, which is an exact clone of the cluster template except for the names of its resources and their namespace. The work-around is to make a copy of the cluster template directory, push the new directory, make the desired changes, commit & push the changes, and register the directory as a new cluster template. The clusters created directly from the cluster template are completely declarative whereas the clusters which are created using override property are not completely declarative. If a user passes a different repository for patch repo url from the repo where cluster template is present, argocd won't be able to detect if there are any changes in the cluster template repository but will deect all the chnages in patch repo url for the cluster. If you modify and commit a change to one or more properties of the cluster template that the underlying Cluster API provider deems as \"immutable\", new workload clusters created from the cluster template will have the modified propert(ies), but ArgoCD will flag existing clusters as OutOfSync, since the provider will continually reject attempts to apply the new property values. The existing clusters continue to function, despite appearing unhealthy in the ArgoCD UI and CLI outputs. Examples of mutable properties in Cluster API resources: Number of replicas (modification will result in a scale-up / down) Kubernetes version (modification will result in an upgrade) Examples of immutable properties: Most fields of AWSMachineTemplate (instance type, labels, etc...)","title":"Known issues and limitations"},{"location":"gen2_Tutorial/#for-more-information","text":"For more details on gen2 clusters, refer to the design document .","title":"For more information"},{"location":"gen2_overrides_proposal_1/","text":"Gen2 Cluster Overrides - Proposal 1 \u00b6 This is a design proposal doc for gen2 cluster overrides. Right now, according to our gen2 design, we can deploy multiple clusters with same specifications from one cluster template. But what if we want to deploy cluster with a different sshkeyname from the same manifest?. To allow deploying clusters with different specifications from the same cluster template we are introducing the concept of clusteroverrides. So, clusteroverrides is being able to deploy clusters with different specs using same manifest and overriding the specs which we want to change. We have 2 different approches to override in a cluster: Converting the git repo where cluster template is present to helm charts Overriding specifies fields using kustomize In the first approach, We first let user upload the cluster template in the repo and deploy the cluster from it and then convert it into helm chart, so that we will be able to override fields in the manifest. The downside of this approach is that we don't have a specific template for cluster template, a user can use any form of the template in which case we will not be able to convert the manifest to heml chart. So, continuing with the 2nd approach, in the kustomize approach, we create an overlay folder parallel to the cluster template folder which contains folders named with the cluster name. These cluster named folders contain the specific override files to the cluster. An example of the folder structure is as belows: A directory layout \u00b6 . \u251c\u2500\u2500 ClusterTemplate # ClusterTemplate folder(Contains cluster template) \u251c\u2500\u2500 Overlays # Contains folders specific to each cluster created from cluster template \u251c\u2500\u2500 Cluster1 # Contains overrides corresponding to cluster1 \u251c\u2500\u2500 Cluster2 # Contains overrides corresponding to cluster2 Let's consider an example case to understand the kustomize approach \u00b6 Let's consider three different clusters on AWS. The management cluster already exists. Two of these clusters will run in the AWS \u201cus-west-2\u201d region, while the third will run in the \u201cus-east-2\u201d region. One of the two \u201cus-west-2\u201d clusters will use larger instance types to accommodate more resource-intensive workloads. Now, we need to get our gitrepo ready by pushing the cluster template manifest into a folder named clustertemplate and for overriding, we need to create a folder for each cluster with the cluster name and place them in overlays folder which is parallel to the clustertemplate folder Setting up the Directory Structure \u00b6 To accommodate this use case, we will need to use a directory structure that supports the use of kustomize overlays. Therefore, the directory structure would look like this for the project: (parent) |- clustertemplate |- overlays |- usw2-cluster1 |- usw2-cluster2 |- use2-cluster1 The clustertemplate directory will store the cluster template for the final Cluster API manifests, as well as a kustomization.yaml file that identifies these Cluster API manifests as resources for kustomize to use. The contents of kustomize file in clustertemplate folder is as follows: resources : - clustertemplatemanifest.yaml The kustomization.yaml states that the resources for cluster is in cluster templatemanifest.yaml file What consists in the cluster named folders? \u00b6 The intriguing parts begin with the overlays. You will need to provide the cluster-specific patches kustomize will use to generate the YAML for each cluster in its own directory under the overlays directory. With the \"usw2-cluster1\" overlay, let's begin. You must first comprehend what modifications must be made to the basic configuration in order to develop the appropriate configuration for this specific cluster in order to grasp what will be required. We can use two methods for patches 1. JSON patches 2. YAML patches In JSON patches, we have to write a JSON file to replace fields in the manifests. So, we need to write a different file for each replace and that would become hectic. Example of a JSON patch: [ { \"op\": \"replace\", \"path\": \"/metadata/name\", \"value\": \"usw2-cluster-1\" }, { \"op\": \"replace\", \"path\": \"/spec/infrastructureRef/name\", \"value\": \"usw2-cluster-1\" } ] So, let's discuss the YAML approach which will be much easier to handle the overrides --- apiVersion: cluster.x-k8s.io/v1alpha2 kind: Machine metadata: name: .* labels: cluster.x-k8s.io/cluster-name: \"usw2-cluster1\" This will add a cluster name field to label in the manifest which is an advantage over JSON approach. We can both add and replace fields in manifest unlike just just replace in JSON aproach. You would once more require a reference to the patch file in kustomization for this last example. Both the patch file itself and kustomization.yaml This kustomization.yaml file will be pointing to both the cluster template manifest and patch file basically working like a link between both the cluster and manifest. Using this particular approach the present cluster template approach will need to take a redesign as we will need to skip the name suffix method we using before to create a manifest for each cluster respectively with their own names. In this approach, Instead of the configurations.yaml(Needed for name suffix), we will have a folder for each cluster and argocd path pointing to the cluster folder. This will help us in skipping the name suffix method we were using before. We will be able to basically override any of the field in manifest without any limitations before creating a cluster using this approach. UX(User experience): \u00b6 To provide a user the freedom to completely override any part of the cluster template manifest, we ask the user to point to a yaml file in which the fields have been overridden. This would be easier to user as well because he/she would generate the manifest file anyway. So, they need to make changes to the already generated and point it. But we should even take care of the point that the cluster template manifest in the git and the overriden manifest file are comparable. Example of a command: arlon cluster create <cluster name> --repo-url <repo url> --repo-path <repo path> --overrides <path to overriden manifest file> Limitations: \u00b6 Manifests (clustertemplate and overlays) for the clustertemplate cluster as well as workload clusters reside in the same repository. This means those who create the workload cluster will need write access to the clustertemplate repository which might not be the case in enterprises. So, if we consider having the manifests (clustertemplate and overlays) are in different repositories, they will need a link to each other and as of now, if we update the cluster template while having manifest in one repo and patches in another repo. Argocd will not be able to take up the updated changes in cluster template The main goal of gen2 clusters was to remove the dependency on git to store metadata and make the clusters completely declarative unlike gen1 clusters. But here, we re-introduce a dependency on Arlon API (library) and git (state in git with dir structure) Although we can make this approach declarative by introducing another controller (CRD), this would increase the whole complexity of the issue and arlon. Using this approach, we might not be able to prefix a name in the cluster template manifest which is an issue because, Some resources generate external resources, like AWS load balancer and we need to avoid naming conflict - hence name prefix (not sufficient) + name reference in gen2 is required","title":"Gen2 Cluster Overrides - Proposal 1"},{"location":"gen2_overrides_proposal_1/#gen2-cluster-overrides-proposal-1","text":"This is a design proposal doc for gen2 cluster overrides. Right now, according to our gen2 design, we can deploy multiple clusters with same specifications from one cluster template. But what if we want to deploy cluster with a different sshkeyname from the same manifest?. To allow deploying clusters with different specifications from the same cluster template we are introducing the concept of clusteroverrides. So, clusteroverrides is being able to deploy clusters with different specs using same manifest and overriding the specs which we want to change. We have 2 different approches to override in a cluster: Converting the git repo where cluster template is present to helm charts Overriding specifies fields using kustomize In the first approach, We first let user upload the cluster template in the repo and deploy the cluster from it and then convert it into helm chart, so that we will be able to override fields in the manifest. The downside of this approach is that we don't have a specific template for cluster template, a user can use any form of the template in which case we will not be able to convert the manifest to heml chart. So, continuing with the 2nd approach, in the kustomize approach, we create an overlay folder parallel to the cluster template folder which contains folders named with the cluster name. These cluster named folders contain the specific override files to the cluster. An example of the folder structure is as belows:","title":"Gen2 Cluster Overrides - Proposal 1"},{"location":"gen2_overrides_proposal_1/#a-directory-layout","text":". \u251c\u2500\u2500 ClusterTemplate # ClusterTemplate folder(Contains cluster template) \u251c\u2500\u2500 Overlays # Contains folders specific to each cluster created from cluster template \u251c\u2500\u2500 Cluster1 # Contains overrides corresponding to cluster1 \u251c\u2500\u2500 Cluster2 # Contains overrides corresponding to cluster2","title":"A directory layout"},{"location":"gen2_overrides_proposal_1/#lets-consider-an-example-case-to-understand-the-kustomize-approach","text":"Let's consider three different clusters on AWS. The management cluster already exists. Two of these clusters will run in the AWS \u201cus-west-2\u201d region, while the third will run in the \u201cus-east-2\u201d region. One of the two \u201cus-west-2\u201d clusters will use larger instance types to accommodate more resource-intensive workloads. Now, we need to get our gitrepo ready by pushing the cluster template manifest into a folder named clustertemplate and for overriding, we need to create a folder for each cluster with the cluster name and place them in overlays folder which is parallel to the clustertemplate folder","title":"Let's consider an example case to understand the kustomize approach"},{"location":"gen2_overrides_proposal_1/#setting-up-the-directory-structure","text":"To accommodate this use case, we will need to use a directory structure that supports the use of kustomize overlays. Therefore, the directory structure would look like this for the project: (parent) |- clustertemplate |- overlays |- usw2-cluster1 |- usw2-cluster2 |- use2-cluster1 The clustertemplate directory will store the cluster template for the final Cluster API manifests, as well as a kustomization.yaml file that identifies these Cluster API manifests as resources for kustomize to use. The contents of kustomize file in clustertemplate folder is as follows: resources : - clustertemplatemanifest.yaml The kustomization.yaml states that the resources for cluster is in cluster templatemanifest.yaml file","title":"Setting up the Directory Structure"},{"location":"gen2_overrides_proposal_1/#what-consists-in-the-cluster-named-folders","text":"The intriguing parts begin with the overlays. You will need to provide the cluster-specific patches kustomize will use to generate the YAML for each cluster in its own directory under the overlays directory. With the \"usw2-cluster1\" overlay, let's begin. You must first comprehend what modifications must be made to the basic configuration in order to develop the appropriate configuration for this specific cluster in order to grasp what will be required. We can use two methods for patches 1. JSON patches 2. YAML patches In JSON patches, we have to write a JSON file to replace fields in the manifests. So, we need to write a different file for each replace and that would become hectic. Example of a JSON patch: [ { \"op\": \"replace\", \"path\": \"/metadata/name\", \"value\": \"usw2-cluster-1\" }, { \"op\": \"replace\", \"path\": \"/spec/infrastructureRef/name\", \"value\": \"usw2-cluster-1\" } ] So, let's discuss the YAML approach which will be much easier to handle the overrides --- apiVersion: cluster.x-k8s.io/v1alpha2 kind: Machine metadata: name: .* labels: cluster.x-k8s.io/cluster-name: \"usw2-cluster1\" This will add a cluster name field to label in the manifest which is an advantage over JSON approach. We can both add and replace fields in manifest unlike just just replace in JSON aproach. You would once more require a reference to the patch file in kustomization for this last example. Both the patch file itself and kustomization.yaml This kustomization.yaml file will be pointing to both the cluster template manifest and patch file basically working like a link between both the cluster and manifest. Using this particular approach the present cluster template approach will need to take a redesign as we will need to skip the name suffix method we using before to create a manifest for each cluster respectively with their own names. In this approach, Instead of the configurations.yaml(Needed for name suffix), we will have a folder for each cluster and argocd path pointing to the cluster folder. This will help us in skipping the name suffix method we were using before. We will be able to basically override any of the field in manifest without any limitations before creating a cluster using this approach.","title":"What consists in the cluster named folders?"},{"location":"gen2_overrides_proposal_1/#uxuser-experience","text":"To provide a user the freedom to completely override any part of the cluster template manifest, we ask the user to point to a yaml file in which the fields have been overridden. This would be easier to user as well because he/she would generate the manifest file anyway. So, they need to make changes to the already generated and point it. But we should even take care of the point that the cluster template manifest in the git and the overriden manifest file are comparable. Example of a command: arlon cluster create <cluster name> --repo-url <repo url> --repo-path <repo path> --overrides <path to overriden manifest file>","title":"UX(User experience):"},{"location":"gen2_overrides_proposal_1/#limitations","text":"Manifests (clustertemplate and overlays) for the clustertemplate cluster as well as workload clusters reside in the same repository. This means those who create the workload cluster will need write access to the clustertemplate repository which might not be the case in enterprises. So, if we consider having the manifests (clustertemplate and overlays) are in different repositories, they will need a link to each other and as of now, if we update the cluster template while having manifest in one repo and patches in another repo. Argocd will not be able to take up the updated changes in cluster template The main goal of gen2 clusters was to remove the dependency on git to store metadata and make the clusters completely declarative unlike gen1 clusters. But here, we re-introduce a dependency on Arlon API (library) and git (state in git with dir structure) Although we can make this approach declarative by introducing another controller (CRD), this would increase the whole complexity of the issue and arlon. Using this approach, we might not be able to prefix a name in the cluster template manifest which is an issue because, Some resources generate external resources, like AWS load balancer and we need to avoid naming conflict - hence name prefix (not sufficient) + name reference in gen2 is required","title":"Limitations:"},{"location":"gen2_overrides_proposal_2/","text":"Gen2 cluster overrides - Proposal 2 \u00b6 This is an update to proposal of Gen2 cluster overrides proposal design to add the ability to override the Gen 2 cluster. This update doc has a more clear version of the proposal. Broader idea of the proposal \u00b6 Basically, in the cluster overrides we want to generate different clusters with patches from the same cluster template. So, considering this feature in enterprise scope, the user who is uploading the cluster template in git might be an admin and another employee who wants to create a cluster form the cluster template might not have access to the git repo where the cluster template is present. So, In this approach a user can use a different git repository to store the patches of a manifest and create the cluster from the cluster template which is in different repository. Let's consider an example where our manifest is a repo called arlon-bc and with the repo path bc1. So, these are the files which will be present in bc1 folder: Resouces file which is the cluster template Kustomization.yaml Contents of the kustomization.yaml file are as follows: resources: - capi-quickstart-eks.yaml In this case capi-quickstart-eks.yaml is the name of the cluster template. Now, let's say that our patch file is in another repository. This repository should caontain a folder named with the cluster's name and the files inside the cluster named folder are: configurations.yaml configurations.yaml file corresponds to name suffix addition to the yaml file. kustomization.yaml kustomization.yaml file contains fields for resouces, configurations and patches as shown in the example below: --- apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - git::https://github.com/jayanth-tjvrr/arlon-bc//bc1 configurations: - configurations.yaml patches: - target: group: controlplane.cluster.x-k8s.io version: v1beta1 kind: MachineDeployment path: md-details.yaml patch files These patch files contain the patches which we want to include for the cluster As we can observe in the above example, the resource field in kustomization.yaml is pointing to a different repository which contains our cluster template. This is how we first organize our repositories to deploy a cluster with patches Once, we organize the clusters, while creating the argocd app, the path of the argocd app should point to the cluster named folder. The code will then run kustomize build in the patch file directory and that will produce our final manifest. A user can just print out the manifest without letting the code apply it. This makes this approach declarative. At the same time, a user can even opt to apply the manifest to argocd app from the code. This is a clear proposal of the approach which we will be following for Gen2 cluster overrides. Limitations: \u00b6 Since, we are pointing to the repository where our patch file is present, the argocd won't be able to detect the changes in the cluster template repository. - This can be an added feature aw well in one way because whenever there is a change in the cluster template configuration, it won't be immediately picked up. This brings user the ability to promote cluster template changes sensibly through each of our environments. Clean up of the cluster named folders when a user deletes a cluster is one of the issue which is still being addressed and looked up on. UX (User experience): \u00b6 A user can pass the patch file for the cluster as an argument while executing the arlon cluster create .. command with the --override flag. The command would look like: arlon cluster create <cluster-name> --repo-url <repo url where patch files should be present> --repo-path <repo path to cluster named folder> --override <path to the patch file>","title":"Gen2 overrides proposal 2"},{"location":"gen2_overrides_proposal_2/#gen2-cluster-overrides-proposal-2","text":"This is an update to proposal of Gen2 cluster overrides proposal design to add the ability to override the Gen 2 cluster. This update doc has a more clear version of the proposal.","title":"Gen2 cluster overrides - Proposal 2"},{"location":"gen2_overrides_proposal_2/#broader-idea-of-the-proposal","text":"Basically, in the cluster overrides we want to generate different clusters with patches from the same cluster template. So, considering this feature in enterprise scope, the user who is uploading the cluster template in git might be an admin and another employee who wants to create a cluster form the cluster template might not have access to the git repo where the cluster template is present. So, In this approach a user can use a different git repository to store the patches of a manifest and create the cluster from the cluster template which is in different repository. Let's consider an example where our manifest is a repo called arlon-bc and with the repo path bc1. So, these are the files which will be present in bc1 folder: Resouces file which is the cluster template Kustomization.yaml Contents of the kustomization.yaml file are as follows: resources: - capi-quickstart-eks.yaml In this case capi-quickstart-eks.yaml is the name of the cluster template. Now, let's say that our patch file is in another repository. This repository should caontain a folder named with the cluster's name and the files inside the cluster named folder are: configurations.yaml configurations.yaml file corresponds to name suffix addition to the yaml file. kustomization.yaml kustomization.yaml file contains fields for resouces, configurations and patches as shown in the example below: --- apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - git::https://github.com/jayanth-tjvrr/arlon-bc//bc1 configurations: - configurations.yaml patches: - target: group: controlplane.cluster.x-k8s.io version: v1beta1 kind: MachineDeployment path: md-details.yaml patch files These patch files contain the patches which we want to include for the cluster As we can observe in the above example, the resource field in kustomization.yaml is pointing to a different repository which contains our cluster template. This is how we first organize our repositories to deploy a cluster with patches Once, we organize the clusters, while creating the argocd app, the path of the argocd app should point to the cluster named folder. The code will then run kustomize build in the patch file directory and that will produce our final manifest. A user can just print out the manifest without letting the code apply it. This makes this approach declarative. At the same time, a user can even opt to apply the manifest to argocd app from the code. This is a clear proposal of the approach which we will be following for Gen2 cluster overrides.","title":"Broader idea of the proposal"},{"location":"gen2_overrides_proposal_2/#limitations","text":"Since, we are pointing to the repository where our patch file is present, the argocd won't be able to detect the changes in the cluster template repository. - This can be an added feature aw well in one way because whenever there is a change in the cluster template configuration, it won't be immediately picked up. This brings user the ability to promote cluster template changes sensibly through each of our environments. Clean up of the cluster named folders when a user deletes a cluster is one of the issue which is still being addressed and looked up on.","title":"Limitations:"},{"location":"gen2_overrides_proposal_2/#ux-user-experience","text":"A user can pass the patch file for the cluster as an argument while executing the arlon cluster create .. command with the --override flag. The command would look like: arlon cluster create <cluster-name> --repo-url <repo url where patch files should be present> --repo-path <repo path to cluster named folder> --override <path to the patch file>","title":"UX (User experience):"},{"location":"gen2_overrides_proposal_3/","text":"Gen2 cluster overrides - final proposal \u00b6 This is an update to proposal of Gen2 cluster overrides proposal-2 design to add the ability to override the Gen 2 cluster. This update doc has the final version of clusterride feature. Design of overrides feature for gen2 cluster \u00b6 Basically, we want to construct various clusters with patches from the same cluster template in the cluster overrides. The person uploading the cluster template in git may be an administrator, and another employee who wants to construct a cluster from the cluster template may not have access to the git repository where the cluster template is located. This is because the capability is intended to be used in an enterprise setting. The cluster overrides feature is built on top of the existing cluster template design. So, there won't be any changes in the design of cluster template folder. Let's consider an example where our manifest is a repo called arlon-bc and with the repo path bc1. So, these are the files which will be present in bc1 folder: Resouces file which is the cluster template Kustomization.yaml configurations.yaml Contents of the kustomization.yaml file are as follows: resources: - capi-quickstart-eks.yaml configurations: - configurations.yaml In this case capi-quickstart-eks.yaml is the name of the cluster template. Now, let's say that our patch file is in another repository. This repository should caontain a folder named with the cluster's name and the files inside the cluster named folder are: configurations.yaml configurations.yaml file corresponds to name suffix addition to the yaml file. kustomization.yaml kustomization.yaml file contains fields for resouces, configurations and patches as shown in the example below: --- apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - git::https://github.com/jayanth-tjvrr/arlon-bc//bc1 configurations: - configurations.yaml patches: - target: group: controlplane.cluster.x-k8s.io version: v1beta1 kind: MachineDeployment path: md-details.yaml patch files These patch files contain the patches which we want to include for the cluster As we can observe in the above example, the resource field in kustomization.yaml is pointing to a different repository which contains our cluster template. Exampple of how a patch file looks like: --- apiVersion: cluster.x-k8s.io/v1beta1 kind: MachineDeployment metadata: name: .* spec: replicas: 2 This is how we first organize our repositories to deploy a cluster with patches When we structure the clusters, the path of the resulting argocd app should direct users to the cluster-specific folder. After that, the code will launch kustomize build in the patch file directory, which will result in the creation of our final manifest. The manifest can be printed out directly by the user without having to go through the code. This technique is declarative as a result. A user can choose to apply the manifest to the Argocd app directly from the code at the same time. The above patch file directory structure is only applicable to clusters which have patch files associated with them. Other clusters which are built from the cluster template directly with no patch files attached won't have any directory created anywhere in git. When a user deletes a cluster which has patch files associated with it. The patch files get cleaned up from the repository as well. This is a clear proposal of the approach which we will be following for Gen2 cluster overrides. Limitations: \u00b6 Since, we are pointing to the repository where our patch file is present, the argocd won't be able to detect the changes in the cluster template repository. This can be an added feature aw well in one way because whenever there is a change in the cluster template configuration, it won't be immediately picked up. This brings user the ability to promote cluster template changes sensibly through each of our environments. The cluster created using overrides approach are not completely declarative. UX (User experience): \u00b6 A user can pass the patch file for the cluster as an argument while executing the arlon cluster create .. command with the --override flag. The command would look like: arlon cluster create <cluster-name> --repo-url <repo url where cluster template is present> --repo-path <repo path to the cluster template> --override <path to the patch file> --patch-repo-url <repo url where patch file should be stored> --patch-repo-path <repo path to store the patch file> The above command will create a cluster named folder in patch repo url which contains all the patch files and the argocd app created for the respective cluster will be pointing to the cluster named folder which has been created. To delete a cluster, a user can follow the same command as usual, arlon cluster delete <cluster-name> This command checks if the cluster is overriden and if it is overrides, then the code first deletes the associated cluster named folder and then it deletes the argocd app.","title":"Gen2 overrides proposal 3"},{"location":"gen2_overrides_proposal_3/#gen2-cluster-overrides-final-proposal","text":"This is an update to proposal of Gen2 cluster overrides proposal-2 design to add the ability to override the Gen 2 cluster. This update doc has the final version of clusterride feature.","title":"Gen2 cluster overrides - final proposal"},{"location":"gen2_overrides_proposal_3/#design-of-overrides-feature-for-gen2-cluster","text":"Basically, we want to construct various clusters with patches from the same cluster template in the cluster overrides. The person uploading the cluster template in git may be an administrator, and another employee who wants to construct a cluster from the cluster template may not have access to the git repository where the cluster template is located. This is because the capability is intended to be used in an enterprise setting. The cluster overrides feature is built on top of the existing cluster template design. So, there won't be any changes in the design of cluster template folder. Let's consider an example where our manifest is a repo called arlon-bc and with the repo path bc1. So, these are the files which will be present in bc1 folder: Resouces file which is the cluster template Kustomization.yaml configurations.yaml Contents of the kustomization.yaml file are as follows: resources: - capi-quickstart-eks.yaml configurations: - configurations.yaml In this case capi-quickstart-eks.yaml is the name of the cluster template. Now, let's say that our patch file is in another repository. This repository should caontain a folder named with the cluster's name and the files inside the cluster named folder are: configurations.yaml configurations.yaml file corresponds to name suffix addition to the yaml file. kustomization.yaml kustomization.yaml file contains fields for resouces, configurations and patches as shown in the example below: --- apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - git::https://github.com/jayanth-tjvrr/arlon-bc//bc1 configurations: - configurations.yaml patches: - target: group: controlplane.cluster.x-k8s.io version: v1beta1 kind: MachineDeployment path: md-details.yaml patch files These patch files contain the patches which we want to include for the cluster As we can observe in the above example, the resource field in kustomization.yaml is pointing to a different repository which contains our cluster template. Exampple of how a patch file looks like: --- apiVersion: cluster.x-k8s.io/v1beta1 kind: MachineDeployment metadata: name: .* spec: replicas: 2 This is how we first organize our repositories to deploy a cluster with patches When we structure the clusters, the path of the resulting argocd app should direct users to the cluster-specific folder. After that, the code will launch kustomize build in the patch file directory, which will result in the creation of our final manifest. The manifest can be printed out directly by the user without having to go through the code. This technique is declarative as a result. A user can choose to apply the manifest to the Argocd app directly from the code at the same time. The above patch file directory structure is only applicable to clusters which have patch files associated with them. Other clusters which are built from the cluster template directly with no patch files attached won't have any directory created anywhere in git. When a user deletes a cluster which has patch files associated with it. The patch files get cleaned up from the repository as well. This is a clear proposal of the approach which we will be following for Gen2 cluster overrides.","title":"Design of overrides feature for gen2 cluster"},{"location":"gen2_overrides_proposal_3/#limitations","text":"Since, we are pointing to the repository where our patch file is present, the argocd won't be able to detect the changes in the cluster template repository. This can be an added feature aw well in one way because whenever there is a change in the cluster template configuration, it won't be immediately picked up. This brings user the ability to promote cluster template changes sensibly through each of our environments. The cluster created using overrides approach are not completely declarative.","title":"Limitations:"},{"location":"gen2_overrides_proposal_3/#ux-user-experience","text":"A user can pass the patch file for the cluster as an argument while executing the arlon cluster create .. command with the --override flag. The command would look like: arlon cluster create <cluster-name> --repo-url <repo url where cluster template is present> --repo-path <repo path to the cluster template> --override <path to the patch file> --patch-repo-url <repo url where patch file should be stored> --patch-repo-path <repo path to store the patch file> The above command will create a cluster named folder in patch repo url which contains all the patch files and the argocd app created for the respective cluster will be pointing to the cluster named folder which has been created. To delete a cluster, a user can follow the same command as usual, arlon cluster delete <cluster-name> This command checks if the cluster is overriden and if it is overrides, then the code first deletes the associated cluster named folder and then it deletes the argocd app.","title":"UX (User experience):"},{"location":"gen2_profiles_proposal_1/","text":"Gen2 Profiles - Proposal 1 \u00b6 In this design proposal, gen2 profiles are completely built on native ArgoCD ApplicationSets and resource labels. There are no first-class Arlon objects. Object model \u00b6 Arlon Application: a thin wrapper around an ApplicationSet. An ApplicationSet is an Arlon Application if it has the managed-by=arlon label. Profile name: any unique label value that appears in the spec.generators[0].clusters.selector.matchExpressions.values[] array of at least one Arlon application. Cluster: any cluster registered in ArgoCD. Not limited to clusters created by Arlon. Observations: - A profile can be associated with any number of applications. And an application can be associated with multiple profiles. - A cluster is said to be associated with a profile if it is labeled with arlon.io/profile=<profileName> . A cluster can be associated with at most one profile. A profile may be associated (attached to) any number of clusters. Usage \u00b6 Managing applications \u00b6 arlon app list shows the current list of Arlon applications. It is initially empty. The prototype does not currently support direct Arlon application creation. (This is easy to add later as a new command) An Arlon app has to be created manually by one of these methods: - Create a new ApplicationSet from a YAML file with - The managed-by=arlon label - The spec as follows: spec: generators: - clusters: selector: matchExpressions: - key: arlon.io/profile operator: In values: [] - Modify an existing ApplicationSet with the above requirements Managing profiles \u00b6 Profiles are not first class objects. They only exist as labels referenced by applications and placed on clusters. If a particular profile label value is not referenced from any application, it does not exist. arlon ngprofile list shows the current list of profiles, the applications associated with each profile, and the clusters currently using each profile. The list is constructed by scanning all applications and determining the unique set of labels referenced in the matchExpressions.values[] array of each. To create a profile that doesn't exist yet, it needs to be added to at least one application's label set. This is conceptually achieved by \"adding the app to the profile\": arlon app addtoprofile <appName> <profileName> Conversely, a profile label can be removed from an application by \"removing the app from the profile\": arlon app removefromprofile <appname> <profileName> Caution: this can cause the profile to cease to exist if that was the last app referencing it. Associating profiles with clusters \u00b6 A cluster can have at most one profile attached to it. To attach a profile to a cluster: arlon nprofile attach <profilename> <clustername> Similarly, to detach: arlon nprofile detach <profilename> <clustername> Internally, an attach operation simply labels the cluster (via ArgoCD API) with the arlon.io/profile=<profileName> key value pair. Discussion \u00b6 Pros of the design: * Lightweight, elegant, simple * Fully declarative (no new resources introduced, relies entirely on existing ArgoCD resources) * Does not require \"workspace git repo\" since a profile has no compiled component. Cons: * Profiles are not first class objects. A profile can cease to exist if it becomes unreferenced from any application. This can be confusing to users. For the same reason, you can't create an empty profile and add apps to it later. This can be alleviated by clearly documenting the fact that profiles are just label values. Once the user understand this, everything will become clearer, and the simplicity of the design can begin to outweigh its quirks. * A cluster can only have one gen2 profile attached to it. This is a result of the limited expressiveness of the matchExpressions logic. In contrast, any number of gen1 profiles can be attached to a cluster (the current implementation only allows one, but could be enhanced to allow many) * It's impossible to specify per-cluster overrides for an application. That's because an ApplicationSet can be deployed to multiple clusters if they have a matching profile label. (To be fair, neither gen1 profiles nor gen2 clusters support cluster overrides either, but for a different reason. This is tracked in a github issue) * Any limitations of ApplicationSets (for e.g. lack of Sync Wave support?) will apply to Arlon Apps using gen2 profiles. * The lightweight nature of this design may cause some to perceive Arlon's contribution to be very minimal (it's a thin wrapper around ArgoCD constructs). * Relies on ApplicationSet, which is ArgoCD specific, making it harder to port Arlon to other gitops tools in the future, e.g. Flux (Trilok mentioned this, though it's not a strong concern at this point, given how invested we already are in ArgoCD) Potential solutions to the profiles-are-not-firstclass-objects issue: \u00b6 The Null App \u00b6 The Null App (NA) is an Arlon app (applicationset) that belongs to (is associated with) all profiles. Arlon ensures that the null app always exists and maintains the above invariant. When deployed to a cluster, the NA does not change the cluster state, so it's a no-OP. A possible implementation is to make the NA deploy the \"default\" namespace, which already exists in all (most?) clusters. Arlon CLI commands (and possibly APIs) will filter out the NA and automatically create and update it as necessary, so the user doesn't see it in practice. The NA gets all profile labels, meaning all profiles \"contain\" the null app. A user can now create an empty profile. Internally, it is added to the NA's label list. When a profile is attached to any cluster, that cluster automatically \"gets\" the NA (since it's in all profiles), in addition to any other apps associated with the profile. When an app is \"added\" to a profile, meaning the profile is added to the app's labels list, the profile may not previously exist, therefore the profile is also added to the null app's label list. Therefore, when an app is added to a profile, two apps are modified. When an app is \"removed\" from a profile, meaning the profile is removed from the app's labels list, no change is made to the null app, therefore the profile remains in the null app's label list. (Actually, this behavior must change to support inconsistent states, see \"declarative installation ...\" section below. Lifecycle operations on profiles \u00b6 With the presence of the null app, profiles can appear to be first class objects with defined lifecycle operations. Creating an empty profile: a profile is \"created\" by adding its name to the null app's label list. If it already exists in the null app's list, the app is unmodified. If it already existed in another app's list, then that's fine too. That app is not modified either. At the end of this operation, which is idempotent, the profile is guaranteed to exist in at least one app. Deleting a profile: this deletes the profile from all apps in which it appears in the label list. The operation is idempotent. If the profile did not initially exist, a warning will be printed by no error occurs. Issue: declarative installation and inconsistent states \u00b6 A user may want to provision profiles and applications in a declarative way, meaning with manifests and \"kubectl apply -f\". Those manifests contain applicationsets that satisfy the \"arlon application\" requirement. The user does not know about the null app. Therefore the user's declared applicationsets (with the arlon requirements) will solely completely define the arlon applications and profiles. We assume that the user has no interest in declaratively create empty profiles, only profiles that have at least one associated application. Arlon must allow a partially inconsistent state, meaning, at any point in time, some profiles may not exist in the null app. This is fine, since the null app's only purpose is to maintain the existence of empty profiles. During an inconsistent state, profiles that exist in some apps but not in the null app are, by definition, existent, since they appear in at least one app. However, one enhancement is necessary on the \"remove app from profile\" operation: - In addition to removing the profile from the app's label list, the operation must ensure the existence of the profile in null app, meaning add it if it's not already there. This will ensure that at the end of the operation, the profile still exists in the null app. If it no longer exists anywhere else, then by definition it is empty. Full Custom Resource \u00b6 (Under construction) We could represent Gen2 profiles using a custom resource, either a new type, or by overloading the existing Profile CR already used by Gen1. The downside is an increase in implementation complexity, for e.g * where is the source of truth for app-to-profile associations? * what if an app refers to a profile label value not represented by any Profile CR? A new controller would be most likely need to be developed.","title":"Gen2 Profiles - Proposal 1"},{"location":"gen2_profiles_proposal_1/#gen2-profiles-proposal-1","text":"In this design proposal, gen2 profiles are completely built on native ArgoCD ApplicationSets and resource labels. There are no first-class Arlon objects.","title":"Gen2 Profiles - Proposal 1"},{"location":"gen2_profiles_proposal_1/#object-model","text":"Arlon Application: a thin wrapper around an ApplicationSet. An ApplicationSet is an Arlon Application if it has the managed-by=arlon label. Profile name: any unique label value that appears in the spec.generators[0].clusters.selector.matchExpressions.values[] array of at least one Arlon application. Cluster: any cluster registered in ArgoCD. Not limited to clusters created by Arlon. Observations: - A profile can be associated with any number of applications. And an application can be associated with multiple profiles. - A cluster is said to be associated with a profile if it is labeled with arlon.io/profile=<profileName> . A cluster can be associated with at most one profile. A profile may be associated (attached to) any number of clusters.","title":"Object model"},{"location":"gen2_profiles_proposal_1/#usage","text":"","title":"Usage"},{"location":"gen2_profiles_proposal_1/#managing-applications","text":"arlon app list shows the current list of Arlon applications. It is initially empty. The prototype does not currently support direct Arlon application creation. (This is easy to add later as a new command) An Arlon app has to be created manually by one of these methods: - Create a new ApplicationSet from a YAML file with - The managed-by=arlon label - The spec as follows: spec: generators: - clusters: selector: matchExpressions: - key: arlon.io/profile operator: In values: [] - Modify an existing ApplicationSet with the above requirements","title":"Managing applications"},{"location":"gen2_profiles_proposal_1/#managing-profiles","text":"Profiles are not first class objects. They only exist as labels referenced by applications and placed on clusters. If a particular profile label value is not referenced from any application, it does not exist. arlon ngprofile list shows the current list of profiles, the applications associated with each profile, and the clusters currently using each profile. The list is constructed by scanning all applications and determining the unique set of labels referenced in the matchExpressions.values[] array of each. To create a profile that doesn't exist yet, it needs to be added to at least one application's label set. This is conceptually achieved by \"adding the app to the profile\": arlon app addtoprofile <appName> <profileName> Conversely, a profile label can be removed from an application by \"removing the app from the profile\": arlon app removefromprofile <appname> <profileName> Caution: this can cause the profile to cease to exist if that was the last app referencing it.","title":"Managing profiles"},{"location":"gen2_profiles_proposal_1/#associating-profiles-with-clusters","text":"A cluster can have at most one profile attached to it. To attach a profile to a cluster: arlon nprofile attach <profilename> <clustername> Similarly, to detach: arlon nprofile detach <profilename> <clustername> Internally, an attach operation simply labels the cluster (via ArgoCD API) with the arlon.io/profile=<profileName> key value pair.","title":"Associating profiles with clusters"},{"location":"gen2_profiles_proposal_1/#discussion","text":"Pros of the design: * Lightweight, elegant, simple * Fully declarative (no new resources introduced, relies entirely on existing ArgoCD resources) * Does not require \"workspace git repo\" since a profile has no compiled component. Cons: * Profiles are not first class objects. A profile can cease to exist if it becomes unreferenced from any application. This can be confusing to users. For the same reason, you can't create an empty profile and add apps to it later. This can be alleviated by clearly documenting the fact that profiles are just label values. Once the user understand this, everything will become clearer, and the simplicity of the design can begin to outweigh its quirks. * A cluster can only have one gen2 profile attached to it. This is a result of the limited expressiveness of the matchExpressions logic. In contrast, any number of gen1 profiles can be attached to a cluster (the current implementation only allows one, but could be enhanced to allow many) * It's impossible to specify per-cluster overrides for an application. That's because an ApplicationSet can be deployed to multiple clusters if they have a matching profile label. (To be fair, neither gen1 profiles nor gen2 clusters support cluster overrides either, but for a different reason. This is tracked in a github issue) * Any limitations of ApplicationSets (for e.g. lack of Sync Wave support?) will apply to Arlon Apps using gen2 profiles. * The lightweight nature of this design may cause some to perceive Arlon's contribution to be very minimal (it's a thin wrapper around ArgoCD constructs). * Relies on ApplicationSet, which is ArgoCD specific, making it harder to port Arlon to other gitops tools in the future, e.g. Flux (Trilok mentioned this, though it's not a strong concern at this point, given how invested we already are in ArgoCD)","title":"Discussion"},{"location":"gen2_profiles_proposal_1/#potential-solutions-to-the-profiles-are-not-firstclass-objects-issue","text":"","title":"Potential solutions to the profiles-are-not-firstclass-objects issue:"},{"location":"gen2_profiles_proposal_1/#the-null-app","text":"The Null App (NA) is an Arlon app (applicationset) that belongs to (is associated with) all profiles. Arlon ensures that the null app always exists and maintains the above invariant. When deployed to a cluster, the NA does not change the cluster state, so it's a no-OP. A possible implementation is to make the NA deploy the \"default\" namespace, which already exists in all (most?) clusters. Arlon CLI commands (and possibly APIs) will filter out the NA and automatically create and update it as necessary, so the user doesn't see it in practice. The NA gets all profile labels, meaning all profiles \"contain\" the null app. A user can now create an empty profile. Internally, it is added to the NA's label list. When a profile is attached to any cluster, that cluster automatically \"gets\" the NA (since it's in all profiles), in addition to any other apps associated with the profile. When an app is \"added\" to a profile, meaning the profile is added to the app's labels list, the profile may not previously exist, therefore the profile is also added to the null app's label list. Therefore, when an app is added to a profile, two apps are modified. When an app is \"removed\" from a profile, meaning the profile is removed from the app's labels list, no change is made to the null app, therefore the profile remains in the null app's label list. (Actually, this behavior must change to support inconsistent states, see \"declarative installation ...\" section below.","title":"The Null App"},{"location":"gen2_profiles_proposal_1/#lifecycle-operations-on-profiles","text":"With the presence of the null app, profiles can appear to be first class objects with defined lifecycle operations. Creating an empty profile: a profile is \"created\" by adding its name to the null app's label list. If it already exists in the null app's list, the app is unmodified. If it already existed in another app's list, then that's fine too. That app is not modified either. At the end of this operation, which is idempotent, the profile is guaranteed to exist in at least one app. Deleting a profile: this deletes the profile from all apps in which it appears in the label list. The operation is idempotent. If the profile did not initially exist, a warning will be printed by no error occurs.","title":"Lifecycle operations on profiles"},{"location":"gen2_profiles_proposal_1/#issue-declarative-installation-and-inconsistent-states","text":"A user may want to provision profiles and applications in a declarative way, meaning with manifests and \"kubectl apply -f\". Those manifests contain applicationsets that satisfy the \"arlon application\" requirement. The user does not know about the null app. Therefore the user's declared applicationsets (with the arlon requirements) will solely completely define the arlon applications and profiles. We assume that the user has no interest in declaratively create empty profiles, only profiles that have at least one associated application. Arlon must allow a partially inconsistent state, meaning, at any point in time, some profiles may not exist in the null app. This is fine, since the null app's only purpose is to maintain the existence of empty profiles. During an inconsistent state, profiles that exist in some apps but not in the null app are, by definition, existent, since they appear in at least one app. However, one enhancement is necessary on the \"remove app from profile\" operation: - In addition to removing the profile from the app's label list, the operation must ensure the existence of the profile in null app, meaning add it if it's not already there. This will ensure that at the end of the operation, the profile still exists in the null app. If it no longer exists anywhere else, then by definition it is empty.","title":"Issue: declarative installation and inconsistent states"},{"location":"gen2_profiles_proposal_1/#full-custom-resource","text":"(Under construction) We could represent Gen2 profiles using a custom resource, either a new type, or by overloading the existing Profile CR already used by Gen1. The downside is an increase in implementation complexity, for e.g * where is the source of truth for app-to-profile associations? * what if an app refers to a profile label value not represented by any Profile CR? A new controller would be most likely need to be developed.","title":"Full Custom Resource"},{"location":"gen2_profiles_proposal_2/","text":"Gen2 Profiles - Proposal 2 \u00b6 This is an update to the previous Proposal 1 for Gen2 Profiles design. The main change is the introduction of the new AppProfile custom resource, which elevates profiles to first-class objects. The rest of the design remains mostly unchanged, meaning Arlon apps are still based on ApplicationSets, and a cluster is associated with an AppProfile by labeling it, except the labeling is handled slightly differently (see Labeling Algorithm ). AppProfiles are now the source of truth for profile-to-app mappings. A new controller was introduced to reconcile not only AppProfiles, but clusters and ApplicationSets as well since they are all inter linked. Object model \u00b6 Arlon Application (or App for short): a thin wrapper around an ApplicationSet. An ApplicationSet is an Arlon Application if it has the managed-by=arlon label. App Profile : a uniquely named set of Arlon Applications. It is backed by a new custom resource and CRD. (The resource is named AppProfile to distinguish it from the gen1 Profile resource. Even though gen1 profiles will deprecated and eventually retired, the naming scheme avoids conflicts during the transition). Arlon Cluster : a gen2 cluster created by Arlon. As a reminder, it is represented by between 2 and 3 ArgoCD Application resources: The cluster application (named with the workload cluster name) The arlon application (named by appending the -arlon suffix to the cluster application's name) The optional profile application (named by appending -profile suffix to the cluster application's name) The first application (the cluster application) is treated as the anchor for the entire set. When an Arlon Cluster is associated with an AppProfile, the cluster application will be labeled with the AppProfile's name. An Arlon Cluster that was successfully deployed always has an associated ArgoCD cluster (thanks to the ClusterRegistration mechanism). ArgoCD Cluster : the set of ArgoCD clusters is a superset of Arlon clusters. External Cluster : any ArgoCD cluster that was not created by Arlon. So essentially External Clusters Set = ArgoCD Clusters Set - Arlon Clusters Set . A user may want to associate an external cluster with an app profile. Observations: - An app profile can be associated with (or \"contain\") any number of applications - And an app can be associated with multiple profiles. - A cluster is said to be associated with a profile if it is labeled with arlon.io/profile=<profileName> . - A cluster can be associated with at most one profile. A profile may be associated (attached to) any number of clusters. This table summarizes the actual resources backing the objects: | Object Type | Actual Resource | |--------------------|------------------------| | Arlon Application | ArgoCD ApplicationSet | | AppProfile | AppProfile | | Arlon Cluster | ArgoCD Application | | ArgoCD Cluster | Kubernetes Secret | Labeling Algorithm \u00b6 Just like in proposal 1, associating a cluster with an app profile is done by labeling the cluster with the profile's name, and ensuring that that name is included in the corresponding ApplicationSet's matchExpressions values list. But there are some differences: - For an Arlon cluster, which is anchored by an ArgoCD Application resource, the user should label the Application resource, not the corresponding ArgoCD cluster. The new AppProfile controller will propagate the label to the corresponding ArgoCD cluster. This allows the user to deploy an Arlon cluster, create and populate a profile, and associate the cluster to the profile all in one declarative \"apply\" operation. (A user can't label an ArgoCD cluster that doesn't exist yet) - For non-Arlon clusters, generally referred as \"external\", the design allows those existing ArgoCD clusters to be labeled directly, but this will be managed outside of the AppProfiles controller and essentially the user's responsibility, and has limitations. Controller \u00b6 A new controller was developed to not only reconcile AppProfiles, but also clusters and ApplicationSets (those representing Arlon Applications) since they are now all inter linked through profiles. - The main controller logic resides in pkg/appprofile/reconcile.go and controllers/appprofile_controller.go . - Additionally, logic was added to reconcile ArgoCD applications (representing Arlon clusters) and ArgoCD ApplicationSets (representing Arlon apps) with the relationships defined by AppProfiles: - controllers/application_controller.go - controllers/applicationset_controller.go - The new logic will be eventually merged into the main controller. In the prototype, it is available as a separate CLI command to simplify testing: arlon appprofile-controller . The reconciliation algorithm is complex due to the number of interdependent resources. See Appendix A: Reconciliation Algorithm for details. Usage \u00b6 Since all objects direcly map to Kubernetes resources, the user can in theory manage everything with kubectl [create/apply/edit/delete/lebel] . However, the arlon CLI and API are still useful for some things as explained below. Creating an initial Arlon Application \u00b6 Even though an Arlon app is directly represented by an ApplicationSet, the resource has strict requirements, so it's easiest to create it with the help of the future arlon app create , which can create the resource directly or dump its YAML to standard out. (This command is not yet implemented in the initial Proposal 2 prototype). The requirements on the ApplicationSet are: - Must have the arlon-type=application label. - The spec's generators section must use a single generator with a matchExpressions selector with the arlon.io/profile key and In as operator. (Note: the values field will later be set the AppProfile controller, automatically) - The spec's template section must templatize the generated ArgoCD application names based on the workload cluster name. The user is free to configure the template's spec subsection to target any manifest or Helm chart in git. Example: apiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: labels: arlon-type: application name: guestbook namespace: argocd spec: generators: - clusters: selector: matchExpressions: - key: arlon.io/profile operator: In values: [] template: metadata: name: '{{name}}-appset-guestbook' spec: destination: namespace: default server: '{{server}}' project: default source: path: guestbook repoURL: https://github.com/argoproj/argocd-example-apps/ targetRevision: HEAD syncPolicy: automated: prune: true Managing Application Profiles \u00b6 An AppProfile is a new custom resource with a simple spec: a list of Arlon Application names: apiVersion: core.arlon.io/v1 kind: AppProfile metadata: name: engineering namespace: arlon spec: appNames: - guestbook - wordpress - nginx status: health: degraded invalidAppNames: - wordpress - nginx The resource also has a Status field that the controller updates. The Status.health subfield is either healthy or degraded . Degraded means that one or more specified apps don't exist. The Status.invalidAppNames indicates which app names are invalid, if any. The presence of the Status section is a result of the decision to validate the profile asynchronously and allow invalid app names to be specified. This generally results in a simpler design, and follows the Kubernetes philosophy of allowing a user to specify any spec and report status later, after reconciliation. The alternative would have been to validate the app names synchronously using a webhook. This alternative was not currently chosen due to the complexity of webhook devopment and testing, but may be reconsidered in the future if a good use case arises. Updating a profile's Spec.appNames can have immediate side effects: corresponding ArgoCD Applications can be deployed or destroyed in any clusters labeled with the profile name, as soon as the controller finishes reconciliation. To list profiles: kubectl -n arlon get appprofiles To delete a profile: kubectl -n arlon delete appprofile <name> . As expected, this command can have immediate effects on impacted clusters. Associating profiles with Arlon clusters \u00b6 A cluster can have at most one profile attached to it. The labeling mechanism can be viewed as an internal Arlon implementation detail, and it will be hidden from users with arlon CLI providing commands to attach/detach profiles to/from clusters, but until that's implemented, the user can achieve the same effect by using Kubernetes labeling directly: To attach a profile to an Arlon cluster, simply label the corresponding anchor ArgoCD Application as follows: kubectl -n argocd label application --overwrite <clusterName> arlon.io/profile=<profileName> To detach, remove the label: kubectl -n argocd label application <clusterName> arlon.io/profile- In addition to those raw kubectl commands, we propose to add arlon CLI commands and APIs to slightly simplify the task and abstract out the labeling. Associating profiles with external clusters (experimental, not tested yet) \u00b6 An external cluster does not have an Arlon representation (meaning there is no \"anchor\" ArgoCD Application with the same name). It can still be associated with an AppProfile by labeling the raw ArgoCD cluster: - Unfortunately as of ArgoCD 2.4, there is no argocd CLI command to label a cluster - A user can still manually label the secret resource where ArgoCD stores the cluster's metadata and credentials. This requires some work, as those secrets are named using an non-obvious convention. - The best thing Arlon should do is to provide dedicated CLI commands and APIs to simplify this task. Fully declarative initialization \u00b6 Now that all Arlon concepts are represented by declarative resources, it is now possible to provision a complete set of resources with a single kubectl apply -f on a file or folder of manifests. In particular, we can now develop a fully self-contained demo folder that contains all of these resources containing references to each other: - App Profiles - Apps - Arlon Clusters The user can then kubectl apply -f the whole folder and observe the automatic creation of the cluster(s), profiles, and apps. (Note: the workload clusters need a cluster template in git. This can be supplied by the Arlon repo itself. This does point to an issue that may become a problem later on: cluster templates don't have a representation today. There is no registration mechanism for them. The user is expected to \"know\" where his/her cluster templates reside, and is responsible for specifying their git location when creating new workload clusters) Application Overrides \u00b6 Given that an Arlon Application is just a specialized ApplicationSet, it inherits an ApplicationSet's ability to specify a full ArgoCD Application spec, and this spec can contain overrides for Helm charts. - This does mean that every different permutation of override values requires a new ApplicationSet, and therefore Arlon Application. - This may be reasonable if we accept that an Arlon Application is not a true application, but rather, an intermediate object that points to the true application source residing in Git. The intermediate object can be viewed as a customization or specialization of the application source, so it's ok to have multiple intermediate objects, each holding a different set of customizations. Issues & Limitations \u00b6 The remaining issues & limitations raised in Proposal 1 still apply: - Only one profile per cluster - Inherits any limitations of ApplicationSets - Does not support ApplicationSets with other types of generators - Makes Arlon even more dependent on ArgoCD technology - There is no way to create a per-cluster application override, unless the user creates a unique and dedicated Arlon Application for the cluster, places it in a dedicated profile, and applies the profile to the cluster. Appendix A: Reconciliation Algorithm \u00b6 The pseudocode looks something like:","title":"Gen2 Profiles - Proposal 2"},{"location":"gen2_profiles_proposal_2/#gen2-profiles-proposal-2","text":"This is an update to the previous Proposal 1 for Gen2 Profiles design. The main change is the introduction of the new AppProfile custom resource, which elevates profiles to first-class objects. The rest of the design remains mostly unchanged, meaning Arlon apps are still based on ApplicationSets, and a cluster is associated with an AppProfile by labeling it, except the labeling is handled slightly differently (see Labeling Algorithm ). AppProfiles are now the source of truth for profile-to-app mappings. A new controller was introduced to reconcile not only AppProfiles, but clusters and ApplicationSets as well since they are all inter linked.","title":"Gen2 Profiles - Proposal 2"},{"location":"gen2_profiles_proposal_2/#object-model","text":"Arlon Application (or App for short): a thin wrapper around an ApplicationSet. An ApplicationSet is an Arlon Application if it has the managed-by=arlon label. App Profile : a uniquely named set of Arlon Applications. It is backed by a new custom resource and CRD. (The resource is named AppProfile to distinguish it from the gen1 Profile resource. Even though gen1 profiles will deprecated and eventually retired, the naming scheme avoids conflicts during the transition). Arlon Cluster : a gen2 cluster created by Arlon. As a reminder, it is represented by between 2 and 3 ArgoCD Application resources: The cluster application (named with the workload cluster name) The arlon application (named by appending the -arlon suffix to the cluster application's name) The optional profile application (named by appending -profile suffix to the cluster application's name) The first application (the cluster application) is treated as the anchor for the entire set. When an Arlon Cluster is associated with an AppProfile, the cluster application will be labeled with the AppProfile's name. An Arlon Cluster that was successfully deployed always has an associated ArgoCD cluster (thanks to the ClusterRegistration mechanism). ArgoCD Cluster : the set of ArgoCD clusters is a superset of Arlon clusters. External Cluster : any ArgoCD cluster that was not created by Arlon. So essentially External Clusters Set = ArgoCD Clusters Set - Arlon Clusters Set . A user may want to associate an external cluster with an app profile. Observations: - An app profile can be associated with (or \"contain\") any number of applications - And an app can be associated with multiple profiles. - A cluster is said to be associated with a profile if it is labeled with arlon.io/profile=<profileName> . - A cluster can be associated with at most one profile. A profile may be associated (attached to) any number of clusters. This table summarizes the actual resources backing the objects: | Object Type | Actual Resource | |--------------------|------------------------| | Arlon Application | ArgoCD ApplicationSet | | AppProfile | AppProfile | | Arlon Cluster | ArgoCD Application | | ArgoCD Cluster | Kubernetes Secret |","title":"Object model"},{"location":"gen2_profiles_proposal_2/#labeling-algorithm","text":"Just like in proposal 1, associating a cluster with an app profile is done by labeling the cluster with the profile's name, and ensuring that that name is included in the corresponding ApplicationSet's matchExpressions values list. But there are some differences: - For an Arlon cluster, which is anchored by an ArgoCD Application resource, the user should label the Application resource, not the corresponding ArgoCD cluster. The new AppProfile controller will propagate the label to the corresponding ArgoCD cluster. This allows the user to deploy an Arlon cluster, create and populate a profile, and associate the cluster to the profile all in one declarative \"apply\" operation. (A user can't label an ArgoCD cluster that doesn't exist yet) - For non-Arlon clusters, generally referred as \"external\", the design allows those existing ArgoCD clusters to be labeled directly, but this will be managed outside of the AppProfiles controller and essentially the user's responsibility, and has limitations.","title":"Labeling Algorithm"},{"location":"gen2_profiles_proposal_2/#controller","text":"A new controller was developed to not only reconcile AppProfiles, but also clusters and ApplicationSets (those representing Arlon Applications) since they are now all inter linked through profiles. - The main controller logic resides in pkg/appprofile/reconcile.go and controllers/appprofile_controller.go . - Additionally, logic was added to reconcile ArgoCD applications (representing Arlon clusters) and ArgoCD ApplicationSets (representing Arlon apps) with the relationships defined by AppProfiles: - controllers/application_controller.go - controllers/applicationset_controller.go - The new logic will be eventually merged into the main controller. In the prototype, it is available as a separate CLI command to simplify testing: arlon appprofile-controller . The reconciliation algorithm is complex due to the number of interdependent resources. See Appendix A: Reconciliation Algorithm for details.","title":"Controller"},{"location":"gen2_profiles_proposal_2/#usage","text":"Since all objects direcly map to Kubernetes resources, the user can in theory manage everything with kubectl [create/apply/edit/delete/lebel] . However, the arlon CLI and API are still useful for some things as explained below.","title":"Usage"},{"location":"gen2_profiles_proposal_2/#creating-an-initial-arlon-application","text":"Even though an Arlon app is directly represented by an ApplicationSet, the resource has strict requirements, so it's easiest to create it with the help of the future arlon app create , which can create the resource directly or dump its YAML to standard out. (This command is not yet implemented in the initial Proposal 2 prototype). The requirements on the ApplicationSet are: - Must have the arlon-type=application label. - The spec's generators section must use a single generator with a matchExpressions selector with the arlon.io/profile key and In as operator. (Note: the values field will later be set the AppProfile controller, automatically) - The spec's template section must templatize the generated ArgoCD application names based on the workload cluster name. The user is free to configure the template's spec subsection to target any manifest or Helm chart in git. Example: apiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: labels: arlon-type: application name: guestbook namespace: argocd spec: generators: - clusters: selector: matchExpressions: - key: arlon.io/profile operator: In values: [] template: metadata: name: '{{name}}-appset-guestbook' spec: destination: namespace: default server: '{{server}}' project: default source: path: guestbook repoURL: https://github.com/argoproj/argocd-example-apps/ targetRevision: HEAD syncPolicy: automated: prune: true","title":"Creating an initial Arlon Application"},{"location":"gen2_profiles_proposal_2/#managing-application-profiles","text":"An AppProfile is a new custom resource with a simple spec: a list of Arlon Application names: apiVersion: core.arlon.io/v1 kind: AppProfile metadata: name: engineering namespace: arlon spec: appNames: - guestbook - wordpress - nginx status: health: degraded invalidAppNames: - wordpress - nginx The resource also has a Status field that the controller updates. The Status.health subfield is either healthy or degraded . Degraded means that one or more specified apps don't exist. The Status.invalidAppNames indicates which app names are invalid, if any. The presence of the Status section is a result of the decision to validate the profile asynchronously and allow invalid app names to be specified. This generally results in a simpler design, and follows the Kubernetes philosophy of allowing a user to specify any spec and report status later, after reconciliation. The alternative would have been to validate the app names synchronously using a webhook. This alternative was not currently chosen due to the complexity of webhook devopment and testing, but may be reconsidered in the future if a good use case arises. Updating a profile's Spec.appNames can have immediate side effects: corresponding ArgoCD Applications can be deployed or destroyed in any clusters labeled with the profile name, as soon as the controller finishes reconciliation. To list profiles: kubectl -n arlon get appprofiles To delete a profile: kubectl -n arlon delete appprofile <name> . As expected, this command can have immediate effects on impacted clusters.","title":"Managing Application Profiles"},{"location":"gen2_profiles_proposal_2/#associating-profiles-with-arlon-clusters","text":"A cluster can have at most one profile attached to it. The labeling mechanism can be viewed as an internal Arlon implementation detail, and it will be hidden from users with arlon CLI providing commands to attach/detach profiles to/from clusters, but until that's implemented, the user can achieve the same effect by using Kubernetes labeling directly: To attach a profile to an Arlon cluster, simply label the corresponding anchor ArgoCD Application as follows: kubectl -n argocd label application --overwrite <clusterName> arlon.io/profile=<profileName> To detach, remove the label: kubectl -n argocd label application <clusterName> arlon.io/profile- In addition to those raw kubectl commands, we propose to add arlon CLI commands and APIs to slightly simplify the task and abstract out the labeling.","title":"Associating profiles with Arlon clusters"},{"location":"gen2_profiles_proposal_2/#associating-profiles-with-external-clusters-experimental-not-tested-yet","text":"An external cluster does not have an Arlon representation (meaning there is no \"anchor\" ArgoCD Application with the same name). It can still be associated with an AppProfile by labeling the raw ArgoCD cluster: - Unfortunately as of ArgoCD 2.4, there is no argocd CLI command to label a cluster - A user can still manually label the secret resource where ArgoCD stores the cluster's metadata and credentials. This requires some work, as those secrets are named using an non-obvious convention. - The best thing Arlon should do is to provide dedicated CLI commands and APIs to simplify this task.","title":"Associating profiles with external clusters (experimental, not tested yet)"},{"location":"gen2_profiles_proposal_2/#fully-declarative-initialization","text":"Now that all Arlon concepts are represented by declarative resources, it is now possible to provision a complete set of resources with a single kubectl apply -f on a file or folder of manifests. In particular, we can now develop a fully self-contained demo folder that contains all of these resources containing references to each other: - App Profiles - Apps - Arlon Clusters The user can then kubectl apply -f the whole folder and observe the automatic creation of the cluster(s), profiles, and apps. (Note: the workload clusters need a cluster template in git. This can be supplied by the Arlon repo itself. This does point to an issue that may become a problem later on: cluster templates don't have a representation today. There is no registration mechanism for them. The user is expected to \"know\" where his/her cluster templates reside, and is responsible for specifying their git location when creating new workload clusters)","title":"Fully declarative initialization"},{"location":"gen2_profiles_proposal_2/#application-overrides","text":"Given that an Arlon Application is just a specialized ApplicationSet, it inherits an ApplicationSet's ability to specify a full ArgoCD Application spec, and this spec can contain overrides for Helm charts. - This does mean that every different permutation of override values requires a new ApplicationSet, and therefore Arlon Application. - This may be reasonable if we accept that an Arlon Application is not a true application, but rather, an intermediate object that points to the true application source residing in Git. The intermediate object can be viewed as a customization or specialization of the application source, so it's ok to have multiple intermediate objects, each holding a different set of customizations.","title":"Application Overrides"},{"location":"gen2_profiles_proposal_2/#issues-limitations","text":"The remaining issues & limitations raised in Proposal 1 still apply: - Only one profile per cluster - Inherits any limitations of ApplicationSets - Does not support ApplicationSets with other types of generators - Makes Arlon even more dependent on ArgoCD technology - There is no way to create a per-cluster application override, unless the user creates a unique and dedicated Arlon Application for the cluster, places it in a dedicated profile, and applies the profile to the cluster.","title":"Issues &amp; Limitations"},{"location":"gen2_profiles_proposal_2/#appendix-a-reconciliation-algorithm","text":"The pseudocode looks something like:","title":"Appendix A: Reconciliation Algorithm"},{"location":"gen2_profiles_proposal_3/","text":"Gen2 Profiles - Proposal 3 \u00b6 This is an update to Proposal 2 for Gen2 Profiles design to add the ability to associate (attach) multiple AppProfiles to a cluster. Summary of changes \u00b6 The arlon.io/profiles annotation replaces the arlon.io/profile label in an Arlon cluster (represented by an ArgoCD Application resource with label arlon-type=cluster-app ). The annotation stores a comma separated list of AppProfile names. An annotation was chosen instead of a label because Kubernetes label values cannot contain the comma ( , ) character. Arlon applications are still implemented as ArgoCD ApplicationSets. But the List generator replaces the Clusters generator used in Proposal 2. The List generator allows the Arlon controller to generate the precise list of clusters associated with an Arlon Application, which is computed from the profile-to-application and cluster-to-profile mappings at any given time. Every generated Application resource receives these two template parameters: cluster_name : The cluster's name cluster_server : The URL of the cluster's API server. The initial manifest generated by arlon app create (see below) is configured to set any generated Application resource's name to <cluster_name>-app-<app_name> . The user is free to change it by editing the ApplicationSet resource directly New CLI commands and features arlon app create appName repoUrl repoPath [flags] creates an ApplicationSet manifest that satisfies the requirements to serve as an initial Arlon Application. In particular, it includes a List generator with an Elements list of zero items. The Arlon AppProfile controller will update the list dynamically as needed. The manifest is applied to the management cluster immediately unless the user specifies the --output-yaml option, in which case the manifest can be saved, edited, and applied later. arlon cluster list now displays the app profile list in the APP_PROFILES column. arlon cluster setappprofiles <clusterName> <commaSeparatedAppProfileNames allows a user to set the list of app profiles associated with a cluster. Setting the list to an empty string removes all app profiles from the cluster. No CRUD operations are provided for Arlon AppProfile resources since they are simple resources that are best manipulated and applied as manifests with kubectl . The one exception is arlon appprofile list , which is a convenience and shows the list of apps for each AppProfile, and the Status.healthy field. Testing \u00b6 The reconciliation algorithm is tested by a comprensive unit test located at pkg/appprofile/appprofile_test.go","title":"Gen2 Profiles - Proposal 3"},{"location":"gen2_profiles_proposal_3/#gen2-profiles-proposal-3","text":"This is an update to Proposal 2 for Gen2 Profiles design to add the ability to associate (attach) multiple AppProfiles to a cluster.","title":"Gen2 Profiles - Proposal 3"},{"location":"gen2_profiles_proposal_3/#summary-of-changes","text":"The arlon.io/profiles annotation replaces the arlon.io/profile label in an Arlon cluster (represented by an ArgoCD Application resource with label arlon-type=cluster-app ). The annotation stores a comma separated list of AppProfile names. An annotation was chosen instead of a label because Kubernetes label values cannot contain the comma ( , ) character. Arlon applications are still implemented as ArgoCD ApplicationSets. But the List generator replaces the Clusters generator used in Proposal 2. The List generator allows the Arlon controller to generate the precise list of clusters associated with an Arlon Application, which is computed from the profile-to-application and cluster-to-profile mappings at any given time. Every generated Application resource receives these two template parameters: cluster_name : The cluster's name cluster_server : The URL of the cluster's API server. The initial manifest generated by arlon app create (see below) is configured to set any generated Application resource's name to <cluster_name>-app-<app_name> . The user is free to change it by editing the ApplicationSet resource directly New CLI commands and features arlon app create appName repoUrl repoPath [flags] creates an ApplicationSet manifest that satisfies the requirements to serve as an initial Arlon Application. In particular, it includes a List generator with an Elements list of zero items. The Arlon AppProfile controller will update the list dynamically as needed. The manifest is applied to the management cluster immediately unless the user specifies the --output-yaml option, in which case the manifest can be saved, edited, and applied later. arlon cluster list now displays the app profile list in the APP_PROFILES column. arlon cluster setappprofiles <clusterName> <commaSeparatedAppProfileNames allows a user to set the list of app profiles associated with a cluster. Setting the list to an empty string removes all app profiles from the cluster. No CRUD operations are provided for Arlon AppProfile resources since they are simple resources that are best manipulated and applied as manifests with kubectl . The one exception is arlon appprofile list , which is a convenience and shows the list of apps for each AppProfile, and the Status.healthy field.","title":"Summary of changes"},{"location":"gen2_profiles_proposal_3/#testing","text":"The reconciliation algorithm is tested by a comprensive unit test located at pkg/appprofile/appprofile_test.go","title":"Testing"},{"location":"installation/","text":"Installation \u00b6 Arlon CLI downloads are provided on GitHub. The CLI is not a self-contained standalone executable though. It is required to point the CLI to a management cluster and set up the Arlon controller in this management cluster. For a quickstart minimal demonstration setup, follow the instructions to set up a KIND based testbed with Arlon and ArgoCD running here . Please follow the manual instructions in this section for a customised setup or refer the instructions for automated installation here . Customised Setup \u00b6 Management cluster \u00b6 You can use any Kubernetes cluster that you have admin access to. Ensure: kubectl is in your path KUBECONFIG is pointing to the right file and the context set properly ArgoCD \u00b6 Follow steps 1-4 of the ArgoCD installation guide to install ArgoCD onto your management cluster. After this step, you should be logged in as admin and a config file was created at ${HOME}/.config/argocd/config Create your workspace repository in your git provider if necessary, then register it. Example: argocd repo add https://github.com/myname/arlon_workspace --username myname --password secret . -- Note: type argocd repo add --help to see all available options. -- For Arlon developers, this is not your fork of the Arlon source code repository, but a separate git repo where some artifacts like profiles created by Arlon will be stored. Highly recommended: configure a webhook to immediately notify ArgoCD of changes to the repo. This will be especially useful during the tutorial. Without a webhook, repo changes may take up to 3 minutes to be detected, delaying cluster configuration updates. Create a local user named arlon with the apiKey capability. This involves editing the argocd-cm ConfigMap using kubectl . Adjust the RBAC settings to grant admin permissions to the arlon user. This involves editing the argocd-rbac-cm ConfigMap to add the entry g, arlon, role:admin under the policy.csv section. Example: apiVersion : v1 data : policy.csv : | g, arlon, role:admin kind : ConfigMap [ ... ] Generate an account token: argocd account generate-token --account arlon Make a temporary copy of this config-file in /tmp/config then edit it to replace the value of auth-token with the token from the previous step. Save changes. This file will be used to configure the Arlon controller's ArgoCD credentials during the next steps. Arlon controller \u00b6 Create the arlon namespace: kubectl create ns arlon Create the ArgoCD credentials secret from the temporary config file: kubectl -n arlon create secret generic argocd-creds --from-file /tmp/config Delete the temporary config file Clone the arlon git repo and cd to its top directory Create the CRDs: kubectl apply -f config/crd/bases/ Deploy the controller: kubectl apply -f deploy/manifests/ Ensure the controller eventually enters the Running state: watch kubectl -n arlon get pod Arlon CLI \u00b6 Download the CLI for the latest release from GitHub. Currently, Linux and MacOS operating systems are supported. Uncompress the tarball, rename it as arlon and add to your PATH Run arlon verify to check for prerequisites. Run arlon install to install any missing prerequisites. The following instructions are to manually build CLI from this code repository. Building the CLI \u00b6 Clone this repository and pull the latest version of a branch (main by default) From the top directory, run make build Optionally create a symlink from a directory (e.g. /usr/local/bin ) included in your ${PATH} to the bin/arlon binary to make it easy to invoke the command. Cluster orchestration API providers \u00b6 Arlon currently supports Cluster API on AWS cloud. It also has experimental support for Crossplane on AWS. Cluster API \u00b6 Using the Cluster API Quickstart Guide as reference, complete these steps: Install clusterctl Initialize the management cluster. In particular, follow instructions for your specific cloud provider (AWS in this example) Ensure clusterctl init completes successfully and produces the expected output. Crossplane (experimental) \u00b6 Using the Upbound AWS Reference Platform Quickstart Guide as reference, complete these steps: Install UXP on your management cluster Install Crossplane kubectl extension Install the platform configuration Configure the cloud provider credentials You do not need to go any further, but you're welcome to try the Network Fabric example. FYI: we noticed the dozens/hundreds of CRDs that Crossplane installs in the management cluster can noticeably slow down kubectl, and you may see a warning that looks like : I0222 17 :31:14.112689 27922 request.go:668 ] Waited for 1 .046146023s due to client-side throttling, not priority and fairness, request: GET:https://AA61XXXXXXXXXXX.gr7.us-west-2.eks.amazonaws.com/apis/servicediscovery.aws.crossplane.io/v1alpha1?timeout = 32s Automatic Setup \u00b6 Arlon CLI provides an init command to install \"itself\" on a management cluster. This command performs a basic setup of argocd (if needed) and arlon controller. If argocd is already installed, it assumes that admin password is the same as in argocd-initial-admin-secret ConfigMap and that argocd resides in the argocd namespace. Similar assumptions are made for detecting Arlon installation as well: assuming that the existence of arlon namespace means Arlon controller exists. To install Arlon controller using the init command these pre-requisites need to be met: A valid kubeconfig pointing to the management cluster. A hosted Git repository with at least a README file present. Pre-requisites for supported CAPI infrastructure providers(AWS and Docker as of now). To start the installation process, simply run arlon init -e --username <GIT_USER> --repoURL <WORKSPACE_URL> --password <GIT_PASSWORD> --examples -y . This installs the controller, argocd(if not already present) -e flag adds cluster template manifests to the for using the given credentials. To not add examples, just remove the -e flag. The -y flag refers to silent installation, which is useful for scripts. For an interactive installation, exclude the -y or --no-confirm flag.","title":"Installation"},{"location":"installation/#installation","text":"Arlon CLI downloads are provided on GitHub. The CLI is not a self-contained standalone executable though. It is required to point the CLI to a management cluster and set up the Arlon controller in this management cluster. For a quickstart minimal demonstration setup, follow the instructions to set up a KIND based testbed with Arlon and ArgoCD running here . Please follow the manual instructions in this section for a customised setup or refer the instructions for automated installation here .","title":"Installation"},{"location":"installation/#customised-setup","text":"","title":"Customised Setup"},{"location":"installation/#management-cluster","text":"You can use any Kubernetes cluster that you have admin access to. Ensure: kubectl is in your path KUBECONFIG is pointing to the right file and the context set properly","title":"Management cluster"},{"location":"installation/#argocd","text":"Follow steps 1-4 of the ArgoCD installation guide to install ArgoCD onto your management cluster. After this step, you should be logged in as admin and a config file was created at ${HOME}/.config/argocd/config Create your workspace repository in your git provider if necessary, then register it. Example: argocd repo add https://github.com/myname/arlon_workspace --username myname --password secret . -- Note: type argocd repo add --help to see all available options. -- For Arlon developers, this is not your fork of the Arlon source code repository, but a separate git repo where some artifacts like profiles created by Arlon will be stored. Highly recommended: configure a webhook to immediately notify ArgoCD of changes to the repo. This will be especially useful during the tutorial. Without a webhook, repo changes may take up to 3 minutes to be detected, delaying cluster configuration updates. Create a local user named arlon with the apiKey capability. This involves editing the argocd-cm ConfigMap using kubectl . Adjust the RBAC settings to grant admin permissions to the arlon user. This involves editing the argocd-rbac-cm ConfigMap to add the entry g, arlon, role:admin under the policy.csv section. Example: apiVersion : v1 data : policy.csv : | g, arlon, role:admin kind : ConfigMap [ ... ] Generate an account token: argocd account generate-token --account arlon Make a temporary copy of this config-file in /tmp/config then edit it to replace the value of auth-token with the token from the previous step. Save changes. This file will be used to configure the Arlon controller's ArgoCD credentials during the next steps.","title":"ArgoCD"},{"location":"installation/#arlon-controller","text":"Create the arlon namespace: kubectl create ns arlon Create the ArgoCD credentials secret from the temporary config file: kubectl -n arlon create secret generic argocd-creds --from-file /tmp/config Delete the temporary config file Clone the arlon git repo and cd to its top directory Create the CRDs: kubectl apply -f config/crd/bases/ Deploy the controller: kubectl apply -f deploy/manifests/ Ensure the controller eventually enters the Running state: watch kubectl -n arlon get pod","title":"Arlon controller"},{"location":"installation/#arlon-cli","text":"Download the CLI for the latest release from GitHub. Currently, Linux and MacOS operating systems are supported. Uncompress the tarball, rename it as arlon and add to your PATH Run arlon verify to check for prerequisites. Run arlon install to install any missing prerequisites. The following instructions are to manually build CLI from this code repository.","title":"Arlon CLI"},{"location":"installation/#building-the-cli","text":"Clone this repository and pull the latest version of a branch (main by default) From the top directory, run make build Optionally create a symlink from a directory (e.g. /usr/local/bin ) included in your ${PATH} to the bin/arlon binary to make it easy to invoke the command.","title":"Building the CLI"},{"location":"installation/#cluster-orchestration-api-providers","text":"Arlon currently supports Cluster API on AWS cloud. It also has experimental support for Crossplane on AWS.","title":"Cluster orchestration API providers"},{"location":"installation/#cluster-api","text":"Using the Cluster API Quickstart Guide as reference, complete these steps: Install clusterctl Initialize the management cluster. In particular, follow instructions for your specific cloud provider (AWS in this example) Ensure clusterctl init completes successfully and produces the expected output.","title":"Cluster API"},{"location":"installation/#crossplane-experimental","text":"Using the Upbound AWS Reference Platform Quickstart Guide as reference, complete these steps: Install UXP on your management cluster Install Crossplane kubectl extension Install the platform configuration Configure the cloud provider credentials You do not need to go any further, but you're welcome to try the Network Fabric example. FYI: we noticed the dozens/hundreds of CRDs that Crossplane installs in the management cluster can noticeably slow down kubectl, and you may see a warning that looks like : I0222 17 :31:14.112689 27922 request.go:668 ] Waited for 1 .046146023s due to client-side throttling, not priority and fairness, request: GET:https://AA61XXXXXXXXXXX.gr7.us-west-2.eks.amazonaws.com/apis/servicediscovery.aws.crossplane.io/v1alpha1?timeout = 32s","title":"Crossplane (experimental)"},{"location":"installation/#automatic-setup","text":"Arlon CLI provides an init command to install \"itself\" on a management cluster. This command performs a basic setup of argocd (if needed) and arlon controller. If argocd is already installed, it assumes that admin password is the same as in argocd-initial-admin-secret ConfigMap and that argocd resides in the argocd namespace. Similar assumptions are made for detecting Arlon installation as well: assuming that the existence of arlon namespace means Arlon controller exists. To install Arlon controller using the init command these pre-requisites need to be met: A valid kubeconfig pointing to the management cluster. A hosted Git repository with at least a README file present. Pre-requisites for supported CAPI infrastructure providers(AWS and Docker as of now). To start the installation process, simply run arlon init -e --username <GIT_USER> --repoURL <WORKSPACE_URL> --password <GIT_PASSWORD> --examples -y . This installs the controller, argocd(if not already present) -e flag adds cluster template manifests to the for using the given credentials. To not add examples, just remove the -e flag. The -y flag refers to silent installation, which is useful for scripts. For an interactive installation, exclude the -y or --no-confirm flag.","title":"Automatic Setup"},{"location":"tutorial/","text":"Tutorial (gen1) \u00b6 This assumes that you plan to deploy workload clusters on AWS cloud, with Cluster API (\"CAPI\") as the cluster orchestration API provider. Also ensure you have set up a workspace repository , and it is registered as a git repo in ArgoCD. The tutorial will assume the existence of these environment variables: ${ARLON_REPO} : where the arlon repo is locally checked out ${WORKSPACE_REPO} : where the workspace repo is locally checked out ${WORKSPACE_REPO_URL} : the workspace repo's git URL. It typically looks like https://github.com/${username}/${reponame}.git ${CLOUD_REGION} : the region where you want to deploy example clusters and workloads (e.g. us-west-2) ${SSH_KEY_NAME} : the name of a public ssh key name registered in your cloud account, to enable ssh to your cluster nodes Additionally, for examples assuming arlon git register , \"default\" and a \"prod\" git repo aliases will also be given. _Note: for the best experience, make sure your workspace repo is configured to send change notifications to ArgoCD via a webhook. See the Installation section for details. Cluster specs \u00b6 We first create a few cluster specs with different combinations of API providers and cluster types (kubeadm vs EKS). One of the cluster specs is for an unconfigured API provider (Crossplane); this is for illustrative purposes, since we will not use it in this tutorial. arlon clusterspec create capi-kubeadm-3node --api capi --cloud aws --type kubeadm --kubeversion v1.21.10 --nodecount 3 --nodetype t2.medium --tags devel,test --desc \"3 node kubeadm for dev/test\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } arlon clusterspec create capi-eks --api capi --cloud aws --type eks --kubeversion v1.21.10 --nodecount 2 --nodetype t2.large --tags staging --desc \"2 node eks for general purpose\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } arlon clusterspec create xplane-eks-3node --api xplane --cloud aws --type eks --kubeversion v1.21.10 --nodecount 4 --nodetype t2.small --tags experimental --desc \"4 node eks managed by crossplane\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } Ensure you can now list the cluster specs: $ arlon clusterspec list NAME APIPROV CLOUDPROV TYPE KUBEVERSION NODETYPE NODECNT MSTNODECNT SSHKEY CAS CASMIN CASMAX TAGS DESCRIPTION capi-eks capi aws eks v1.21.10 t2.large 2 3 leb false 1 9 staging 2 node eks for general purpose capi-kubeadm-3node capi aws kubeadm v1.21.10 t2.medium 3 3 leb false 1 9 devel,test 3 node kubeadm for dev/test xplane-eks-3node xplane aws eks v1.21.10 t2.small 4 3 leb false 1 9 experimental 4 node eks managed by crossplane Bundles \u00b6 First create a static bundle containing raw YAML for the guestbook sample application from this example file: cd ${ ARLON_REPO } arlon bundle create guestbook-static --tags applications --desc \"guestbook app\" --from-file examples/bundles/guestbook.yaml ( Note: the YAML is simply a concatenation of the files found in the ArgoCD Example Apps repo ) To illustrate the difference between static and dynamic bundles, we create a dynamic version of the same application, this time using a reference to a git directory containing the YAML. We could point it directly to the copy in the ArgoCD Example Apps repo , but we'll want to make modifications to it, so we instead create a new directory to host our own copy in our workspace directory: cd ${ WORKSPACE_REPO } mkdir -p bundles/guestbook cp ${ ARLON_REPO } /examples/bundles/guestbook.yaml bundles/guestbook git add bundles/guestbook git commit -m \"add guestbook\" git push origin main arlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-url ${ WORKSPACE_REPO_URL } --repo-path bundles/guestbook # OR # using repository aliases # using the default alias arlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-path bundles/guestbook # using the prod alias arlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-path bundles/guestbook --repo-alias prod Next, we create a static bundle for another \"dummy\" application, an Ubuntu pod (OS version: \"Xenial\") that does nothing but print the date-time in an infinite sleep loop: cd ${ ARLON_REPO } arlon bundle create xenial-static --tags applications --desc \"xenial pod\" --from-file examples/bundles/xenial.yaml Finally, we create a bundle for the Calico CNI, which provides pod networking. Some types of clusters (e.g. kubeadm) require a CNI provider to be installed onto a newly created cluster, so encapsulating the provider as a bundle will give us a flexible way to install it. We download a known copy from the authoritative source and store it the workspace repo in order to create a dynamic bundle from it: cd ${ WORKSPACE_REPO } mkdir -p bundles/calico curl https://docs.projectcalico.org/v3.21/manifests/calico.yaml -o bundles/calico/calico.yaml git add bundles/calico git commit -m \"add calico\" git push origin main arlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-url ${ WORKSPACE_REPO_URL } --repo-path bundles/calico # OR # using repository aliases # using the default alias arlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-path bundles/calico # using the prod alias arlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-path bundles/calico --repo-alias prod List your bundles to verify they were correctly entered: $ arlon bundle list NAME TYPE TAGS REPO-URL REPO-PATH DESCRIPTION calico dynamic networking,cni ${ WORKSPACE_REPO_URL } bundles/calico Calico CNI guestbook-dynamic dynamic applications ${ WORKSPACE_REPO_URL } bundles/guestbook guestbook app ( dynamic ) guestbook-static static applications ( N/A ) ( N/A ) guestbook app xenial-static static applications ( N/A ) ( N/A ) ubuntu pod in infinite sleep loop Profiles \u00b6 We can now create profiles to group bundles into useful, deployable sets. First, create a static profile containing bundles xenial-static and guestbook-static: arlon profile create static-1 --static --bundles guestbook-static,xenial-static --desc \"static profile 1\" --tags examples Secondly, create a dynamic version of the same profile. We'll store the compiled form of the profile in the profiles/dynamic-1 directory of the workspace repo. We don't create it manually; instead, the arlon CLI will create it for us, and it will push the change to git: arlon profile create dynamic-1 --repo-url ${ WORKSPACE_REPO_URL } --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples # OR # using repository aliases # using the default alias arlon profile create dynamic-1 --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples # using the prod alias arlon profile create dynamic-1 --repo-alias prod --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples Note: the --repo-base-path profiles option tells arlon to create the profile under a base directory profiles/ (to be created if it doesn't exist). That is in fact the default value of that option, so it is not necessary to specify it in this case. To verify that the compiled profile was created correctly: $ cd ${ WORKSPACE_REPO } $ git pull $ tree profiles profiles \u251c\u2500\u2500 dynamic-1 \u2502 \u251c\u2500\u2500 mgmt \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u251c\u2500\u2500 guestbook-dynamic.yaml \u2502 \u2502 \u251c\u2500\u2500 placeholder_configmap.yaml \u2502 \u2502 \u2514\u2500\u2500 xenial.yaml \u2502 \u2514\u2500\u2500 workload \u2502 \u2514\u2500\u2500 xenial \u2502 \u2514\u2500\u2500 xenial.yaml [ ... ] Since xenial is a static bundle, a copy of its YAML was stored in workload/xenial/xenial.yaml . This is not done for guestbook-dynamic because it is dynamic. Finally, we create another variant of the same profile, with the only difference being the addition of Calico bundle. It'll be used on clusters that need a CNI provider: arlon profile create dynamic-2-calico --repo-url ${ WORKSPACE_REPO_URL } --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples # OR # using repository aliases # using the default alias arlon profile create dynamic-2-calico --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples # using the prod alias arlon profile create dynamic-2-calico --repo-alias prod --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples Listing the profiles should show: $ arlon profile list NAME TYPE BUNDLES REPO-URL REPO-PATH TAGS DESCRIPTION dynamic-1 dynamic guestbook-static,xenial-static ${ WORKSPACE_REPO_URL } profiles/dynamic-1 examples dynamic test 1 dynamic-2-calico dynamic calico,guestbook-static,xenial-static ${ WORKSPACE_REPO_URL } profiles/dynamic-2-calico examples dynamic test 1 static-1 static guestbook-dynamic,xenial-static ( N/A ) ( N/A ) examples static profile 1 Clusters (gen1) \u00b6 We are now ready to deploy our first cluster. It will be of type EKS. Since EKS clusters come configured with pod networking out of the box, we choose a profile that does not include Calico: dynamic-1 . When deploying a cluster, arlon creates in git a Helm chart containing the manifests for creating and bootstrapping the cluster. Arlon then creates an ArgoCD App referencing the chart, thereby relying on ArgoCD to orchestrate the whole process of deploying and configuring the cluster. The arlon deploy command accepts a git URL and path for this git location. Any git repo can be used (so long as it's registered with ArgoCD), but we'll use the workspace cluster for convenience: arlon cluster deploy --repo-url ${ WORKSPACE_REPO_URL } --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks # OR # using repository aliases # using the default alias arlon cluster deploy --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks # using the prod alias arlon cluster deploy --repo-alias prod --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks The git directory hosting the cluster Helm chart is created as a subdirectory of a base path in the repo. The base path can be specified with --base-path , but we'll leave it unspecified in order to use the default value of clusters . Consequently, this example produces the directory clusters/eks-1/ in the repo. To verify its presence: $ cd ${ WORKSPACE_REPO } $ git pull $ tree clusters/eks-1 clusters/eks-1 \u2514\u2500\u2500 mgmt \u251c\u2500\u2500 charts \u2502 \u251c\u2500\u2500 capi-aws-eks \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u251c\u2500\u2500 capi-aws-kubeadm \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u2514\u2500\u2500 xplane-aws-eks \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2514\u2500\u2500 templates \u2502 \u251c\u2500\u2500 cluster.yaml \u2502 \u2514\u2500\u2500 network.yaml \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 clusterregistration.yaml \u2502 \u251c\u2500\u2500 ns.yaml \u2502 \u251c\u2500\u2500 profile.yaml \u2502 \u2514\u2500\u2500 rbac.yaml \u2514\u2500\u2500 values.yaml The chart contains several subcharts under mgmt/charts/ , one for each supported type of cluster. Only one of them will be enabled, in this case capi-aws-eks (Cluster API on AWS with type EKS). At this point, the cluster is provisioning and can be seen in arlon and AWS EKS: $ arlon cluster list NAME CLUSTERSPEC PROFILE eks-1 capi-eks dynamic-1 $ aws eks list-clusters { \"clusters\" : [ \"eks-1_eks-1-control-plane\" , ] } Eventually, it will also be seen as a registered cluster in argocd, but this won't be visible for a while, because the cluster is not registered until its control plane (the Kubernetes API) is ready: $ argocd cluster list SERVER NAME VERSION STATUS MESSAGE https://9F07DC211252C6F7686F90FA5B8B8447.gr7.us-west-2.eks.amazonaws.com eks-1 1 .18+ Successful https://kubernetes.default.svc in -cluster 1 .20+ Successful To monitor the progress of the cluster deployment, check the status of the ArgoCD app of the same name: $ argocd app list NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGET eks-1 https://kubernetes.default.svc default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } clusters/eks-1/mgmt main eks-1-guestbook-static default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-1/workload/guestbook-static HEAD eks-1-profile-dynamic-1 https://kubernetes.default.svc argocd default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-1/mgmt HEAD eks-1-xenial default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-1/workload/xenial HEAD The top-level app eks-1 is the root of all argocd apps that make up the cluster and its configuration contents. The next level app eks-1-profile-dynamic-1 represents the profile, and its children apps eks-1-guestbook-static and eks-1-xenial correspond to the bundles. Note: The overall tree-like organization of the apps and their health status can be visually observed in the ArgoCD web user interface._ The cluster is fully deployed once those apps are all Synced and Healthy . An EKS cluster typically takes 10-15 minutes to finish deploying. Behavioral differences between static and dynamic bundles & profiles \u00b6 Static bundle \u00b6 A change to a static bundle does not affect existing clusters using that bundle (through a profile). To illustrate this, bring up the ArgoCD UI and open the detailed view of the eks-1-guestbook-static application, which applies the guestbook-static bundle to the eks-1 cluster. Note that there is only one guestbook-ui pod. Next, update the guestbook-static bundle to have 3 replicas of the pod: arlon bundle update guestbook-static --from-file examples/bundles/guestbook-3replicas.yaml Note that the UI continues to show one pod. Only new clusters consuming this bundle will have the 3 replicas. Dynamic profile \u00b6 Before discussing dynamic bundles, we take a small detour to introduce dynamic profiles, since this will help understand the relationship between profiles and bundles. To illustrate how a profile can be updated, we remove guestbook-static bundle from dynamic-1 by specifying a new bundle set: arlon profile update dynamic-1 --bundles xenial Since the old bundle set was guestbook-static,xenial-static , that command resulted in the removal of guestbook-static from the profile. In the UI, observe the eks-1-profile-dynamic-1 app going through Sync and Progressing phases, eventually reaching the healthy (green) state. And most importantly, the eks-1-guestbook-static app is gone. The reason this real-time change to the cluster was possible is that the dynamic-1 profile is dynamic, meaning any change to its composition results in arlon updating the corresponding compiled Helm chart in git. ArgoCD detects this git change and propagates the app / configuration updates to the cluster. If the profile were of the static type, a change in its composition (the set of bundles) would not have affected existing clusters using that profile. It would only affect new clusters created with the profile. Dynamic bundle \u00b6 To illustrate the defining characteristic of a dynamic bundle, we first add guestbook-dynamic to dynamic-1 : arlon profile update dynamic-1 --bundles xenial,guestbook-dynamic Observe the re-appearance of the guestbook application, which is managed by the eks-1-guestbook-dynamic ArgoCD app. A detailed view of the app shows 1 guestbook-ui pod. Remember that a dynamic bundle's manifest content is stored in git. Use these commands to change the number of pod replicas to 3: cd ${ WORKSPACE_REPO } git pull # to get all latest changes pushed by arlon vim bundles/guestbook/guestbook.yaml # edit to change deployment's replicas to 3 git commit -am \"increase guestbook replicas\" git push origin main Observe the number of pods increasing to 3 in the UI. Any existing cluster consuming this dynamic bundle will be updated similarly, regardless of whether the bundle is consumed via a dynamic or static profile. Static profile \u00b6 Finally, a profile can be static. It means that it has no corresponding \"compiled\" component (a Helm chart) living in git. When a cluster is deployed using a static profile, the set of bundles (whether static or dynamic) it receives is determined by the bundle set defined by the profile at deployment time, and will not change in the future, even if the profile is updated to a new set at a later time. Cluster updates and upgrades \u00b6 The arlon cluster update [flags] command allows you to make changes to an existing cluster. The clusterspec, profile, or both can change, provided that the following rules and guidelines are followed. Clusterspec \u00b6 There are two scenarios. In the first, the clusterspec name associated with the cluster hasn't changed, meaning the cluster is using the same clusterspec. However, some properties of the clusterspec's properties have changed since the cluster was deployed or last updated, using arlon clusterspec update Arlon supports updating the cluster to use updated values of the following properties: kubernetesVersion nodeCount nodeType Note: Updating the cluster is not allowed if other properties of its clusterspec (e.g. cluster orchestration API provider, cloud, cluster type, region, pod CIDR block, etc...) have changed, however new clusters can always be created/deployed using the changed clusterspec. A change in kubernetesVersion will result in a cluster upgrade/downgrade. There are some restrictions and caveats you need to be aware of: The specific Kubernetes version must be supported by the particular implementation and release of the underlying cluster orchestration API provider cloud, and cluster type. In general, the control plane will be upgraded first Existing nodes are not typically not upgraded to the new Kubernetes version. Only new nodes (added as part of manual nodeCount change or autoscaling) In the second scenario, as part of an update operation, you may choose to associate the cluster with a different clusterspec altogether. The rule governing the allowed property changes remains the same: the cluster update operation is allowed if, relative to the previously associated clusterspec, the new clusterspec's properties differ only in the values listed above. Profile \u00b6 You can specify a completely different profile when updating a cluster. All bundles previously used will be removed from the cluster, and new ones specified by the new profile will be applied. This is regardless of whether the old and new profiles are static or dynamic. Examples \u00b6 These sequence of commands updates a clusterspec to a newer Kubernetes version and a higher node count, then upgrades the cluster to the newer specifications: arlon clusterspec update capi-eks --nodecount 3 --kubeversion v1.19.15 arlon cluster update eks-1 Note that the 2nd command didn't need any flags because the clusterspec used is the same as before. This example updates a cluster to use a new profile my-new-profile : arlon cluster update eks-1 --profile my-new-profile Enabling Cluster Autoscaler in the workload cluster: \u00b6 Bundle creation: \u00b6 Register a dynamic bundle pointing to the bundles/capi-cluster-autoscaler in the Arlon repo. To enable the cluster-autoscaler bundle, add one more parameter during cluster creation: srcType . This is the ArgoCD-defined application source type (Helm, Kustomize, Directory). In addition to this, the repo-revision parameter should also be set to a stable arlon release branch ( in this case v0.10 ). This example creates a bundle pointing to the bundles/capi-cluster-autoscaler in Arlon repo arlon bundle create cas-bundle --tags cas,devel,test --desc \"CAS Bundle\" --repo-url https://github.com/arlonproj/arlon.git --repo-path bundles/capi-cluster-autoscaler --srctype helm --repo-revision v0.10.0 Profile creation: \u00b6 Create a profile that contains this capi-cluster-autoscaler bundle. arlon profile create dynamic-cas --repo-url ${ WORKSPACE_REPO_URL } --repo-base-path profiles --bundles cas-bundle --desc \"dynamic cas profile\" --tags examples Clusterspec creation: \u00b6 Create a clusterspec with CAPI as ApiProvider and autoscaling enabled.In addition to this, the ClusterAutoscaler(Min|Max)Nodes properties are used to set 2 annotations on MachineDeployment required by the cluster autoscaler for CAPI. arlon clusterspec create cas-spec --api capi --cloud aws --type eks --kubeversion v1.21.10 --nodecount 2 --nodetype t2.medium --tags devel,test --desc \"dev/test\" --region ${ REGION } --sshkey ${ SSH_KEY_NAME } --casenabled Cluster creation: \u00b6 Deploy a cluster from this cluster spec and profile created in the previous steps. arlon cluster deploy --repo-url ${ WORKSPACE_REPO_URL } --cluster-name cas-cluster --profile dynamic-cas --cluster-spec cas-spec Consequently, this example produces the directory clusters/cas-cluster/ in the repo. This will contain the capi-autoscaler subchart and manifests mgmt/charts/ . To verify its contents: $ cd ${ WORKSPACE_REPO_URL } $ tree clusters/cas-cluster clusters/cas-cluster \u2514\u2500\u2500 mgmt \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 charts \u2502 \u251c\u2500\u2500 capi-aws-eks \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u251c\u2500\u2500 capi-aws-kubeadm \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u251c\u2500\u2500 capi-cluster-autoscaler \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u251c\u2500\u2500 callhomeconfig.yaml \u2502 \u2502 \u2514\u2500\u2500 rbac.yaml \u2502 \u2514\u2500\u2500 xplane-aws-eks \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2514\u2500\u2500 templates \u2502 \u251c\u2500\u2500 cluster.yaml \u2502 \u2514\u2500\u2500 network.yaml \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 clusterregistration.yaml \u2502 \u251c\u2500\u2500 ns.yaml \u2502 \u251c\u2500\u2500 profile.yaml \u2502 \u2514\u2500\u2500 rbac.yaml \u2514\u2500\u2500 values.yaml At this point, the cluster is provisioning. To monitor the progress of the cluster deployment, check the status of the ArgoCD app of the same name. Eventually, the ArgoCD apps will be synced and healthy. $ argocd app list NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGET cas-cluster https://kubernetes.default.svc default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } clusters/cas-cluster/mgmt main cas-cluster-cas-bundle cas-cluster default default Synced Healthy Auto-Prune <none> https://github.com/arlonproj/arlon.git bundles/capi-cluster-autoscaler HEAD cas-cluster-profile-dynamic-cas https://kubernetes.default.svc argocd default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-cas/mgmt","title":"Tutorial (gen1)"},{"location":"tutorial/#tutorial-gen1","text":"This assumes that you plan to deploy workload clusters on AWS cloud, with Cluster API (\"CAPI\") as the cluster orchestration API provider. Also ensure you have set up a workspace repository , and it is registered as a git repo in ArgoCD. The tutorial will assume the existence of these environment variables: ${ARLON_REPO} : where the arlon repo is locally checked out ${WORKSPACE_REPO} : where the workspace repo is locally checked out ${WORKSPACE_REPO_URL} : the workspace repo's git URL. It typically looks like https://github.com/${username}/${reponame}.git ${CLOUD_REGION} : the region where you want to deploy example clusters and workloads (e.g. us-west-2) ${SSH_KEY_NAME} : the name of a public ssh key name registered in your cloud account, to enable ssh to your cluster nodes Additionally, for examples assuming arlon git register , \"default\" and a \"prod\" git repo aliases will also be given. _Note: for the best experience, make sure your workspace repo is configured to send change notifications to ArgoCD via a webhook. See the Installation section for details.","title":"Tutorial (gen1)"},{"location":"tutorial/#cluster-specs","text":"We first create a few cluster specs with different combinations of API providers and cluster types (kubeadm vs EKS). One of the cluster specs is for an unconfigured API provider (Crossplane); this is for illustrative purposes, since we will not use it in this tutorial. arlon clusterspec create capi-kubeadm-3node --api capi --cloud aws --type kubeadm --kubeversion v1.21.10 --nodecount 3 --nodetype t2.medium --tags devel,test --desc \"3 node kubeadm for dev/test\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } arlon clusterspec create capi-eks --api capi --cloud aws --type eks --kubeversion v1.21.10 --nodecount 2 --nodetype t2.large --tags staging --desc \"2 node eks for general purpose\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } arlon clusterspec create xplane-eks-3node --api xplane --cloud aws --type eks --kubeversion v1.21.10 --nodecount 4 --nodetype t2.small --tags experimental --desc \"4 node eks managed by crossplane\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } Ensure you can now list the cluster specs: $ arlon clusterspec list NAME APIPROV CLOUDPROV TYPE KUBEVERSION NODETYPE NODECNT MSTNODECNT SSHKEY CAS CASMIN CASMAX TAGS DESCRIPTION capi-eks capi aws eks v1.21.10 t2.large 2 3 leb false 1 9 staging 2 node eks for general purpose capi-kubeadm-3node capi aws kubeadm v1.21.10 t2.medium 3 3 leb false 1 9 devel,test 3 node kubeadm for dev/test xplane-eks-3node xplane aws eks v1.21.10 t2.small 4 3 leb false 1 9 experimental 4 node eks managed by crossplane","title":"Cluster specs"},{"location":"tutorial/#bundles","text":"First create a static bundle containing raw YAML for the guestbook sample application from this example file: cd ${ ARLON_REPO } arlon bundle create guestbook-static --tags applications --desc \"guestbook app\" --from-file examples/bundles/guestbook.yaml ( Note: the YAML is simply a concatenation of the files found in the ArgoCD Example Apps repo ) To illustrate the difference between static and dynamic bundles, we create a dynamic version of the same application, this time using a reference to a git directory containing the YAML. We could point it directly to the copy in the ArgoCD Example Apps repo , but we'll want to make modifications to it, so we instead create a new directory to host our own copy in our workspace directory: cd ${ WORKSPACE_REPO } mkdir -p bundles/guestbook cp ${ ARLON_REPO } /examples/bundles/guestbook.yaml bundles/guestbook git add bundles/guestbook git commit -m \"add guestbook\" git push origin main arlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-url ${ WORKSPACE_REPO_URL } --repo-path bundles/guestbook # OR # using repository aliases # using the default alias arlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-path bundles/guestbook # using the prod alias arlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-path bundles/guestbook --repo-alias prod Next, we create a static bundle for another \"dummy\" application, an Ubuntu pod (OS version: \"Xenial\") that does nothing but print the date-time in an infinite sleep loop: cd ${ ARLON_REPO } arlon bundle create xenial-static --tags applications --desc \"xenial pod\" --from-file examples/bundles/xenial.yaml Finally, we create a bundle for the Calico CNI, which provides pod networking. Some types of clusters (e.g. kubeadm) require a CNI provider to be installed onto a newly created cluster, so encapsulating the provider as a bundle will give us a flexible way to install it. We download a known copy from the authoritative source and store it the workspace repo in order to create a dynamic bundle from it: cd ${ WORKSPACE_REPO } mkdir -p bundles/calico curl https://docs.projectcalico.org/v3.21/manifests/calico.yaml -o bundles/calico/calico.yaml git add bundles/calico git commit -m \"add calico\" git push origin main arlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-url ${ WORKSPACE_REPO_URL } --repo-path bundles/calico # OR # using repository aliases # using the default alias arlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-path bundles/calico # using the prod alias arlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-path bundles/calico --repo-alias prod List your bundles to verify they were correctly entered: $ arlon bundle list NAME TYPE TAGS REPO-URL REPO-PATH DESCRIPTION calico dynamic networking,cni ${ WORKSPACE_REPO_URL } bundles/calico Calico CNI guestbook-dynamic dynamic applications ${ WORKSPACE_REPO_URL } bundles/guestbook guestbook app ( dynamic ) guestbook-static static applications ( N/A ) ( N/A ) guestbook app xenial-static static applications ( N/A ) ( N/A ) ubuntu pod in infinite sleep loop","title":"Bundles"},{"location":"tutorial/#profiles","text":"We can now create profiles to group bundles into useful, deployable sets. First, create a static profile containing bundles xenial-static and guestbook-static: arlon profile create static-1 --static --bundles guestbook-static,xenial-static --desc \"static profile 1\" --tags examples Secondly, create a dynamic version of the same profile. We'll store the compiled form of the profile in the profiles/dynamic-1 directory of the workspace repo. We don't create it manually; instead, the arlon CLI will create it for us, and it will push the change to git: arlon profile create dynamic-1 --repo-url ${ WORKSPACE_REPO_URL } --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples # OR # using repository aliases # using the default alias arlon profile create dynamic-1 --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples # using the prod alias arlon profile create dynamic-1 --repo-alias prod --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples Note: the --repo-base-path profiles option tells arlon to create the profile under a base directory profiles/ (to be created if it doesn't exist). That is in fact the default value of that option, so it is not necessary to specify it in this case. To verify that the compiled profile was created correctly: $ cd ${ WORKSPACE_REPO } $ git pull $ tree profiles profiles \u251c\u2500\u2500 dynamic-1 \u2502 \u251c\u2500\u2500 mgmt \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u251c\u2500\u2500 guestbook-dynamic.yaml \u2502 \u2502 \u251c\u2500\u2500 placeholder_configmap.yaml \u2502 \u2502 \u2514\u2500\u2500 xenial.yaml \u2502 \u2514\u2500\u2500 workload \u2502 \u2514\u2500\u2500 xenial \u2502 \u2514\u2500\u2500 xenial.yaml [ ... ] Since xenial is a static bundle, a copy of its YAML was stored in workload/xenial/xenial.yaml . This is not done for guestbook-dynamic because it is dynamic. Finally, we create another variant of the same profile, with the only difference being the addition of Calico bundle. It'll be used on clusters that need a CNI provider: arlon profile create dynamic-2-calico --repo-url ${ WORKSPACE_REPO_URL } --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples # OR # using repository aliases # using the default alias arlon profile create dynamic-2-calico --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples # using the prod alias arlon profile create dynamic-2-calico --repo-alias prod --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples Listing the profiles should show: $ arlon profile list NAME TYPE BUNDLES REPO-URL REPO-PATH TAGS DESCRIPTION dynamic-1 dynamic guestbook-static,xenial-static ${ WORKSPACE_REPO_URL } profiles/dynamic-1 examples dynamic test 1 dynamic-2-calico dynamic calico,guestbook-static,xenial-static ${ WORKSPACE_REPO_URL } profiles/dynamic-2-calico examples dynamic test 1 static-1 static guestbook-dynamic,xenial-static ( N/A ) ( N/A ) examples static profile 1","title":"Profiles"},{"location":"tutorial/#clusters-gen1","text":"We are now ready to deploy our first cluster. It will be of type EKS. Since EKS clusters come configured with pod networking out of the box, we choose a profile that does not include Calico: dynamic-1 . When deploying a cluster, arlon creates in git a Helm chart containing the manifests for creating and bootstrapping the cluster. Arlon then creates an ArgoCD App referencing the chart, thereby relying on ArgoCD to orchestrate the whole process of deploying and configuring the cluster. The arlon deploy command accepts a git URL and path for this git location. Any git repo can be used (so long as it's registered with ArgoCD), but we'll use the workspace cluster for convenience: arlon cluster deploy --repo-url ${ WORKSPACE_REPO_URL } --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks # OR # using repository aliases # using the default alias arlon cluster deploy --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks # using the prod alias arlon cluster deploy --repo-alias prod --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks The git directory hosting the cluster Helm chart is created as a subdirectory of a base path in the repo. The base path can be specified with --base-path , but we'll leave it unspecified in order to use the default value of clusters . Consequently, this example produces the directory clusters/eks-1/ in the repo. To verify its presence: $ cd ${ WORKSPACE_REPO } $ git pull $ tree clusters/eks-1 clusters/eks-1 \u2514\u2500\u2500 mgmt \u251c\u2500\u2500 charts \u2502 \u251c\u2500\u2500 capi-aws-eks \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u251c\u2500\u2500 capi-aws-kubeadm \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u2514\u2500\u2500 xplane-aws-eks \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2514\u2500\u2500 templates \u2502 \u251c\u2500\u2500 cluster.yaml \u2502 \u2514\u2500\u2500 network.yaml \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 clusterregistration.yaml \u2502 \u251c\u2500\u2500 ns.yaml \u2502 \u251c\u2500\u2500 profile.yaml \u2502 \u2514\u2500\u2500 rbac.yaml \u2514\u2500\u2500 values.yaml The chart contains several subcharts under mgmt/charts/ , one for each supported type of cluster. Only one of them will be enabled, in this case capi-aws-eks (Cluster API on AWS with type EKS). At this point, the cluster is provisioning and can be seen in arlon and AWS EKS: $ arlon cluster list NAME CLUSTERSPEC PROFILE eks-1 capi-eks dynamic-1 $ aws eks list-clusters { \"clusters\" : [ \"eks-1_eks-1-control-plane\" , ] } Eventually, it will also be seen as a registered cluster in argocd, but this won't be visible for a while, because the cluster is not registered until its control plane (the Kubernetes API) is ready: $ argocd cluster list SERVER NAME VERSION STATUS MESSAGE https://9F07DC211252C6F7686F90FA5B8B8447.gr7.us-west-2.eks.amazonaws.com eks-1 1 .18+ Successful https://kubernetes.default.svc in -cluster 1 .20+ Successful To monitor the progress of the cluster deployment, check the status of the ArgoCD app of the same name: $ argocd app list NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGET eks-1 https://kubernetes.default.svc default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } clusters/eks-1/mgmt main eks-1-guestbook-static default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-1/workload/guestbook-static HEAD eks-1-profile-dynamic-1 https://kubernetes.default.svc argocd default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-1/mgmt HEAD eks-1-xenial default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-1/workload/xenial HEAD The top-level app eks-1 is the root of all argocd apps that make up the cluster and its configuration contents. The next level app eks-1-profile-dynamic-1 represents the profile, and its children apps eks-1-guestbook-static and eks-1-xenial correspond to the bundles. Note: The overall tree-like organization of the apps and their health status can be visually observed in the ArgoCD web user interface._ The cluster is fully deployed once those apps are all Synced and Healthy . An EKS cluster typically takes 10-15 minutes to finish deploying.","title":"Clusters (gen1)"},{"location":"tutorial/#behavioral-differences-between-static-and-dynamic-bundles-profiles","text":"","title":"Behavioral differences between static and dynamic bundles &amp; profiles"},{"location":"tutorial/#static-bundle","text":"A change to a static bundle does not affect existing clusters using that bundle (through a profile). To illustrate this, bring up the ArgoCD UI and open the detailed view of the eks-1-guestbook-static application, which applies the guestbook-static bundle to the eks-1 cluster. Note that there is only one guestbook-ui pod. Next, update the guestbook-static bundle to have 3 replicas of the pod: arlon bundle update guestbook-static --from-file examples/bundles/guestbook-3replicas.yaml Note that the UI continues to show one pod. Only new clusters consuming this bundle will have the 3 replicas.","title":"Static bundle"},{"location":"tutorial/#dynamic-profile","text":"Before discussing dynamic bundles, we take a small detour to introduce dynamic profiles, since this will help understand the relationship between profiles and bundles. To illustrate how a profile can be updated, we remove guestbook-static bundle from dynamic-1 by specifying a new bundle set: arlon profile update dynamic-1 --bundles xenial Since the old bundle set was guestbook-static,xenial-static , that command resulted in the removal of guestbook-static from the profile. In the UI, observe the eks-1-profile-dynamic-1 app going through Sync and Progressing phases, eventually reaching the healthy (green) state. And most importantly, the eks-1-guestbook-static app is gone. The reason this real-time change to the cluster was possible is that the dynamic-1 profile is dynamic, meaning any change to its composition results in arlon updating the corresponding compiled Helm chart in git. ArgoCD detects this git change and propagates the app / configuration updates to the cluster. If the profile were of the static type, a change in its composition (the set of bundles) would not have affected existing clusters using that profile. It would only affect new clusters created with the profile.","title":"Dynamic profile"},{"location":"tutorial/#dynamic-bundle","text":"To illustrate the defining characteristic of a dynamic bundle, we first add guestbook-dynamic to dynamic-1 : arlon profile update dynamic-1 --bundles xenial,guestbook-dynamic Observe the re-appearance of the guestbook application, which is managed by the eks-1-guestbook-dynamic ArgoCD app. A detailed view of the app shows 1 guestbook-ui pod. Remember that a dynamic bundle's manifest content is stored in git. Use these commands to change the number of pod replicas to 3: cd ${ WORKSPACE_REPO } git pull # to get all latest changes pushed by arlon vim bundles/guestbook/guestbook.yaml # edit to change deployment's replicas to 3 git commit -am \"increase guestbook replicas\" git push origin main Observe the number of pods increasing to 3 in the UI. Any existing cluster consuming this dynamic bundle will be updated similarly, regardless of whether the bundle is consumed via a dynamic or static profile.","title":"Dynamic bundle"},{"location":"tutorial/#static-profile","text":"Finally, a profile can be static. It means that it has no corresponding \"compiled\" component (a Helm chart) living in git. When a cluster is deployed using a static profile, the set of bundles (whether static or dynamic) it receives is determined by the bundle set defined by the profile at deployment time, and will not change in the future, even if the profile is updated to a new set at a later time.","title":"Static profile"},{"location":"tutorial/#cluster-updates-and-upgrades","text":"The arlon cluster update [flags] command allows you to make changes to an existing cluster. The clusterspec, profile, or both can change, provided that the following rules and guidelines are followed.","title":"Cluster updates and upgrades"},{"location":"tutorial/#clusterspec","text":"There are two scenarios. In the first, the clusterspec name associated with the cluster hasn't changed, meaning the cluster is using the same clusterspec. However, some properties of the clusterspec's properties have changed since the cluster was deployed or last updated, using arlon clusterspec update Arlon supports updating the cluster to use updated values of the following properties: kubernetesVersion nodeCount nodeType Note: Updating the cluster is not allowed if other properties of its clusterspec (e.g. cluster orchestration API provider, cloud, cluster type, region, pod CIDR block, etc...) have changed, however new clusters can always be created/deployed using the changed clusterspec. A change in kubernetesVersion will result in a cluster upgrade/downgrade. There are some restrictions and caveats you need to be aware of: The specific Kubernetes version must be supported by the particular implementation and release of the underlying cluster orchestration API provider cloud, and cluster type. In general, the control plane will be upgraded first Existing nodes are not typically not upgraded to the new Kubernetes version. Only new nodes (added as part of manual nodeCount change or autoscaling) In the second scenario, as part of an update operation, you may choose to associate the cluster with a different clusterspec altogether. The rule governing the allowed property changes remains the same: the cluster update operation is allowed if, relative to the previously associated clusterspec, the new clusterspec's properties differ only in the values listed above.","title":"Clusterspec"},{"location":"tutorial/#profile","text":"You can specify a completely different profile when updating a cluster. All bundles previously used will be removed from the cluster, and new ones specified by the new profile will be applied. This is regardless of whether the old and new profiles are static or dynamic.","title":"Profile"},{"location":"tutorial/#examples","text":"These sequence of commands updates a clusterspec to a newer Kubernetes version and a higher node count, then upgrades the cluster to the newer specifications: arlon clusterspec update capi-eks --nodecount 3 --kubeversion v1.19.15 arlon cluster update eks-1 Note that the 2nd command didn't need any flags because the clusterspec used is the same as before. This example updates a cluster to use a new profile my-new-profile : arlon cluster update eks-1 --profile my-new-profile","title":"Examples"},{"location":"tutorial/#enabling-cluster-autoscaler-in-the-workload-cluster","text":"","title":"Enabling Cluster Autoscaler in the workload cluster:"},{"location":"tutorial/#bundle-creation","text":"Register a dynamic bundle pointing to the bundles/capi-cluster-autoscaler in the Arlon repo. To enable the cluster-autoscaler bundle, add one more parameter during cluster creation: srcType . This is the ArgoCD-defined application source type (Helm, Kustomize, Directory). In addition to this, the repo-revision parameter should also be set to a stable arlon release branch ( in this case v0.10 ). This example creates a bundle pointing to the bundles/capi-cluster-autoscaler in Arlon repo arlon bundle create cas-bundle --tags cas,devel,test --desc \"CAS Bundle\" --repo-url https://github.com/arlonproj/arlon.git --repo-path bundles/capi-cluster-autoscaler --srctype helm --repo-revision v0.10.0","title":"Bundle creation:"},{"location":"tutorial/#profile-creation","text":"Create a profile that contains this capi-cluster-autoscaler bundle. arlon profile create dynamic-cas --repo-url ${ WORKSPACE_REPO_URL } --repo-base-path profiles --bundles cas-bundle --desc \"dynamic cas profile\" --tags examples","title":"Profile creation:"},{"location":"tutorial/#clusterspec-creation","text":"Create a clusterspec with CAPI as ApiProvider and autoscaling enabled.In addition to this, the ClusterAutoscaler(Min|Max)Nodes properties are used to set 2 annotations on MachineDeployment required by the cluster autoscaler for CAPI. arlon clusterspec create cas-spec --api capi --cloud aws --type eks --kubeversion v1.21.10 --nodecount 2 --nodetype t2.medium --tags devel,test --desc \"dev/test\" --region ${ REGION } --sshkey ${ SSH_KEY_NAME } --casenabled","title":"Clusterspec creation:"},{"location":"tutorial/#cluster-creation","text":"Deploy a cluster from this cluster spec and profile created in the previous steps. arlon cluster deploy --repo-url ${ WORKSPACE_REPO_URL } --cluster-name cas-cluster --profile dynamic-cas --cluster-spec cas-spec Consequently, this example produces the directory clusters/cas-cluster/ in the repo. This will contain the capi-autoscaler subchart and manifests mgmt/charts/ . To verify its contents: $ cd ${ WORKSPACE_REPO_URL } $ tree clusters/cas-cluster clusters/cas-cluster \u2514\u2500\u2500 mgmt \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 charts \u2502 \u251c\u2500\u2500 capi-aws-eks \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u251c\u2500\u2500 capi-aws-kubeadm \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u251c\u2500\u2500 capi-cluster-autoscaler \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u251c\u2500\u2500 callhomeconfig.yaml \u2502 \u2502 \u2514\u2500\u2500 rbac.yaml \u2502 \u2514\u2500\u2500 xplane-aws-eks \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2514\u2500\u2500 templates \u2502 \u251c\u2500\u2500 cluster.yaml \u2502 \u2514\u2500\u2500 network.yaml \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 clusterregistration.yaml \u2502 \u251c\u2500\u2500 ns.yaml \u2502 \u251c\u2500\u2500 profile.yaml \u2502 \u2514\u2500\u2500 rbac.yaml \u2514\u2500\u2500 values.yaml At this point, the cluster is provisioning. To monitor the progress of the cluster deployment, check the status of the ArgoCD app of the same name. Eventually, the ArgoCD apps will be synced and healthy. $ argocd app list NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGET cas-cluster https://kubernetes.default.svc default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } clusters/cas-cluster/mgmt main cas-cluster-cas-bundle cas-cluster default default Synced Healthy Auto-Prune <none> https://github.com/arlonproj/arlon.git bundles/capi-cluster-autoscaler HEAD cas-cluster-profile-dynamic-cas https://kubernetes.default.svc argocd default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-cas/mgmt","title":"Cluster creation:"}]}