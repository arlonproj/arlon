# Next-Generation (gen2) Clusters - New in version 0.9.x

Gen1 clusters are limited in capability by the Helm chart used to deploy the infrastructure resources.
Advanced Cluster API configurations, such as those using multiple MachinePools, each with different
instance types, is not supported.

Gen2 clusters solve this problem by allowing you to create workload clusters from a *base cluster*
that you design and provide in the form of a manifest file stored in a git directory. The manifest
typically contains multiple related resources that together define an arbitrarily complex cluster.
If you make subsequent changes to the base cluster, workload clusters originally created from it
will automatically acquire the changes.

## Creating Cluster-API cluster manifest

Note: The CAPA version used here is v2.0 and the manifests created here are in accordance with this version.

Refer the [compatibility matrix for Cluster API provider and CAPA versions](https://github.com/kubernetes-sigs/cluster-api-provider-aws#compatibility-with-cluster-api-and-kubernetes-versions) for supported versions.

Before deploying a EKS cluster, make sure to setup the AWS Environment as stated in the [quickstart giude for CAPI](https://cluster-api.sigs.k8s.io/user/quick-start.html)

### MachineDeployment

Here is an example of a manifest file that we can use to create a *base cluster*. This manifest file helps in
deploying an EKS cluster with 'machine deployment' component from the cluster API (CAPI). This file has been generated by the following command

```shell
clusterctl generate cluster capi-quickstart --flavor eks \
  --kubernetes-version v1.24.0 \
  --control-plane-machine-count=3 \
  --worker-machine-count=3 \
  > capi-quickstart.yaml
```

```yaml
# YAML
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: capi-quickstart
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta2
    kind: AWSManagedControlPlane
    name: capi-quickstart-control-plane
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
    kind: AWSManagedCluster
    name: capi-quickstart
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSManagedCluster
metadata:
  name: capi-quickstart
spec: {}
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta2
kind: AWSManagedControlPlane
metadata:
  name: capi-quickstart-control-plane
  namespace: default
spec:
  region: {REGION}
  sshKeyName: {SSH_KEYNAME}
  version: v1.24.0
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: capi-quickstart-md-0
  namespace: default
spec:
  clusterName: capi-quickstart
  replicas: 3
  selector:
    matchLabels: null
  template:
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta2
          kind: EKSConfigTemplate
          name: capi-quickstart-md-0
      clusterName: capi-quickstart
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: AWSMachineTemplate
        name: capi-quickstart-md-0
      version: v1.24.0
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSMachineTemplate
metadata:
  name: capi-quickstart-md-0
  namespace: default
spec:
  template:
    spec:
      iamInstanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io
      instanceType: {INSTANCE_TYPE}
      sshKeyName: {SSH_KEYNAME}
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta2
kind: EKSConfigTemplate
metadata:
  name: capi-quickstart-md-0
  namespace: default
spec:
  template: {}
```

### AWSManagedMachinePool

Initialize the environment for AWSManagedMachinePool as stated [here](https://cluster-api-aws.sigs.k8s.io/topics/machinepools.html#awsmanagedmachinepool)

Before deploying an EKS cluster, make sure that the MachinePool feature gate is enabled. To do so, run this command:

```shell
kubectl describe deployment capa-controller-manager -n capa-system
```

In the output, in the feature gates section of the deployment, MachinePool must be set to true.

```shell
> kubectl describe deployment capa-controller-manager -n capa-system
..........
..........
--featuregates=EKS=true,EKSEnableIAM=false,EKSAllowAddRoles=false,EKSFargate=true,MachinePool=true,EventBridgeInstanceState=false,
AutoControllerIdentityCreator=true,BootstrapFormatIgnition=false,ExternalResourceGC=false
..........
..........
```

This manifest file helps in deploying an EKS cluster with 'AWSManagedMachinePool' component from the cluster API (CAPI). This file has been generated by the following command

```shell
clusterctl generate cluster awsmanaged-cluster --kubernetes-version v1.22.0 --flavor eks-managedmachinepool > manifest.yaml
```

```yaml
# YAML
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: awsmanaged-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta2
    kind: AWSManagedControlPlane
    name: awsmanaged-cluster-control-plane
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
    kind: AWSManagedCluster
    name: awsmanaged-cluster
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSManagedCluster
metadata:
  name: awsmanaged-cluster
  namespace: default
spec: {}
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta2
kind: AWSManagedControlPlane
metadata:
  name: awsmanaged-cluster-control-plane
  namespace: default
spec:
  region: {REGION}
  sshKeyName: {SSH_KEYNAME}
  version: v1.22.0
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachinePool
metadata:
  name: awsmanaged-cluster-pool-0
  namespace: default
spec:
  clusterName: awsmanaged-cluster
  replicas: 1
  template:
    spec:
      bootstrap:
        dataSecretName: ""
      clusterName: awsmanaged-cluster
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: AWSManagedMachinePool
        name: awsmanaged-cluster-pool-0
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSManagedMachinePool
metadata:
  name: awsmanaged-cluster-pool-0
  namespace: default
spec: {}
```

### AWSMachinePool

An AWSMachinePool corresponds to an AWS AutoScaling Groups, which provides the cloud provider specific resource for orchestrating a group of EC2 machines.

Initialize the environment for AWSMachinePool as stated [here](<https://cluster-api-aws.sigs.k8s.io/topics/machinepools.html#awsmachinepool>)

Before deploying an EKS cluster, make sure that the AWSMachinePool feature gate is enabled. To do so, run this command:

```shell
kubectl describe deployment capa-controller-manager -n capa-system
```

In the output, in the feature gates section of the deployment, MachinePool must be set to true.

```shell
> kubectl describe deployment capa-controller-manager -n capa-system
..........
..........
--featuregates=EKS=true,EKSEnableIAM=false,EKSAllowAddRoles=false,EKSFargate=true,MachinePool=true,EventBridgeInstanceState=false,
AutoControllerIdentityCreator=true,BootstrapFormatIgnition=false,ExternalResourceGC=false
..........
..........
```

This manifest file helps in deploying an EKS cluster with 'AWSManagedMachinePool' component from the cluster API (CAPI). This file has been generated by the following command

```shell
clusterctl generate cluster awsmanaged-cluster --kubernetes-version v1.22.0 --flavor eks-machinepool > manifest.yaml
```

```yaml
# YAML
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: awsmanaged-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta2
    kind: AWSManagedControlPlane
    name: awsmanaged-cluster-control-plane
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
    kind: AWSManagedCluster
    name: awsmanaged-cluster
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSManagedCluster
metadata:
  name: awsmanaged-cluster
  namespace: default
spec: {}
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta2
kind: AWSManagedControlPlane
metadata:
  name: awsmanaged-cluster-control-plane
  namespace: default
spec:
  region: {REGION}
  sshKeyName: {SSH_KEYNAME}
  version: v1.22.0
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachinePool
metadata:
  name: awsmanaged-cluster-mp-0
  namespace: default
spec:
  clusterName: awsmanaged-cluster
  replicas: 1
  template:
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta2
          kind: EKSConfig
          name: awsmanaged-cluster-mp-0
      clusterName: awsmanaged-cluster
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: AWSMachinePool
        name: awsmanaged-cluster-mp-0
      version: v1.22.0
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSMachinePool
metadata:
  name: awsmanaged-cluster-mp-0
  namespace: default
spec:
  awsLaunchTemplate:
    iamInstanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io
    instanceType: {INSTANCE_TYPE}
    sshKeyName: {SSH_KEYNAME}
  maxSize: 10
  minSize: 1
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta2
kind: EKSConfig
metadata:
  name: awsmanaged-cluster-mp-0
  namespace: default
spec: {}
```

## gen2 cluster creation using Arlon

This manifest file needs to be pushed to the workspace repository before the manifest directory is prepped and then validated.

Before a manifest directory can be used as a base cluster, it must first be "prepared" or "prepped"
by Arlon. The "prep" phase makes minor changes to the directory and manifest to help Arlon deploy
multiple copies of the cluster without naming conflicts.

## manifest directory preparation

To prepare a git directory to serve as base cluster, use the `basecluster preparegit` command:

```shell
arlon basecluster preparegit --repo-url <repoUrl> --repo-path <pathToDirectory> [--repo-revision revision]
            # OR
# using repository aliases
  # using the default alias
arlon basecluster preparegit --repo-path <pathToDirectory> [--repo-revision revision]
  # using the prod alias
arlon basecluster preparegit --repo-alias prod --repo-path <pathToDirectory> [--repo-revision revision]
```

## manifest directory validation

Post the successful preparation of the basecluster manifest directory using `basecluster preparegit`, the basecluster manifest directory needs to be validated before the basecluster is created.

To determine if a git directory is eligible to serve as base cluster, run the `basecluster validategit` command:

```shell
arlon basecluster validategit --repo-url <repoUrl> --repo-path <pathToDirectory> [--repo-revision revision]
            # OR
# using repository aliases
  # using the default alias
arlon basecluster validategit --repo-path <pathToDirectory> [--repo-revision revision]
  # using the prod alias
arlon basecluster validategit --repo-alias prod --repo-path <pathToDirectory> [--repo-revision revision]
```

## gen2 cluster creation

To create a gen2 workload cluster from the base cluster:

```shell
arlon cluster create --cluster-name <clusterName> --repo-url <repoUrl> --repo-path <pathToDirectory> [--output-yaml] [--profile <profileName>] [--repo-revision <repoRevision>]
            # OR
# using repository aliases
  # using the default alias
arlon cluster create --cluster-name <clusterName> --repo-path <pathToDirectory> [--output-yaml] [--profile <profileName>] [--repo-revision <repoRevision>]
  # using the prod alias
arlon cluster create --cluster-name <clusterName> --repo-alias prod --repo-path <pathToDirectory> [--output-yaml] [--profile <profileName>] [--repo-revision <repoRevision>]
```

## gen2 cluster update

To update the profiles of a gen2 workload cluster:

```shell
# To add a new profile to the existing cluster
arlon cluster ngupdate <clustername> --profile <profilename>
# To delete an existing profile from the existing cluster
arlon cluster ngupdate <clustername> --delete-profile <profilename>
```

A gen2 cluster can be created without any profile app associated with the cluster. So, the above commands can be used to add a new profile
to the existing cluster which will create profile app in argocd along with bundle apps associated with the profile.

An existing profile can be deleted from the cluster as well using the above command. Executing this command will delete the profile app and
all the bundles associated with the profile in argocd.

## gen2 cluster deletion

To destroy a gen2 workload cluster:

```shell
arlon cluster delete <clusterName>
```

Arlon creates between 2 and 3 ArgoCD application resources to compose a gen2 cluster (the 3rd application, called "profile app", is used when
an optional profile is specified at cluster creation time). When you destroy a gen2 cluster, Arlon will find all related ArgoCD applications
and clean them up.

## Known issues and limitations

Gen2 clusters are powerful because the base cluster can be arbitrarily complex and feature rich. Since they are fairly
new and still evolving, gen2 clusters have several known limitations relative to gen1.

* You cannot customize/override any property of the base cluster on the fly when creating a workload cluster,
  which is an exact clone of the base cluster except for the names of its resources and their namespace.
  The work-around is to make a copy of the base cluster directory, push the new directory, make
  the desired changes, commit & push the changes, and register the directory as a new base cluster.
* If you modify and commit a change to one or more properties of the base cluster that the underlying Cluster API provider deems as "immutable", new
  workload clusters created from the base cluster will have the modified propert(ies), but ArgoCD will flag existing clusters as OutOfSync, since
  the provider will continually reject attempts to apply the new property values. The existing clusters continue to function, despite appearing unhealthy
  in the ArgoCD UI and CLI outputs.

Examples of mutable properties in Cluster API resources:

* Number of replicas (modification will result in a scale-up / down)
* Kubernetes version (modification will result in an upgrade)

Examples of immutable properties:

* Most fields of AWSMachineTemplate (instance type, labels, etc...)

## For more information

For more details on gen2 clusters, refer to the [design document](baseclusters.md).
