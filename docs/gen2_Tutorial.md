# Next-Generation (gen2) Clusters - New in version 0.9.x

Gen1 clusters are limited in capability by the Helm chart used to deploy the infrastructure resources.
Advanced Cluster API configurations, such as those using multiple MachinePools, each with different
instance types, is not supported.

Gen2 clusters solve this problem by allowing you to create workload clusters from a *base cluster*
that you design and provide in the form of a manifest file stored in a git directory. The manifest
typically contains multiple related resources that together define an arbitrarily complex cluster.
If you make subsequent changes to the base cluster, workload clusters originally created from it
will automatically acquire the changes.

## Creating Cluster-API cluster manifest

Note: The CAPA version used here is v2.0 and the manifests created here are in accordance with this version.

Refer the [compatibility matrix for Cluster API provider and CAPA versions](https://github.com/kubernetes-sigs/cluster-api-provider-aws#compatibility-with-cluster-api-and-kubernetes-versions) for supported versions.

Before deploying a EKS cluster, make sure to setup the AWS Environment as stated in the [quickstart giude for CAPI](https://cluster-api.sigs.k8s.io/user/quick-start.html)

### MachineDeployment

Here is an example of a manifest file that we can use to create a *base cluster*. This manifest file helps in
deploying an EKS cluster with 'machine deployment' component from the cluster API (CAPI). This file has been generated by the following command

```shell
clusterctl generate cluster capi-quickstart --flavor eks \
  --kubernetes-version v1.24.0 \
  --control-plane-machine-count=3 \
  --worker-machine-count=3 \
  > capi-quickstart.yaml
```

```yaml
# YAML
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: capi-quickstart
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta2
    kind: AWSManagedControlPlane
    name: capi-quickstart-control-plane
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
    kind: AWSManagedCluster
    name: capi-quickstart
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSManagedCluster
metadata:
  name: capi-quickstart
spec: {}
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta2
kind: AWSManagedControlPlane
metadata:
  name: capi-quickstart-control-plane
  namespace: default
spec:
  region: {REGION}
  sshKeyName: {SSH_KEYNAME}
  version: v1.24.0
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: capi-quickstart-md-0
  namespace: default
spec:
  clusterName: capi-quickstart
  replicas: 3
  selector:
    matchLabels: null
  template:
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta2
          kind: EKSConfigTemplate
          name: capi-quickstart-md-0
      clusterName: capi-quickstart
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: AWSMachineTemplate
        name: capi-quickstart-md-0
      version: v1.24.0
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSMachineTemplate
metadata:
  name: capi-quickstart-md-0
  namespace: default
spec:
  template:
    spec:
      iamInstanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io
      instanceType: {INSTANCE_TYPE}
      sshKeyName: {SSH_KEYNAME}
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta2
kind: EKSConfigTemplate
metadata:
  name: capi-quickstart-md-0
  namespace: default
spec:
  template: {}
```

### AWSManagedMachinePool

Initialize the environment for AWSManagedMachinePool as stated [here](https://cluster-api-aws.sigs.k8s.io/topics/machinepools.html#awsmanagedmachinepool)

Before deploying an EKS cluster, make sure that the MachinePool feature gate is enabled. To do so, run this command:

```shell
kubectl describe deployment capa-controller-manager -n capa-system
```

In the output, in the feature gates section of the deployment, MachinePool must be set to true.

```shell
> kubectl describe deployment capa-controller-manager -n capa-system
..........
..........
--featuregates=EKS=true,EKSEnableIAM=false,EKSAllowAddRoles=false,EKSFargate=true,MachinePool=true,EventBridgeInstanceState=false,
AutoControllerIdentityCreator=true,BootstrapFormatIgnition=false,ExternalResourceGC=false
..........
..........
```

This manifest file helps in deploying an EKS cluster with 'AWSManagedMachinePool' component from the cluster API (CAPI). This file has been generated by the following command

```shell
clusterctl generate cluster awsmanaged-cluster --kubernetes-version v1.22.0 --flavor eks-managedmachinepool > manifest.yaml
```

```yaml
# YAML
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: awsmanaged-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta2
    kind: AWSManagedControlPlane
    name: awsmanaged-cluster-control-plane
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
    kind: AWSManagedCluster
    name: awsmanaged-cluster
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSManagedCluster
metadata:
  name: awsmanaged-cluster
  namespace: default
spec: {}
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta2
kind: AWSManagedControlPlane
metadata:
  name: awsmanaged-cluster-control-plane
  namespace: default
spec:
  region: {REGION}
  sshKeyName: {SSH_KEYNAME}
  version: v1.22.0
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachinePool
metadata:
  name: awsmanaged-cluster-pool-0
  namespace: default
spec:
  clusterName: awsmanaged-cluster
  replicas: 1
  template:
    spec:
      bootstrap:
        dataSecretName: ""
      clusterName: awsmanaged-cluster
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: AWSManagedMachinePool
        name: awsmanaged-cluster-pool-0
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSManagedMachinePool
metadata:
  name: awsmanaged-cluster-pool-0
  namespace: default
spec: {}
```

### AWSMachinePool

An AWSMachinePool corresponds to an AWS AutoScaling Groups, which provides the cloud provider specific resource for orchestrating a group of EC2 machines.

Initialize the environment for AWSMachinePool as stated [here](<https://cluster-api-aws.sigs.k8s.io/topics/machinepools.html#awsmachinepool>)

Before deploying an EKS cluster, make sure that the AWSMachinePool feature gate is enabled. To do so, run this command:

```shell
kubectl describe deployment capa-controller-manager -n capa-system
```

In the output, in the feature gates section of the deployment, MachinePool must be set to true.

```shell
> kubectl describe deployment capa-controller-manager -n capa-system
..........
..........
--featuregates=EKS=true,EKSEnableIAM=false,EKSAllowAddRoles=false,EKSFargate=true,MachinePool=true,EventBridgeInstanceState=false,
AutoControllerIdentityCreator=true,BootstrapFormatIgnition=false,ExternalResourceGC=false
..........
..........
```

This manifest file helps in deploying an EKS cluster with 'AWSManagedMachinePool' component from the cluster API (CAPI). This file has been generated by the following command

```shell
clusterctl generate cluster awsmanaged-cluster --kubernetes-version v1.22.0 --flavor eks-machinepool > manifest.yaml
```

```yaml
# YAML
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: awsmanaged-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 192.168.0.0/16
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta2
    kind: AWSManagedControlPlane
    name: awsmanaged-cluster-control-plane
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
    kind: AWSManagedCluster
    name: awsmanaged-cluster
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSManagedCluster
metadata:
  name: awsmanaged-cluster
  namespace: default
spec: {}
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta2
kind: AWSManagedControlPlane
metadata:
  name: awsmanaged-cluster-control-plane
  namespace: default
spec:
  region: {REGION}
  sshKeyName: {SSH_KEYNAME}
  version: v1.22.0
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachinePool
metadata:
  name: awsmanaged-cluster-mp-0
  namespace: default
spec:
  clusterName: awsmanaged-cluster
  replicas: 1
  template:
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta2
          kind: EKSConfig
          name: awsmanaged-cluster-mp-0
      clusterName: awsmanaged-cluster
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: AWSMachinePool
        name: awsmanaged-cluster-mp-0
      version: v1.22.0
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSMachinePool
metadata:
  name: awsmanaged-cluster-mp-0
  namespace: default
spec:
  awsLaunchTemplate:
    iamInstanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io
    instanceType: {INSTANCE_TYPE}
    sshKeyName: {SSH_KEYNAME}
  maxSize: 10
  minSize: 1
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta2
kind: EKSConfig
metadata:
  name: awsmanaged-cluster-mp-0
  namespace: default
spec: {}
```

## gen2 cluster creation using Arlon

This manifest file needs to be pushed to the workspace repository before the manifest directory is prepped and then validated.

Before a manifest directory can be used as a base cluster, it must first be "prepared" or "prepped"
by Arlon. The "prep" phase makes minor changes to the directory and manifest to help Arlon deploy
multiple copies of the cluster without naming conflicts.

## manifest directory preparation

To prepare a git directory to serve as base cluster, use the `basecluster preparegit` command:

```shell
arlon basecluster preparegit --repo-url <repoUrl> --repo-path <pathToDirectory> [--repo-revision revision]
            # OR
# using repository aliases
  # using the default alias
arlon basecluster preparegit --repo-path <pathToDirectory> [--repo-revision revision]
  # using the prod alias
arlon basecluster preparegit --repo-alias prod --repo-path <pathToDirectory> [--repo-revision revision]
```

## manifest directory validation

Post the successful preparation of the basecluster manifest directory using `basecluster preparegit`, the basecluster manifest directory needs to be validated before the basecluster is created.

To determine if a git directory is eligible to serve as base cluster, run the `basecluster validategit` command:

```shell
arlon basecluster validategit --repo-url <repoUrl> --repo-path <pathToDirectory> [--repo-revision revision]
            # OR
# using repository aliases
  # using the default alias
arlon basecluster validategit --repo-path <pathToDirectory> [--repo-revision revision]
  # using the prod alias
arlon basecluster validategit --repo-alias prod --repo-path <pathToDirectory> [--repo-revision revision]
```

## gen2 cluster creation

**Note: Base clusters only support dynamic profiles.**

To create a gen2 workload cluster from the base cluster:

```shell
arlon cluster create --cluster-name <clusterName> --repo-url <repoUrl> --repo-path <pathToDirectory> [--output-yaml] [--profile <profileName>] [--repo-revision <repoRevision>]
            # OR
# using repository aliases
  # using the default alias
arlon cluster create --cluster-name <clusterName> --repo-path <pathToDirectory> [--output-yaml] [--profile <profileName>] [--repo-revision <repoRevision>]
  # using the prod alias
arlon cluster create --cluster-name <clusterName> --repo-alias prod --repo-path <pathToDirectory> [--output-yaml] [--profile <profileName>] [--repo-revision <repoRevision>]
```


## gen2 cluster creation with overrides

We call the concept of constructing various clusters with patches from the same base manifest as cluster overrides. 
The cluster overrides feature is built on top of the existing base cluster design. So, A user can create a cluster from the base manifest using the same command as in the above step(gen2 cluster creation).
Now, to create a cluster with overrides in the base manifest, a user should have the corresponding patch files in a dedicated folder in local which doesn't contain any other files except patch files. Example of a patch file where we want to override replicas count to 2 is:

```shell
  ---
  apiVersion: cluster.x-k8s.io/v1beta1
  kind: MachineDeployment
  metadata:
    name: .*
  spec:
    replicas: 2
  ```

Refer to this [document](https://blog.scottlowe.org/2019/11/12/using-kustomize-with-cluster-api-manifests/) to know more about patch files

Command to create a gen2 workload cluster form the base cluster manifest with overrides to the manifest is:

```shell
arlon cluster create <cluster-name> --repo-url <repo url where base manifest is present> --repo-path <repo path to the base manifest> --override <path to the patch files folder> --patch-repo-url <repo url where patch files should be stored> --patch-repo-path <repo path to store the patch files>
````
Runnning the above command will create a cluster named folder in patch repo path of patch repo url which contains the patch files, kustomization.yaml and configurations.yaml which are used to create the cluster app.

Note that the patch file repo url can be different or same from the base manifest repo url acoording to the requirement of the user. A user can use a different repo url for string patch files for the cluster.

## gen2 cluster update

To update the profiles of a gen2 workload cluster:

```shell
# To add a new profile to the existing cluster
arlon cluster ngupdate <clustername> --profile <profilename>
# To delete an existing profile from the existing cluster
arlon cluster ngupdate <clustername> --delete-profile
```

A gen2 cluster can be created without any profile app associated with the cluster. So, the above commands can be used to add a new profile
to the existing cluster which will create profile app in argocd along with bundle apps associated with the profile.

An existing profile can be deleted from the cluster as well using the above command. Executing this command will delete the profile app and
all the bundles associated with the profile in argocd.

## gen2 cluster deletion

To destroy a gen2 workload cluster:

```shell
arlon cluster delete <clusterName>
```

Arlon creates between 2 and 3 ArgoCD application resources to compose a gen2 cluster (the 3rd application, called "profile app", is used when
an optional profile is specified at cluster creation time). When you destroy a gen2 cluster, Arlon will find all related ArgoCD applications
and clean them up.

If the cluster which which is being deleted is a cluster created using patch files, the controller first cleans the git repo where the respective patch files of the cluster are present and then it destroys all the related ArgoCD applications and clean them up.

## Enabling Cluster Autoscaler in the workload cluster
To create a gen2 cluster with autoscaler, we need:

- bundle pointing to the bundle/capi-cluster-autoscaler in the arlon repository.
- dynamic profile that contains the above bundle.
- a cluster template manifest(that makes use of MachineDeployment and not MachinePools) which has the CAPI annotations for min and max nodes set ( as a part of `preparegit` or manually add it ).
- run `arlon cluster create` with the repo-path pointing to the cluster template manifest described in the step above, set the profile to  be the one created in step 2 and pass the `autoscaler` flag.

### Bundle creation

Register a dynamic bundle pointing to the bundles/capi-cluster-autoscaler in the Arlon repo.

To enable the cluster-autoscaler bundle, add one more parameter during cluster creation: `srcType`. This is the ArgoCD-defined application source type (Helm, Kustomize, Directory). In addition to this, the `repo-revision` parameter should also be set to a stable arlon release branch ( in this case v0.9 ).

This example creates a bundle pointing to the bundles/capi-cluster-autoscaler in Arlon repo

```shell
arlon bundle create cas-bundle --tags cas,devel,test --desc "CAS Bundle" --repo-url https://github.com/arlonproj/arlon.git --repo-path bundles/capi-cluster-autoscaler --srctype helm --repo-revision v0.9
```

### Profile creation

Create a profile that contains this capi-cluster-autoscaler bundle.

```shell
arlon profile create dynamic-cas --repo-url <repoUrl> --repo-base-path profiles --bundles cas-bundle --desc "dynamic cas profile" --tags examples
```

### manifest directory preparation

Two additional properties `cas-max` and `cas-min` are used to set 2 annotations for Max/Min nodes on MachineDeployment required by the cluster autoscaler for CAPI as a part of the manifest directory preparation. These are the annotations required by the MachineDeployment for autoscaling. 

Note: These are the default values for the `cas-min` and `cas-max` properties
```shell
 annotations:
     cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: '1'
     cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: '9'
```

These annotations are added during the preparegit step. If these annotations are already present in the manifest file, then they will not be added again as a part of `preparegit`

Preparing the git directory to serve as the cluster template: 

```shell
arlon basecluster preparegit --repo-path <pathToDirectory> --cas-min 1 --cas-max 9 --repo-url <repoUrl> 
```

### manifest directory validation

To determine if a git directory is eligible to serve as cluster template, run the `basecluster validategit` command:

```shell
arlon basecluster validategit --repo-path <pathToDirectory> --repo-url <repoUrl> 
```

### gen2 cluster creation with autoscaling enabled

To add CAS support for gen2 clusters, the cluster create sub-command of the arlon CLI has a `autoscaler` flag which deploys the capi-cluster-autoscaler helm chart on the management cluster.

To create a gen2 workload with a cluster-autoscaler pod running, from the cluster template, run this command:

Note: Use the dynamic profile that was created in the previous steps

```shell
arlon cluster create --cluster-name <clusterName> --repo-url <repoUrl> --repo-path <pathToDirectory> --profile dynamic-cas --autoscaler
```

## Known issues and limitations

Gen2 clusters are powerful because the base cluster can be arbitrarily complex and feature rich. Since they are fairly
new and still evolving, gen2 clusters have several known limitations relative to gen1.

* You cannot customize/override any property of the base cluster on the fly when creating a workload cluster,
  which is an exact clone of the base cluster except for the names of its resources and their namespace.
  The work-around is to make a copy of the base cluster directory, push the new directory, make
  the desired changes, commit & push the changes, and register the directory as a new base cluster.
* The clusters created directly from the base manifest are completely declarative whereas the clusters which are created using override property are not completely declarative.
* If a user passes a different repository for patch repo url from the repo where base manifest is present, argocd won't be able to detect if there are any changes in the base manifest repository but will deect all the chnages in patch repo url for the cluster.
* If you modify and commit a change to one or more properties of the base cluster that the underlying Cluster API provider deems as "immutable", new
  workload clusters created from the base cluster will have the modified propert(ies), but ArgoCD will flag existing clusters as OutOfSync, since
  the provider will continually reject attempts to apply the new property values. The existing clusters continue to function, despite appearing unhealthy
  in the ArgoCD UI and CLI outputs.

Examples of mutable properties in Cluster API resources:

* Number of replicas (modification will result in a scale-up / down)
* Kubernetes version (modification will result in an upgrade)

Examples of immutable properties:

* Most fields of AWSMachineTemplate (instance type, labels, etc...)


## For more information

For more details on gen2 clusters, refer to the [design document](baseclusters.md).
