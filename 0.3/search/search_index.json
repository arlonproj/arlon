{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 \u00b6 What Is Arlon? \u00b6 Arlon is a declarative, gitops based fleet management tool for Kubernetes clusters. It allows administrators to: Deploy and upgrade a large number of workload clusters Secure clusters by installing and configuring policies Install a set of applications / add-ons on the clusters all in a structured, predictable manner. Arlon makes Kubernetes cluster fleet management secure, version controlled, auditable and easy to perform at scale. Arlon takes advantage of multiple declarative cluster management API providers for the actual cluster orchestration. The first two supported API providers are Cluster API and Crossplane. Arlon uses ArgoCD as the underlying Kubernetes manifest deployment and enforcement engine. A workload cluster is composed of the following constructs: Cluster spec : a description of the infrastructure and external settings of a cluster, e.g. Kubernetes version, cloud provider, cluster type, node instance type. Profile : a grouping of configuration bundles which will be installed into the cluster Configuration bundle : a unit of configuration which contains (or references) one or more Kubernetes manifests. A bundle can encapsulate anything that can be deployed onto a cluster: an RBAC ruleset, an add-on, an application, etc... Arlon Benefits \u00b6 Improves time to market by enabling better velocity for developers through infrastructure management that is more fluid and agile. Define, store, change and enforce your cluster infrastructure & application add-ons at scale. Reduces the risk of unexpected infrastructure downtime and outages, or unexpected security misconfiguration, with consistent management of infrastructure and security policies. Allows IT and Platform Ops admins to operate large scale of clusters, infrastructure & add-ons with significantly reduced team size & operational overhead, using GitOps.","title":"Overview"},{"location":"#introduction","text":"","title":"Introduction"},{"location":"#_1","text":"","title":""},{"location":"#what-is-arlon","text":"Arlon is a declarative, gitops based fleet management tool for Kubernetes clusters. It allows administrators to: Deploy and upgrade a large number of workload clusters Secure clusters by installing and configuring policies Install a set of applications / add-ons on the clusters all in a structured, predictable manner. Arlon makes Kubernetes cluster fleet management secure, version controlled, auditable and easy to perform at scale. Arlon takes advantage of multiple declarative cluster management API providers for the actual cluster orchestration. The first two supported API providers are Cluster API and Crossplane. Arlon uses ArgoCD as the underlying Kubernetes manifest deployment and enforcement engine. A workload cluster is composed of the following constructs: Cluster spec : a description of the infrastructure and external settings of a cluster, e.g. Kubernetes version, cloud provider, cluster type, node instance type. Profile : a grouping of configuration bundles which will be installed into the cluster Configuration bundle : a unit of configuration which contains (or references) one or more Kubernetes manifests. A bundle can encapsulate anything that can be deployed onto a cluster: an RBAC ruleset, an add-on, an application, etc...","title":"What Is Arlon?"},{"location":"#arlon-benefits","text":"Improves time to market by enabling better velocity for developers through infrastructure management that is more fluid and agile. Define, store, change and enforce your cluster infrastructure & application add-ons at scale. Reduces the risk of unexpected infrastructure downtime and outages, or unexpected security misconfiguration, with consistent management of infrastructure and security policies. Allows IT and Platform Ops admins to operate large scale of clusters, infrastructure & add-ons with significantly reduced team size & operational overhead, using GitOps.","title":"Arlon Benefits"},{"location":"contributing/","text":"How to contribute to Arlon \u00b6 Team Arlon welcomes and encourages everyone to participate in its development via pull requests on GitHub. We prefer to take in pull requests to our active development branch i.e. the main branch. To report a bug or request a feature, we rely on GitHub issues. There are a number of points to keep in mind when submitting a feature request, reporting a bug or contributing in the development of Arlon. Before making a feature request, or reporting a bug please browse through the existing open issues to be sure that it hasn't been already tracked. If a feature request is subsumed by some other open issue, please add your valuable feedback as a comment to the issue. If a bug discovered by you is already being tracked, please provide additional information as you see fit(steps to reproduce, particulars of the environment, version information etc.) as a comment. Before submitting code for a new feature(or a complex, untracked bugfix) please create a new issue. This issue needs to undergo a review process which may involve a discussion on the same GitHub issue to discuss possible approaches and motivation for the said proposal. Please reach out to us on Slack for discussions, help, questions and the roadmap. Code changes \u00b6 Open a pull request (PR) on GitHub following the typical GitHub workflow here . Most of the new code changes are merged to the main branch except backports, bookkeeping changes, library upgrades and some bugs that manifest only a particular version. Before contributing new code, contributors are encouraged to either write unit tests, e2e tests or perform some form of manual validation as a sanity-check. Please adhere to standard good practices for Golang and do ensure that the code is properly formatted and vet succeeds, for which we have fmt and vet targets respectively. Since Arlon is a growing project, various areas require improvements- improving code coverage with unit tests, e2e tests (since v0.10), documentation, CI/CD pipelines using GitHub Actions are a few to name, we highly encourage to contribute to those areas to start with. Issues / Bug reports \u00b6 We track issues on GitHub . You are encouraged to browse through these, add relevant feedback, create new issues or participate in the development. If you are interested in a particular issue or feature request, please leave a comment to reach out to the team. In particular, the issues labeled as help wanted are a great starting point for adding code changes to the project. Documentation \u00b6 The documentation for Arlon is hosted on Read the Docs and comprises of contents from the \"docs\" directory of Arlon source. For making changes to the documentation, please follow the below steps: Fork the Arlon repository on GitHub and make the desired changes. Prerequisites Ensure that python3 , pip3 is installed. Optionally, create a venv by running python3 -m venv ./venv to create a virtual environment if you don't have one. From the root of the Arlon repository, run pip3 install -r docs/requirements.txt to install mkdocs and other pre-requisites. To test your local changes, run mike serve from the repository root. This starts a local server to host the documentation website where you can preview the changes. To publish the changes, just push the changes to your fork repository and open a PR (pull request). Once your PR is accepted by one of the maintainers/ owners of Arlon project, the Arlon website will be updated.","title":"How to contribute to Arlon"},{"location":"contributing/#how-to-contribute-to-arlon","text":"Team Arlon welcomes and encourages everyone to participate in its development via pull requests on GitHub. We prefer to take in pull requests to our active development branch i.e. the main branch. To report a bug or request a feature, we rely on GitHub issues. There are a number of points to keep in mind when submitting a feature request, reporting a bug or contributing in the development of Arlon. Before making a feature request, or reporting a bug please browse through the existing open issues to be sure that it hasn't been already tracked. If a feature request is subsumed by some other open issue, please add your valuable feedback as a comment to the issue. If a bug discovered by you is already being tracked, please provide additional information as you see fit(steps to reproduce, particulars of the environment, version information etc.) as a comment. Before submitting code for a new feature(or a complex, untracked bugfix) please create a new issue. This issue needs to undergo a review process which may involve a discussion on the same GitHub issue to discuss possible approaches and motivation for the said proposal. Please reach out to us on Slack for discussions, help, questions and the roadmap.","title":"How to contribute to Arlon"},{"location":"contributing/#code-changes","text":"Open a pull request (PR) on GitHub following the typical GitHub workflow here . Most of the new code changes are merged to the main branch except backports, bookkeeping changes, library upgrades and some bugs that manifest only a particular version. Before contributing new code, contributors are encouraged to either write unit tests, e2e tests or perform some form of manual validation as a sanity-check. Please adhere to standard good practices for Golang and do ensure that the code is properly formatted and vet succeeds, for which we have fmt and vet targets respectively. Since Arlon is a growing project, various areas require improvements- improving code coverage with unit tests, e2e tests (since v0.10), documentation, CI/CD pipelines using GitHub Actions are a few to name, we highly encourage to contribute to those areas to start with.","title":"Code changes"},{"location":"contributing/#issues-bug-reports","text":"We track issues on GitHub . You are encouraged to browse through these, add relevant feedback, create new issues or participate in the development. If you are interested in a particular issue or feature request, please leave a comment to reach out to the team. In particular, the issues labeled as help wanted are a great starting point for adding code changes to the project.","title":"Issues / Bug reports"},{"location":"contributing/#documentation","text":"The documentation for Arlon is hosted on Read the Docs and comprises of contents from the \"docs\" directory of Arlon source. For making changes to the documentation, please follow the below steps: Fork the Arlon repository on GitHub and make the desired changes. Prerequisites Ensure that python3 , pip3 is installed. Optionally, create a venv by running python3 -m venv ./venv to create a virtual environment if you don't have one. From the root of the Arlon repository, run pip3 install -r docs/requirements.txt to install mkdocs and other pre-requisites. To test your local changes, run mike serve from the repository root. This starts a local server to host the documentation website where you can preview the changes. To publish the changes, just push the changes to your fork repository and open a PR (pull request). Once your PR is accepted by one of the maintainers/ owners of Arlon project, the Arlon website will be updated.","title":"Documentation"},{"location":"design/","text":"Arlon Design and Concepts \u00b6 Management cluster \u00b6 This Kubernetes cluster hosts the following components: - ArgoCD - Arlo - Cluster management stacks e.g. Cluster API and/or Crossplane The Arlo state and controllers reside in the arlo namespace. Configuration bundle \u00b6 A configuration bundle (or just \"bundle\") is grouping of data files that produce a set of Kubernetes manifests via a tool . This closely follows ArgoCD's definition of tool types . Consequently, the list of supported bundle types mirrors ArgoCD's supported set of manifest-producing tools. Each bundle is defined using a Kubernetes ConfigMap resource in the arlo namespace. Additionally, a bundle can embed the data itself (\"static bundle\"), or contain a reference to the data (\"dynamic bundle\"). A reference can be a URL, github location, or Helm repo location. The current list of supported bundle types is: manifest_inline: a single manifest yaml file embedded in the resource manifest_ref: a reference to a single manifest yaml file dir_inline: an embedded tarball that expands to a directory of YAML files helm_inline: an embedded Helm chart package helm_ref: an external reference to a Helm chart Bundle purpose \u00b6 Bundles can specify an optional purpose to help classify and organize them. In the future, Arlo may order bundle installation by purpose order (for e.g. install bundles with purpose= networking before others) but that is not the case today. The currenty suggested purpose values are: - networking - add-on - data-service - application Cluster specification \u00b6 A cluster specification contains desired settings when creating a new cluster. They currently include: - Stack: the cluster provisioning stack, for e.g. cluster-api or crossplane - Provider: the specific cluster management provider under that stack, if applicable. Example: for cluster-api , the possible values are eks and kubeadm - Other settings that specify the \"shape\" of the cluster, such as the size of the control plane and the initial number of nodes of the data plane. - The pod networking technology (under discussion: this may be moved to a bundle because most if not all CNI providers can be installed as manifests) Profile \u00b6 A profile expresses a desired configuration for a Kubernetes cluster. It is composed of - An optional Cluster Specification. If specified, it allows the profile to be used to create new clusters. If absent, the profile can only be applied to existing clusters. - A list of bundles specifying the configuration to apply onto the cluster once it is operational - An optional list of value.yaml settings for any Helm Chart type bundle in the bundle list Cluster chart \u00b6 The cluster chart is a Helm chart that creates (and optionally applies) the manifests necessary to create a cluster and deploy desired configurations and applications to it. When a user uses Arlo to create and configure a cluster, he or she specifies a profile. The profile's cluster specification, bundle list and other settings are used to generate values for the chart, and the chart is deployed as a Helm release into the arlo namespace in the management cluster. Here is a summary of the kinds of resources generated and deployed by the chart: - A unique namespace with a name based on the cluster's name. All subsequent resources below are created inside of that namespace. - The stack-specific resources to create the cluster (for e.g. Cluster API resources) - A ClusterRegistration to automatically register the cluster with ArgoCD - A GitRepoDir to automatically create a git repo and/or directory to host a copy of the expanded bundles. Every bundle referenced by the profile is copied/unpacked into its own subdirectory. - One ArgoCD Application resource for each bundle.","title":"Design"},{"location":"design/#arlon-design-and-concepts","text":"","title":"Arlon Design and Concepts"},{"location":"design/#management-cluster","text":"This Kubernetes cluster hosts the following components: - ArgoCD - Arlo - Cluster management stacks e.g. Cluster API and/or Crossplane The Arlo state and controllers reside in the arlo namespace.","title":"Management cluster"},{"location":"design/#configuration-bundle","text":"A configuration bundle (or just \"bundle\") is grouping of data files that produce a set of Kubernetes manifests via a tool . This closely follows ArgoCD's definition of tool types . Consequently, the list of supported bundle types mirrors ArgoCD's supported set of manifest-producing tools. Each bundle is defined using a Kubernetes ConfigMap resource in the arlo namespace. Additionally, a bundle can embed the data itself (\"static bundle\"), or contain a reference to the data (\"dynamic bundle\"). A reference can be a URL, github location, or Helm repo location. The current list of supported bundle types is: manifest_inline: a single manifest yaml file embedded in the resource manifest_ref: a reference to a single manifest yaml file dir_inline: an embedded tarball that expands to a directory of YAML files helm_inline: an embedded Helm chart package helm_ref: an external reference to a Helm chart","title":"Configuration bundle"},{"location":"design/#bundle-purpose","text":"Bundles can specify an optional purpose to help classify and organize them. In the future, Arlo may order bundle installation by purpose order (for e.g. install bundles with purpose= networking before others) but that is not the case today. The currenty suggested purpose values are: - networking - add-on - data-service - application","title":"Bundle purpose"},{"location":"design/#cluster-specification","text":"A cluster specification contains desired settings when creating a new cluster. They currently include: - Stack: the cluster provisioning stack, for e.g. cluster-api or crossplane - Provider: the specific cluster management provider under that stack, if applicable. Example: for cluster-api , the possible values are eks and kubeadm - Other settings that specify the \"shape\" of the cluster, such as the size of the control plane and the initial number of nodes of the data plane. - The pod networking technology (under discussion: this may be moved to a bundle because most if not all CNI providers can be installed as manifests)","title":"Cluster specification"},{"location":"design/#profile","text":"A profile expresses a desired configuration for a Kubernetes cluster. It is composed of - An optional Cluster Specification. If specified, it allows the profile to be used to create new clusters. If absent, the profile can only be applied to existing clusters. - A list of bundles specifying the configuration to apply onto the cluster once it is operational - An optional list of value.yaml settings for any Helm Chart type bundle in the bundle list","title":"Profile"},{"location":"design/#cluster-chart","text":"The cluster chart is a Helm chart that creates (and optionally applies) the manifests necessary to create a cluster and deploy desired configurations and applications to it. When a user uses Arlo to create and configure a cluster, he or she specifies a profile. The profile's cluster specification, bundle list and other settings are used to generate values for the chart, and the chart is deployed as a Helm release into the arlo namespace in the management cluster. Here is a summary of the kinds of resources generated and deployed by the chart: - A unique namespace with a name based on the cluster's name. All subsequent resources below are created inside of that namespace. - The stack-specific resources to create the cluster (for e.g. Cluster API resources) - A ClusterRegistration to automatically register the cluster with ArgoCD - A GitRepoDir to automatically create a git repo and/or directory to host a copy of the expanded bundles. Every bundle referenced by the profile is copied/unpacked into its own subdirectory. - One ArgoCD Application resource for each bundle.","title":"Cluster chart"},{"location":"help/","text":"Help with MkDocs \u00b6 Welcome to Arlon documentation with MkDocs For full documentation visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Help with MkDocs"},{"location":"help/#help-with-mkdocs","text":"Welcome to Arlon documentation with MkDocs For full documentation visit mkdocs.org .","title":"Help with MkDocs"},{"location":"help/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"help/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"installation/","text":"Installation \u00b6 Arlon CLI downloads are provided on GitHub. The CLI is not a self-contained standalone executable though. It is required to point the CLI to a management cluster and set up the Arlon controller in this management cluster. Customised Setup \u00b6 Please follow the instructions in this section for a customised setup that includes installation of ArgoCD, Arlon CLI and Arlon controller. Management cluster \u00b6 As a prerequisite, you need a Kubernetes cluster as management cluster for Arlon. You can use any Kubernetes cluster that you have admin access to. Ensure: kubectl is in your path KUBECONFIG is pointing to the right file and the context set properly ArgoCD \u00b6 Follow steps 1-4 of the ArgoCD installation guide to install ArgoCD onto your management cluster. After this step, you should be logged in as admin and a config file was created at ${HOME}/.config/argocd/config Create your workspace repository in your git provider if necessary, then register it. Example: argocd repo add https://github.com/myname/arlon_workspace --username myname --password secret . -- Note: type argocd repo add --help to see all available options. -- For Arlon developers, this is not your fork of the Arlon source code repository, but a separate git repo where some artifacts like profiles created by Arlon will be stored. Highly recommended: configure a webhook to immediately notify ArgoCD of changes to the repo. This will be especially useful during the tutorial. Without a webhook, repo changes may take up to 3 minutes to be detected, delaying cluster configuration updates. Create a local user named arlon with the apiKey capability. This involves editing the argocd-cm ConfigMap using kubectl . Adjust the RBAC settings to grant admin permissions to the arlon user. This involves editing the argocd-rbac-cm ConfigMap to add the entry g, arlon, role:admin under the policy.csv section. Example: apiVersion : v1 data : policy.csv : | g, arlon, role:admin kind : ConfigMap [ ... ] Generate an account token: argocd account generate-token --account arlon Make a temporary copy of this config-file in /tmp/config then edit it to replace the value of auth-token with the token from the previous step. Save changes. This file will be used to configure the Arlon controller's ArgoCD credentials during the next steps. Arlon controller \u00b6 Create the arlon namespace: kubectl create ns arlon Create the ArgoCD credentials secret from the temporary config file: kubectl -n arlon create secret generic argocd-creds --from-file /tmp/config Delete the temporary config file Clone the arlon git repo and cd to its top directory Create the CRDs: kubectl apply -f config/crd/bases/ Deploy the controller: kubectl apply -f deploy/manifests/ Ensure the controller eventually enters the Running state: watch kubectl -n arlon get pod Arlon CLI \u00b6 Download the CLI for the latest release from GitHub. Currently, Linux and MacOS operating systems are supported. Uncompress the tarball, rename it as arlon and add to your PATH Run arlon verify to check for prerequisites. Run arlon install to install any missing prerequisites. The following instructions are to manually build CLI from this code repository. Building the CLI \u00b6 Clone this repository and pull the latest version of a branch (main by default) From the top directory, run make build Optionally create a symlink from a directory (e.g. /usr/local/bin ) included in your ${PATH} to the bin/arlon binary to make it easy to invoke the command. Cluster orchestration API providers \u00b6 Arlon currently supports Cluster API on AWS cloud. It also has experimental support for Crossplane on AWS. Cluster API \u00b6 Using the Cluster API Quickstart Guide as reference, complete these steps: Install clusterctl Initialize the management cluster. In particular, follow instructions for your specific cloud provider (AWS in this example) Ensure clusterctl init completes successfully and produces the expected output. Crossplane (experimental) \u00b6 Using the Upbound AWS Reference Platform Quickstart Guide as reference, complete these steps: Install UXP on your management cluster Install Crossplane kubectl extension Install the platform configuration Configure the cloud provider credentials You do not need to go any further, but you're welcome to try the Network Fabric example. FYI: we noticed the dozens/hundreds of CRDs that Crossplane installs in the management cluster can noticeably slow down kubectl, and you may see a warning that looks like : I0222 17 :31:14.112689 27922 request.go:668 ] Waited for 1 .046146023s due to client-side throttling, not priority and fairness, request: GET:https://AA61XXXXXXXXXXX.gr7.us-west-2.eks.amazonaws.com/apis/servicediscovery.aws.crossplane.io/v1alpha1?timeout = 32s Automatic setup \u00b6 Starting from version 0.10 (v0.10), Arlon CLI provides an init command to install \"itself\" on a management cluster. This command performs a basic setup of argocd(if needed) and arlon controller. Refer documentation for v0.10+ for the details.","title":"Installation"},{"location":"installation/#installation","text":"Arlon CLI downloads are provided on GitHub. The CLI is not a self-contained standalone executable though. It is required to point the CLI to a management cluster and set up the Arlon controller in this management cluster.","title":"Installation"},{"location":"installation/#customised-setup","text":"Please follow the instructions in this section for a customised setup that includes installation of ArgoCD, Arlon CLI and Arlon controller.","title":"Customised Setup"},{"location":"installation/#management-cluster","text":"As a prerequisite, you need a Kubernetes cluster as management cluster for Arlon. You can use any Kubernetes cluster that you have admin access to. Ensure: kubectl is in your path KUBECONFIG is pointing to the right file and the context set properly","title":"Management cluster"},{"location":"installation/#argocd","text":"Follow steps 1-4 of the ArgoCD installation guide to install ArgoCD onto your management cluster. After this step, you should be logged in as admin and a config file was created at ${HOME}/.config/argocd/config Create your workspace repository in your git provider if necessary, then register it. Example: argocd repo add https://github.com/myname/arlon_workspace --username myname --password secret . -- Note: type argocd repo add --help to see all available options. -- For Arlon developers, this is not your fork of the Arlon source code repository, but a separate git repo where some artifacts like profiles created by Arlon will be stored. Highly recommended: configure a webhook to immediately notify ArgoCD of changes to the repo. This will be especially useful during the tutorial. Without a webhook, repo changes may take up to 3 minutes to be detected, delaying cluster configuration updates. Create a local user named arlon with the apiKey capability. This involves editing the argocd-cm ConfigMap using kubectl . Adjust the RBAC settings to grant admin permissions to the arlon user. This involves editing the argocd-rbac-cm ConfigMap to add the entry g, arlon, role:admin under the policy.csv section. Example: apiVersion : v1 data : policy.csv : | g, arlon, role:admin kind : ConfigMap [ ... ] Generate an account token: argocd account generate-token --account arlon Make a temporary copy of this config-file in /tmp/config then edit it to replace the value of auth-token with the token from the previous step. Save changes. This file will be used to configure the Arlon controller's ArgoCD credentials during the next steps.","title":"ArgoCD"},{"location":"installation/#arlon-controller","text":"Create the arlon namespace: kubectl create ns arlon Create the ArgoCD credentials secret from the temporary config file: kubectl -n arlon create secret generic argocd-creds --from-file /tmp/config Delete the temporary config file Clone the arlon git repo and cd to its top directory Create the CRDs: kubectl apply -f config/crd/bases/ Deploy the controller: kubectl apply -f deploy/manifests/ Ensure the controller eventually enters the Running state: watch kubectl -n arlon get pod","title":"Arlon controller"},{"location":"installation/#arlon-cli","text":"Download the CLI for the latest release from GitHub. Currently, Linux and MacOS operating systems are supported. Uncompress the tarball, rename it as arlon and add to your PATH Run arlon verify to check for prerequisites. Run arlon install to install any missing prerequisites. The following instructions are to manually build CLI from this code repository.","title":"Arlon CLI"},{"location":"installation/#building-the-cli","text":"Clone this repository and pull the latest version of a branch (main by default) From the top directory, run make build Optionally create a symlink from a directory (e.g. /usr/local/bin ) included in your ${PATH} to the bin/arlon binary to make it easy to invoke the command.","title":"Building the CLI"},{"location":"installation/#cluster-orchestration-api-providers","text":"Arlon currently supports Cluster API on AWS cloud. It also has experimental support for Crossplane on AWS.","title":"Cluster orchestration API providers"},{"location":"installation/#cluster-api","text":"Using the Cluster API Quickstart Guide as reference, complete these steps: Install clusterctl Initialize the management cluster. In particular, follow instructions for your specific cloud provider (AWS in this example) Ensure clusterctl init completes successfully and produces the expected output.","title":"Cluster API"},{"location":"installation/#crossplane-experimental","text":"Using the Upbound AWS Reference Platform Quickstart Guide as reference, complete these steps: Install UXP on your management cluster Install Crossplane kubectl extension Install the platform configuration Configure the cloud provider credentials You do not need to go any further, but you're welcome to try the Network Fabric example. FYI: we noticed the dozens/hundreds of CRDs that Crossplane installs in the management cluster can noticeably slow down kubectl, and you may see a warning that looks like : I0222 17 :31:14.112689 27922 request.go:668 ] Waited for 1 .046146023s due to client-side throttling, not priority and fairness, request: GET:https://AA61XXXXXXXXXXX.gr7.us-west-2.eks.amazonaws.com/apis/servicediscovery.aws.crossplane.io/v1alpha1?timeout = 32s","title":"Crossplane (experimental)"},{"location":"installation/#automatic-setup","text":"Starting from version 0.10 (v0.10), Arlon CLI provides an init command to install \"itself\" on a management cluster. This command performs a basic setup of argocd(if needed) and arlon controller. Refer documentation for v0.10+ for the details.","title":"Automatic setup"},{"location":"tutorial/","text":"Tutorial (gen1) \u00b6 This assumes that you plan to deploy workload clusters on AWS cloud, with Cluster API (\"CAPI\") as the cluster orchestration API provider. Also ensure you have set up a workspace repository , and it is registered as a git repo in ArgoCD. The tutorial will assume the existence of these environment variables: ${ARLON_REPO} : where the arlon repo is locally checked out ${WORKSPACE_REPO} : where the workspace repo is locally checked out ${WORKSPACE_REPO_URL} : the workspace repo's git URL. It typically looks like https://github.com/${username}/${reponame}.git ${CLOUD_REGION} : the region where you want to deploy example clusters and workloads (e.g. us-west-2) ${SSH_KEY_NAME} : the name of a public ssh key name registered in your cloud account, to enable ssh to your cluster nodes Additionally, for examples assuming arlon git register , \"default\" and a \"prod\" git repo aliases will also be given. _Note: for the best experience, make sure your workspace repo is configured to send change notifications to ArgoCD via a webhook. See the Installation section for details. Cluster specs \u00b6 We first create a few cluster specs with different combinations of API providers and cluster types (kubeadm vs EKS). One of the cluster specs is for an unconfigured API provider (Crossplane); this is for illustrative purposes, since we will not use it in this tutorial. arlon clusterspec create capi-kubeadm-3node --api capi --cloud aws --type kubeadm --kubeversion v1.21.10 --nodecount 3 --nodetype t2.medium --tags devel,test --desc \"3 node kubeadm for dev/test\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } arlon clusterspec create capi-eks --api capi --cloud aws --type eks --kubeversion v1.21.10 --nodecount 2 --nodetype t2.large --tags staging --desc \"2 node eks for general purpose\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } arlon clusterspec create xplane-eks-3node --api xplane --cloud aws --type eks --kubeversion v1.21.10 --nodecount 4 --nodetype t2.small --tags experimental --desc \"4 node eks managed by crossplane\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } Ensure you can now list the cluster specs: $ arlon clusterspec list NAME APIPROV CLOUDPROV TYPE KUBEVERSION NODETYPE NODECNT MSTNODECNT SSHKEY CAS CASMIN CASMAX TAGS DESCRIPTION capi-eks capi aws eks v1.21.10 t2.large 2 3 leb false 1 9 staging 2 node eks for general purpose capi-kubeadm-3node capi aws kubeadm v1.21.10 t2.medium 3 3 leb false 1 9 devel,test 3 node kubeadm for dev/test xplane-eks-3node xplane aws eks v1.21.10 t2.small 4 3 leb false 1 9 experimental 4 node eks managed by crossplane Bundles \u00b6 First create a static bundle containing raw YAML for the guestbook sample application from this example file: cd ${ ARLON_REPO } arlon bundle create guestbook-static --tags applications --desc \"guestbook app\" --from-file examples/bundles/guestbook.yaml ( Note: the YAML is simply a concatenation of the files found in the ArgoCD Example Apps repo ) To illustrate the difference between static and dynamic bundles, we create a dynamic version of the same application, this time using a reference to a git directory containing the YAML. We could point it directly to the copy in the ArgoCD Example Apps repo , but we'll want to make modifications to it, so we instead create a new directory to host our own copy in our workspace directory: cd ${ WORKSPACE_REPO } mkdir -p bundles/guestbook cp ${ ARLON_REPO } /examples/bundles/guestbook.yaml bundles/guestbook git add bundles/guestbook git commit -m \"add guestbook\" git push origin main arlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-url ${ WORKSPACE_REPO_URL } --repo-path bundles/guestbook # OR # using repository aliases # using the default alias arlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-path bundles/guestbook # using the prod alias arlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-path bundles/guestbook --repo-alias prod Next, we create a static bundle for another \"dummy\" application, an Ubuntu pod (OS version: \"Xenial\") that does nothing but print the date-time in an infinite sleep loop: cd ${ ARLON_REPO } arlon bundle create xenial-static --tags applications --desc \"xenial pod\" --from-file examples/bundles/xenial.yaml Finally, we create a bundle for the Calico CNI, which provides pod networking. Some types of clusters (e.g. kubeadm) require a CNI provider to be installed onto a newly created cluster, so encapsulating the provider as a bundle will give us a flexible way to install it. We download a known copy from the authoritative source and store it the workspace repo in order to create a dynamic bundle from it: cd ${ WORKSPACE_REPO } mkdir -p bundles/calico curl https://docs.projectcalico.org/v3.21/manifests/calico.yaml -o bundles/calico/calico.yaml git add bundles/calico git commit -m \"add calico\" git push origin main arlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-url ${ WORKSPACE_REPO_URL } --repo-path bundles/calico # OR # using repository aliases # using the default alias arlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-path bundles/calico # using the prod alias arlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-path bundles/calico --repo-alias prod List your bundles to verify they were correctly entered: $ arlon bundle list NAME TYPE TAGS REPO-URL REPO-PATH DESCRIPTION calico dynamic networking,cni ${ WORKSPACE_REPO_URL } bundles/calico Calico CNI guestbook-dynamic dynamic applications ${ WORKSPACE_REPO_URL } bundles/guestbook guestbook app ( dynamic ) guestbook-static static applications ( N/A ) ( N/A ) guestbook app xenial-static static applications ( N/A ) ( N/A ) ubuntu pod in infinite sleep loop Profiles \u00b6 We can now create profiles to group bundles into useful, deployable sets. First, create a static profile containing bundles xenial-static and guestbook-static: arlon profile create static-1 --static --bundles guestbook-static,xenial-static --desc \"static profile 1\" --tags examples Secondly, create a dynamic version of the same profile. We'll store the compiled form of the profile in the profiles/dynamic-1 directory of the workspace repo. We don't create it manually; instead, the arlon CLI will create it for us, and it will push the change to git: arlon profile create dynamic-1 --repo-url ${ WORKSPACE_REPO_URL } --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples # OR # using repository aliases # using the default alias arlon profile create dynamic-1 --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples # using the prod alias arlon profile create dynamic-1 --repo-alias prod --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples Note: the --repo-base-path profiles option tells arlon to create the profile under a base directory profiles/ (to be created if it doesn't exist). That is in fact the default value of that option, so it is not necessary to specify it in this case. To verify that the compiled profile was created correctly: $ cd ${ WORKSPACE_REPO } $ git pull $ tree profiles profiles \u251c\u2500\u2500 dynamic-1 \u2502 \u251c\u2500\u2500 mgmt \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u251c\u2500\u2500 guestbook-dynamic.yaml \u2502 \u2502 \u251c\u2500\u2500 placeholder_configmap.yaml \u2502 \u2502 \u2514\u2500\u2500 xenial.yaml \u2502 \u2514\u2500\u2500 workload \u2502 \u2514\u2500\u2500 xenial \u2502 \u2514\u2500\u2500 xenial.yaml [ ... ] Since xenial is a static bundle, a copy of its YAML was stored in workload/xenial/xenial.yaml . This is not done for guestbook-dynamic because it is dynamic. Finally, we create another variant of the same profile, with the only difference being the addition of Calico bundle. It'll be used on clusters that need a CNI provider: arlon profile create dynamic-2-calico --repo-url ${ WORKSPACE_REPO_URL } --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples # OR # using repository aliases # using the default alias arlon profile create dynamic-2-calico --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples # using the prod alias arlon profile create dynamic-2-calico --repo-alias prod --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples Listing the profiles should show: $ arlon profile list NAME TYPE BUNDLES REPO-URL REPO-PATH TAGS DESCRIPTION dynamic-1 dynamic guestbook-static,xenial-static ${ WORKSPACE_REPO_URL } profiles/dynamic-1 examples dynamic test 1 dynamic-2-calico dynamic calico,guestbook-static,xenial-static ${ WORKSPACE_REPO_URL } profiles/dynamic-2-calico examples dynamic test 1 static-1 static guestbook-dynamic,xenial-static ( N/A ) ( N/A ) examples static profile 1 Clusters (gen1) \u00b6 We are now ready to deploy our first cluster. It will be of type EKS. Since EKS clusters come configured with pod networking out of the box, we choose a profile that does not include Calico: dynamic-1 . When deploying a cluster, arlon creates in git a Helm chart containing the manifests for creating and bootstrapping the cluster. Arlon then creates an ArgoCD App referencing the chart, thereby relying on ArgoCD to orchestrate the whole process of deploying and configuring the cluster. The arlon deploy command accepts a git URL and path for this git location. Any git repo can be used (so long as it's registered with ArgoCD), but we'll use the workspace cluster for convenience: arlon cluster deploy --repo-url ${ WORKSPACE_REPO_URL } --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks # OR # using repository aliases # using the default alias arlon cluster deploy --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks # using the prod alias arlon cluster deploy --repo-alias prod --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks The git directory hosting the cluster Helm chart is created as a subdirectory of a base path in the repo. The base path can be specified with --base-path , but we'll leave it unspecified in order to use the default value of clusters . Consequently, this example produces the directory clusters/eks-1/ in the repo. To verify its presence: $ cd ${ WORKSPACE_REPO } $ git pull $ tree clusters/eks-1 clusters/eks-1 \u2514\u2500\u2500 mgmt \u251c\u2500\u2500 charts \u2502 \u251c\u2500\u2500 capi-aws-eks \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u251c\u2500\u2500 capi-aws-kubeadm \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u2514\u2500\u2500 xplane-aws-eks \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2514\u2500\u2500 templates \u2502 \u251c\u2500\u2500 cluster.yaml \u2502 \u2514\u2500\u2500 network.yaml \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 clusterregistration.yaml \u2502 \u251c\u2500\u2500 ns.yaml \u2502 \u251c\u2500\u2500 profile.yaml \u2502 \u2514\u2500\u2500 rbac.yaml \u2514\u2500\u2500 values.yaml The chart contains several subcharts under mgmt/charts/ , one for each supported type of cluster. Only one of them will be enabled, in this case capi-aws-eks (Cluster API on AWS with type EKS). At this point, the cluster is provisioning and can be seen in arlon and AWS EKS: $ arlon cluster list NAME CLUSTERSPEC PROFILE eks-1 capi-eks dynamic-1 $ aws eks list-clusters { \"clusters\" : [ \"eks-1_eks-1-control-plane\" , ] } Eventually, it will also be seen as a registered cluster in argocd, but this won't be visible for a while, because the cluster is not registered until its control plane (the Kubernetes API) is ready: $ argocd cluster list SERVER NAME VERSION STATUS MESSAGE https://9F07DC211252C6F7686F90FA5B8B8447.gr7.us-west-2.eks.amazonaws.com eks-1 1 .18+ Successful https://kubernetes.default.svc in -cluster 1 .20+ Successful To monitor the progress of the cluster deployment, check the status of the ArgoCD app of the same name: $ argocd app list NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGET eks-1 https://kubernetes.default.svc default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } clusters/eks-1/mgmt main eks-1-guestbook-static default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-1/workload/guestbook-static HEAD eks-1-profile-dynamic-1 https://kubernetes.default.svc argocd default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-1/mgmt HEAD eks-1-xenial default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-1/workload/xenial HEAD The top-level app eks-1 is the root of all argocd apps that make up the cluster and its configuration contents. The next level app eks-1-profile-dynamic-1 represents the profile, and its children apps eks-1-guestbook-static and eks-1-xenial correspond to the bundles. Note: The overall tree-like organization of the apps and their health status can be visually observed in the ArgoCD web user interface._ The cluster is fully deployed once those apps are all Synced and Healthy . An EKS cluster typically takes 10-15 minutes to finish deploying. Behavioral differences between static and dynamic bundles & profiles \u00b6 Static bundle \u00b6 A change to a static bundle does not affect existing clusters using that bundle (through a profile). To illustrate this, bring up the ArgoCD UI and open the detailed view of the eks-1-guestbook-static application, which applies the guestbook-static bundle to the eks-1 cluster. Note that there is only one guestbook-ui pod. Next, update the guestbook-static bundle to have 3 replicas of the pod: arlon bundle update guestbook-static --from-file examples/bundles/guestbook-3replicas.yaml Note that the UI continues to show one pod. Only new clusters consuming this bundle will have the 3 replicas. Dynamic profile \u00b6 Before discussing dynamic bundles, we take a small detour to introduce dynamic profiles, since this will help understand the relationship between profiles and bundles. To illustrate how a profile can be updated, we remove guestbook-static bundle from dynamic-1 by specifying a new bundle set: arlon profile update dynamic-1 --bundles xenial Since the old bundle set was guestbook-static,xenial-static , that command resulted in the removal of guestbook-static from the profile. In the UI, observe the eks-1-profile-dynamic-1 app going through Sync and Progressing phases, eventually reaching the healthy (green) state. And most importantly, the eks-1-guestbook-static app is gone. The reason this real-time change to the cluster was possible is that the dynamic-1 profile is dynamic, meaning any change to its composition results in arlon updating the corresponding compiled Helm chart in git. ArgoCD detects this git change and propagates the app / configuration updates to the cluster. If the profile were of the static type, a change in its composition (the set of bundles) would not have affected existing clusters using that profile. It would only affect new clusters created with the profile. Dynamic bundle \u00b6 To illustrate the defining characteristic of a dynamic bundle, we first add guestbook-dynamic to dynamic-1 : arlon profile update dynamic-1 --bundles xenial,guestbook-dynamic Observe the re-appearance of the guestbook application, which is managed by the eks-1-guestbook-dynamic ArgoCD app. A detailed view of the app shows 1 guestbook-ui pod. Remember that a dynamic bundle's manifest content is stored in git. Use these commands to change the number of pod replicas to 3: cd ${ WORKSPACE_REPO } git pull # to get all latest changes pushed by arlon vim bundles/guestbook/guestbook.yaml # edit to change deployment's replicas to 3 git commit -am \"increase guestbook replicas\" git push origin main Observe the number of pods increasing to 3 in the UI. Any existing cluster consuming this dynamic bundle will be updated similarly, regardless of whether the bundle is consumed via a dynamic or static profile. Static profile \u00b6 Finally, a profile can be static. It means that it has no corresponding \"compiled\" component (a Helm chart) living in git. When a cluster is deployed using a static profile, the set of bundles (whether static or dynamic) it receives is determined by the bundle set defined by the profile at deployment time, and will not change in the future, even if the profile is updated to a new set at a later time. Cluster updates and upgrades \u00b6 The arlon cluster update [flags] command allows you to make changes to an existing cluster. The clusterspec, profile, or both can change, provided that the following rules and guidelines are followed. Clusterspec \u00b6 There are two scenarios. In the first, the clusterspec name associated with the cluster hasn't changed, meaning the cluster is using the same clusterspec. However, some properties of the clusterspec's properties have changed since the cluster was deployed or last updated, using arlon clusterspec update Arlon supports updating the cluster to use updated values of the following properties: kubernetesVersion nodeCount nodeType Note: Updating the cluster is not allowed if other properties of its clusterspec (e.g. cluster orchestration API provider, cloud, cluster type, region, pod CIDR block, etc...) have changed, however new clusters can always be created/deployed using the changed clusterspec. A change in kubernetesVersion will result in a cluster upgrade/downgrade. There are some restrictions and caveats you need to be aware of: The specific Kubernetes version must be supported by the particular implementation and release of the underlying cluster orchestration API provider cloud, and cluster type. In general, the control plane will be upgraded first Existing nodes are not typically not upgraded to the new Kubernetes version. Only new nodes (added as part of manual nodeCount change or autoscaling) In the second scenario, as part of an update operation, you may choose to associate the cluster with a different clusterspec altogether. The rule governing the allowed property changes remains the same: the cluster update operation is allowed if, relative to the previously associated clusterspec, the new clusterspec's properties differ only in the values listed above. Profile \u00b6 You can specify a completely different profile when updating a cluster. All bundles previously used will be removed from the cluster, and new ones specified by the new profile will be applied. This is regardless of whether the old and new profiles are static or dynamic. Examples \u00b6 These sequence of commands updates a clusterspec to a newer Kubernetes version and a higher node count, then upgrades the cluster to the newer specifications: arlon clusterspec update capi-eks --nodecount 3 --kubeversion v1.19.15 arlon cluster update eks-1 Note that the 2nd command didn't need any flags because the clusterspec used is the same as before. This example updates a cluster to use a new profile my-new-profile : arlon cluster update eks-1 --profile my-new-profile Enabling Cluster Autoscaler in the workload cluster \u00b6 Bundle creation \u00b6 Register a dynamic bundle pointing to the bundles/capi-cluster-autoscaler in the Arlon repo. The capi-cluster-autoscaler bundle requires the name of the cluster, so that it knows what namespace in the management cluster to scan for CAPI resources. To enable the cluster-autoscaler bundle, add one more parameter during cluster creation: srcType . This is the ArgoCD-defined application source type (Helm, Kustomize, Directory). This example creates a bundle pointing to the bundles/capi-cluster-autoscaler in Arlon repo arlon bundle create cas-bundle --tags cas,devel,test --desc \"CAS Bundle\" --repo-url https://github.com/arlonproj/arlon.git --repo-path bundles/capi-cluster-autoscaler --srctype helm Profile creation \u00b6 Create a profile that contains this capi-cluster-autoscaler bundle. arlon profile create dynamic-cas --repo-url ${ WORKSPACE_REPO_URL } --repo-base-path profiles --bundles cas-bundle --desc \"dynamic cas profile\" --tags examples Clusterspec creation \u00b6 Create a clusterspec with CAPI as ApiProvider and autoscaling enabled.In addition to this, the ClusterAutoscaler(Min|Max)Nodes properties are used to set 2 annotations on MachineDeployment required by the cluster autoscaler for CAPI. arlon clusterspec create cas-spec --api capi --cloud aws --type eks --kubeversion v1.21.10 --nodecount 2 --nodetype t2.medium --tags devel,test --desc \"dev/test\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } --casenabled Cluster creation \u00b6 Deploy a cluster from this cluster spec and profile created in the previous steps. arlon cluster deploy --repo-url ${ WORKSPACE_REPO_URL } --cluster-name cas-cluster --profile dynamic-cas --cluster-spec cas-spec Consequently, this example produces the directory clusters/cas-cluster/ in the repo. This will contain the capi-autoscaler subchart and manifests mgmt/charts/ . To verify its contents: $ cd ${ WORKSPACE_REPO_URL } $ tree clusters/cas-cluster clusters/cas-cluster \u2514\u2500\u2500 mgmt \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 charts \u2502 \u251c\u2500\u2500 capi-aws-eks \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u251c\u2500\u2500 capi-aws-kubeadm \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u251c\u2500\u2500 capi-cluster-autoscaler \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u251c\u2500\u2500 callhomeconfig.yaml \u2502 \u2502 \u2514\u2500\u2500 rbac.yaml \u2502 \u2514\u2500\u2500 xplane-aws-eks \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2514\u2500\u2500 templates \u2502 \u251c\u2500\u2500 cluster.yaml \u2502 \u2514\u2500\u2500 network.yaml \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 clusterregistration.yaml \u2502 \u251c\u2500\u2500 ns.yaml \u2502 \u251c\u2500\u2500 profile.yaml \u2502 \u2514\u2500\u2500 rbac.yaml \u2514\u2500\u2500 values.yaml At this point, the cluster is provisioning. To monitor the progress of the cluster deployment, check the status of the ArgoCD app of the same name. Eventually, the ArgoCD apps will be synced and healthy. $ argocd app list NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGET cas-cluster https://kubernetes.default.svc default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } clusters/cas-cluster/mgmt main cas-cluster-cas-bundle cas-cluster default default Synced Healthy Auto-Prune <none> https://github.com/arlonproj/arlon.git bundles/capi-cluster-autoscaler HEAD cas-cluster-profile-dynamic-cas https://kubernetes.default.svc argocd default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-cas/mgmt","title":"Tutorial"},{"location":"tutorial/#tutorial-gen1","text":"This assumes that you plan to deploy workload clusters on AWS cloud, with Cluster API (\"CAPI\") as the cluster orchestration API provider. Also ensure you have set up a workspace repository , and it is registered as a git repo in ArgoCD. The tutorial will assume the existence of these environment variables: ${ARLON_REPO} : where the arlon repo is locally checked out ${WORKSPACE_REPO} : where the workspace repo is locally checked out ${WORKSPACE_REPO_URL} : the workspace repo's git URL. It typically looks like https://github.com/${username}/${reponame}.git ${CLOUD_REGION} : the region where you want to deploy example clusters and workloads (e.g. us-west-2) ${SSH_KEY_NAME} : the name of a public ssh key name registered in your cloud account, to enable ssh to your cluster nodes Additionally, for examples assuming arlon git register , \"default\" and a \"prod\" git repo aliases will also be given. _Note: for the best experience, make sure your workspace repo is configured to send change notifications to ArgoCD via a webhook. See the Installation section for details.","title":"Tutorial (gen1)"},{"location":"tutorial/#cluster-specs","text":"We first create a few cluster specs with different combinations of API providers and cluster types (kubeadm vs EKS). One of the cluster specs is for an unconfigured API provider (Crossplane); this is for illustrative purposes, since we will not use it in this tutorial. arlon clusterspec create capi-kubeadm-3node --api capi --cloud aws --type kubeadm --kubeversion v1.21.10 --nodecount 3 --nodetype t2.medium --tags devel,test --desc \"3 node kubeadm for dev/test\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } arlon clusterspec create capi-eks --api capi --cloud aws --type eks --kubeversion v1.21.10 --nodecount 2 --nodetype t2.large --tags staging --desc \"2 node eks for general purpose\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } arlon clusterspec create xplane-eks-3node --api xplane --cloud aws --type eks --kubeversion v1.21.10 --nodecount 4 --nodetype t2.small --tags experimental --desc \"4 node eks managed by crossplane\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } Ensure you can now list the cluster specs: $ arlon clusterspec list NAME APIPROV CLOUDPROV TYPE KUBEVERSION NODETYPE NODECNT MSTNODECNT SSHKEY CAS CASMIN CASMAX TAGS DESCRIPTION capi-eks capi aws eks v1.21.10 t2.large 2 3 leb false 1 9 staging 2 node eks for general purpose capi-kubeadm-3node capi aws kubeadm v1.21.10 t2.medium 3 3 leb false 1 9 devel,test 3 node kubeadm for dev/test xplane-eks-3node xplane aws eks v1.21.10 t2.small 4 3 leb false 1 9 experimental 4 node eks managed by crossplane","title":"Cluster specs"},{"location":"tutorial/#bundles","text":"First create a static bundle containing raw YAML for the guestbook sample application from this example file: cd ${ ARLON_REPO } arlon bundle create guestbook-static --tags applications --desc \"guestbook app\" --from-file examples/bundles/guestbook.yaml ( Note: the YAML is simply a concatenation of the files found in the ArgoCD Example Apps repo ) To illustrate the difference between static and dynamic bundles, we create a dynamic version of the same application, this time using a reference to a git directory containing the YAML. We could point it directly to the copy in the ArgoCD Example Apps repo , but we'll want to make modifications to it, so we instead create a new directory to host our own copy in our workspace directory: cd ${ WORKSPACE_REPO } mkdir -p bundles/guestbook cp ${ ARLON_REPO } /examples/bundles/guestbook.yaml bundles/guestbook git add bundles/guestbook git commit -m \"add guestbook\" git push origin main arlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-url ${ WORKSPACE_REPO_URL } --repo-path bundles/guestbook # OR # using repository aliases # using the default alias arlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-path bundles/guestbook # using the prod alias arlon bundle create guestbook-dynamic --tags applications --desc \"guestbook app (dynamic)\" --repo-path bundles/guestbook --repo-alias prod Next, we create a static bundle for another \"dummy\" application, an Ubuntu pod (OS version: \"Xenial\") that does nothing but print the date-time in an infinite sleep loop: cd ${ ARLON_REPO } arlon bundle create xenial-static --tags applications --desc \"xenial pod\" --from-file examples/bundles/xenial.yaml Finally, we create a bundle for the Calico CNI, which provides pod networking. Some types of clusters (e.g. kubeadm) require a CNI provider to be installed onto a newly created cluster, so encapsulating the provider as a bundle will give us a flexible way to install it. We download a known copy from the authoritative source and store it the workspace repo in order to create a dynamic bundle from it: cd ${ WORKSPACE_REPO } mkdir -p bundles/calico curl https://docs.projectcalico.org/v3.21/manifests/calico.yaml -o bundles/calico/calico.yaml git add bundles/calico git commit -m \"add calico\" git push origin main arlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-url ${ WORKSPACE_REPO_URL } --repo-path bundles/calico # OR # using repository aliases # using the default alias arlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-path bundles/calico # using the prod alias arlon bundle create calico --tags networking,cni --desc \"Calico CNI\" --repo-path bundles/calico --repo-alias prod List your bundles to verify they were correctly entered: $ arlon bundle list NAME TYPE TAGS REPO-URL REPO-PATH DESCRIPTION calico dynamic networking,cni ${ WORKSPACE_REPO_URL } bundles/calico Calico CNI guestbook-dynamic dynamic applications ${ WORKSPACE_REPO_URL } bundles/guestbook guestbook app ( dynamic ) guestbook-static static applications ( N/A ) ( N/A ) guestbook app xenial-static static applications ( N/A ) ( N/A ) ubuntu pod in infinite sleep loop","title":"Bundles"},{"location":"tutorial/#profiles","text":"We can now create profiles to group bundles into useful, deployable sets. First, create a static profile containing bundles xenial-static and guestbook-static: arlon profile create static-1 --static --bundles guestbook-static,xenial-static --desc \"static profile 1\" --tags examples Secondly, create a dynamic version of the same profile. We'll store the compiled form of the profile in the profiles/dynamic-1 directory of the workspace repo. We don't create it manually; instead, the arlon CLI will create it for us, and it will push the change to git: arlon profile create dynamic-1 --repo-url ${ WORKSPACE_REPO_URL } --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples # OR # using repository aliases # using the default alias arlon profile create dynamic-1 --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples # using the prod alias arlon profile create dynamic-1 --repo-alias prod --repo-base-path profiles --bundles guestbook-static,xenial-static --desc \"dynamic test 1\" --tags examples Note: the --repo-base-path profiles option tells arlon to create the profile under a base directory profiles/ (to be created if it doesn't exist). That is in fact the default value of that option, so it is not necessary to specify it in this case. To verify that the compiled profile was created correctly: $ cd ${ WORKSPACE_REPO } $ git pull $ tree profiles profiles \u251c\u2500\u2500 dynamic-1 \u2502 \u251c\u2500\u2500 mgmt \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u251c\u2500\u2500 guestbook-dynamic.yaml \u2502 \u2502 \u251c\u2500\u2500 placeholder_configmap.yaml \u2502 \u2502 \u2514\u2500\u2500 xenial.yaml \u2502 \u2514\u2500\u2500 workload \u2502 \u2514\u2500\u2500 xenial \u2502 \u2514\u2500\u2500 xenial.yaml [ ... ] Since xenial is a static bundle, a copy of its YAML was stored in workload/xenial/xenial.yaml . This is not done for guestbook-dynamic because it is dynamic. Finally, we create another variant of the same profile, with the only difference being the addition of Calico bundle. It'll be used on clusters that need a CNI provider: arlon profile create dynamic-2-calico --repo-url ${ WORKSPACE_REPO_URL } --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples # OR # using repository aliases # using the default alias arlon profile create dynamic-2-calico --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples # using the prod alias arlon profile create dynamic-2-calico --repo-alias prod --repo-base-path profiles --bundles calico,guestbook-dynamic,xenial-static --desc \"dynamic test 1\" --tags examples Listing the profiles should show: $ arlon profile list NAME TYPE BUNDLES REPO-URL REPO-PATH TAGS DESCRIPTION dynamic-1 dynamic guestbook-static,xenial-static ${ WORKSPACE_REPO_URL } profiles/dynamic-1 examples dynamic test 1 dynamic-2-calico dynamic calico,guestbook-static,xenial-static ${ WORKSPACE_REPO_URL } profiles/dynamic-2-calico examples dynamic test 1 static-1 static guestbook-dynamic,xenial-static ( N/A ) ( N/A ) examples static profile 1","title":"Profiles"},{"location":"tutorial/#clusters-gen1","text":"We are now ready to deploy our first cluster. It will be of type EKS. Since EKS clusters come configured with pod networking out of the box, we choose a profile that does not include Calico: dynamic-1 . When deploying a cluster, arlon creates in git a Helm chart containing the manifests for creating and bootstrapping the cluster. Arlon then creates an ArgoCD App referencing the chart, thereby relying on ArgoCD to orchestrate the whole process of deploying and configuring the cluster. The arlon deploy command accepts a git URL and path for this git location. Any git repo can be used (so long as it's registered with ArgoCD), but we'll use the workspace cluster for convenience: arlon cluster deploy --repo-url ${ WORKSPACE_REPO_URL } --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks # OR # using repository aliases # using the default alias arlon cluster deploy --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks # using the prod alias arlon cluster deploy --repo-alias prod --cluster-name eks-1 --profile dynamic-1 --cluster-spec capi-eks The git directory hosting the cluster Helm chart is created as a subdirectory of a base path in the repo. The base path can be specified with --base-path , but we'll leave it unspecified in order to use the default value of clusters . Consequently, this example produces the directory clusters/eks-1/ in the repo. To verify its presence: $ cd ${ WORKSPACE_REPO } $ git pull $ tree clusters/eks-1 clusters/eks-1 \u2514\u2500\u2500 mgmt \u251c\u2500\u2500 charts \u2502 \u251c\u2500\u2500 capi-aws-eks \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u251c\u2500\u2500 capi-aws-kubeadm \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u2514\u2500\u2500 xplane-aws-eks \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2514\u2500\u2500 templates \u2502 \u251c\u2500\u2500 cluster.yaml \u2502 \u2514\u2500\u2500 network.yaml \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 clusterregistration.yaml \u2502 \u251c\u2500\u2500 ns.yaml \u2502 \u251c\u2500\u2500 profile.yaml \u2502 \u2514\u2500\u2500 rbac.yaml \u2514\u2500\u2500 values.yaml The chart contains several subcharts under mgmt/charts/ , one for each supported type of cluster. Only one of them will be enabled, in this case capi-aws-eks (Cluster API on AWS with type EKS). At this point, the cluster is provisioning and can be seen in arlon and AWS EKS: $ arlon cluster list NAME CLUSTERSPEC PROFILE eks-1 capi-eks dynamic-1 $ aws eks list-clusters { \"clusters\" : [ \"eks-1_eks-1-control-plane\" , ] } Eventually, it will also be seen as a registered cluster in argocd, but this won't be visible for a while, because the cluster is not registered until its control plane (the Kubernetes API) is ready: $ argocd cluster list SERVER NAME VERSION STATUS MESSAGE https://9F07DC211252C6F7686F90FA5B8B8447.gr7.us-west-2.eks.amazonaws.com eks-1 1 .18+ Successful https://kubernetes.default.svc in -cluster 1 .20+ Successful To monitor the progress of the cluster deployment, check the status of the ArgoCD app of the same name: $ argocd app list NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGET eks-1 https://kubernetes.default.svc default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } clusters/eks-1/mgmt main eks-1-guestbook-static default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-1/workload/guestbook-static HEAD eks-1-profile-dynamic-1 https://kubernetes.default.svc argocd default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-1/mgmt HEAD eks-1-xenial default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-1/workload/xenial HEAD The top-level app eks-1 is the root of all argocd apps that make up the cluster and its configuration contents. The next level app eks-1-profile-dynamic-1 represents the profile, and its children apps eks-1-guestbook-static and eks-1-xenial correspond to the bundles. Note: The overall tree-like organization of the apps and their health status can be visually observed in the ArgoCD web user interface._ The cluster is fully deployed once those apps are all Synced and Healthy . An EKS cluster typically takes 10-15 minutes to finish deploying.","title":"Clusters (gen1)"},{"location":"tutorial/#behavioral-differences-between-static-and-dynamic-bundles-profiles","text":"","title":"Behavioral differences between static and dynamic bundles &amp; profiles"},{"location":"tutorial/#static-bundle","text":"A change to a static bundle does not affect existing clusters using that bundle (through a profile). To illustrate this, bring up the ArgoCD UI and open the detailed view of the eks-1-guestbook-static application, which applies the guestbook-static bundle to the eks-1 cluster. Note that there is only one guestbook-ui pod. Next, update the guestbook-static bundle to have 3 replicas of the pod: arlon bundle update guestbook-static --from-file examples/bundles/guestbook-3replicas.yaml Note that the UI continues to show one pod. Only new clusters consuming this bundle will have the 3 replicas.","title":"Static bundle"},{"location":"tutorial/#dynamic-profile","text":"Before discussing dynamic bundles, we take a small detour to introduce dynamic profiles, since this will help understand the relationship between profiles and bundles. To illustrate how a profile can be updated, we remove guestbook-static bundle from dynamic-1 by specifying a new bundle set: arlon profile update dynamic-1 --bundles xenial Since the old bundle set was guestbook-static,xenial-static , that command resulted in the removal of guestbook-static from the profile. In the UI, observe the eks-1-profile-dynamic-1 app going through Sync and Progressing phases, eventually reaching the healthy (green) state. And most importantly, the eks-1-guestbook-static app is gone. The reason this real-time change to the cluster was possible is that the dynamic-1 profile is dynamic, meaning any change to its composition results in arlon updating the corresponding compiled Helm chart in git. ArgoCD detects this git change and propagates the app / configuration updates to the cluster. If the profile were of the static type, a change in its composition (the set of bundles) would not have affected existing clusters using that profile. It would only affect new clusters created with the profile.","title":"Dynamic profile"},{"location":"tutorial/#dynamic-bundle","text":"To illustrate the defining characteristic of a dynamic bundle, we first add guestbook-dynamic to dynamic-1 : arlon profile update dynamic-1 --bundles xenial,guestbook-dynamic Observe the re-appearance of the guestbook application, which is managed by the eks-1-guestbook-dynamic ArgoCD app. A detailed view of the app shows 1 guestbook-ui pod. Remember that a dynamic bundle's manifest content is stored in git. Use these commands to change the number of pod replicas to 3: cd ${ WORKSPACE_REPO } git pull # to get all latest changes pushed by arlon vim bundles/guestbook/guestbook.yaml # edit to change deployment's replicas to 3 git commit -am \"increase guestbook replicas\" git push origin main Observe the number of pods increasing to 3 in the UI. Any existing cluster consuming this dynamic bundle will be updated similarly, regardless of whether the bundle is consumed via a dynamic or static profile.","title":"Dynamic bundle"},{"location":"tutorial/#static-profile","text":"Finally, a profile can be static. It means that it has no corresponding \"compiled\" component (a Helm chart) living in git. When a cluster is deployed using a static profile, the set of bundles (whether static or dynamic) it receives is determined by the bundle set defined by the profile at deployment time, and will not change in the future, even if the profile is updated to a new set at a later time.","title":"Static profile"},{"location":"tutorial/#cluster-updates-and-upgrades","text":"The arlon cluster update [flags] command allows you to make changes to an existing cluster. The clusterspec, profile, or both can change, provided that the following rules and guidelines are followed.","title":"Cluster updates and upgrades"},{"location":"tutorial/#clusterspec","text":"There are two scenarios. In the first, the clusterspec name associated with the cluster hasn't changed, meaning the cluster is using the same clusterspec. However, some properties of the clusterspec's properties have changed since the cluster was deployed or last updated, using arlon clusterspec update Arlon supports updating the cluster to use updated values of the following properties: kubernetesVersion nodeCount nodeType Note: Updating the cluster is not allowed if other properties of its clusterspec (e.g. cluster orchestration API provider, cloud, cluster type, region, pod CIDR block, etc...) have changed, however new clusters can always be created/deployed using the changed clusterspec. A change in kubernetesVersion will result in a cluster upgrade/downgrade. There are some restrictions and caveats you need to be aware of: The specific Kubernetes version must be supported by the particular implementation and release of the underlying cluster orchestration API provider cloud, and cluster type. In general, the control plane will be upgraded first Existing nodes are not typically not upgraded to the new Kubernetes version. Only new nodes (added as part of manual nodeCount change or autoscaling) In the second scenario, as part of an update operation, you may choose to associate the cluster with a different clusterspec altogether. The rule governing the allowed property changes remains the same: the cluster update operation is allowed if, relative to the previously associated clusterspec, the new clusterspec's properties differ only in the values listed above.","title":"Clusterspec"},{"location":"tutorial/#profile","text":"You can specify a completely different profile when updating a cluster. All bundles previously used will be removed from the cluster, and new ones specified by the new profile will be applied. This is regardless of whether the old and new profiles are static or dynamic.","title":"Profile"},{"location":"tutorial/#examples","text":"These sequence of commands updates a clusterspec to a newer Kubernetes version and a higher node count, then upgrades the cluster to the newer specifications: arlon clusterspec update capi-eks --nodecount 3 --kubeversion v1.19.15 arlon cluster update eks-1 Note that the 2nd command didn't need any flags because the clusterspec used is the same as before. This example updates a cluster to use a new profile my-new-profile : arlon cluster update eks-1 --profile my-new-profile","title":"Examples"},{"location":"tutorial/#enabling-cluster-autoscaler-in-the-workload-cluster","text":"","title":"Enabling Cluster Autoscaler in the workload cluster"},{"location":"tutorial/#bundle-creation","text":"Register a dynamic bundle pointing to the bundles/capi-cluster-autoscaler in the Arlon repo. The capi-cluster-autoscaler bundle requires the name of the cluster, so that it knows what namespace in the management cluster to scan for CAPI resources. To enable the cluster-autoscaler bundle, add one more parameter during cluster creation: srcType . This is the ArgoCD-defined application source type (Helm, Kustomize, Directory). This example creates a bundle pointing to the bundles/capi-cluster-autoscaler in Arlon repo arlon bundle create cas-bundle --tags cas,devel,test --desc \"CAS Bundle\" --repo-url https://github.com/arlonproj/arlon.git --repo-path bundles/capi-cluster-autoscaler --srctype helm","title":"Bundle creation"},{"location":"tutorial/#profile-creation","text":"Create a profile that contains this capi-cluster-autoscaler bundle. arlon profile create dynamic-cas --repo-url ${ WORKSPACE_REPO_URL } --repo-base-path profiles --bundles cas-bundle --desc \"dynamic cas profile\" --tags examples","title":"Profile creation"},{"location":"tutorial/#clusterspec-creation","text":"Create a clusterspec with CAPI as ApiProvider and autoscaling enabled.In addition to this, the ClusterAutoscaler(Min|Max)Nodes properties are used to set 2 annotations on MachineDeployment required by the cluster autoscaler for CAPI. arlon clusterspec create cas-spec --api capi --cloud aws --type eks --kubeversion v1.21.10 --nodecount 2 --nodetype t2.medium --tags devel,test --desc \"dev/test\" --region ${ CLOUD_REGION } --sshkey ${ SSH_KEY_NAME } --casenabled","title":"Clusterspec creation"},{"location":"tutorial/#cluster-creation","text":"Deploy a cluster from this cluster spec and profile created in the previous steps. arlon cluster deploy --repo-url ${ WORKSPACE_REPO_URL } --cluster-name cas-cluster --profile dynamic-cas --cluster-spec cas-spec Consequently, this example produces the directory clusters/cas-cluster/ in the repo. This will contain the capi-autoscaler subchart and manifests mgmt/charts/ . To verify its contents: $ cd ${ WORKSPACE_REPO_URL } $ tree clusters/cas-cluster clusters/cas-cluster \u2514\u2500\u2500 mgmt \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 charts \u2502 \u251c\u2500\u2500 capi-aws-eks \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u251c\u2500\u2500 capi-aws-kubeadm \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u2514\u2500\u2500 cluster.yaml \u2502 \u251c\u2500\u2500 capi-cluster-autoscaler \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u2514\u2500\u2500 templates \u2502 \u2502 \u251c\u2500\u2500 callhomeconfig.yaml \u2502 \u2502 \u2514\u2500\u2500 rbac.yaml \u2502 \u2514\u2500\u2500 xplane-aws-eks \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2514\u2500\u2500 templates \u2502 \u251c\u2500\u2500 cluster.yaml \u2502 \u2514\u2500\u2500 network.yaml \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 clusterregistration.yaml \u2502 \u251c\u2500\u2500 ns.yaml \u2502 \u251c\u2500\u2500 profile.yaml \u2502 \u2514\u2500\u2500 rbac.yaml \u2514\u2500\u2500 values.yaml At this point, the cluster is provisioning. To monitor the progress of the cluster deployment, check the status of the ArgoCD app of the same name. Eventually, the ArgoCD apps will be synced and healthy. $ argocd app list NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGET cas-cluster https://kubernetes.default.svc default default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } clusters/cas-cluster/mgmt main cas-cluster-cas-bundle cas-cluster default default Synced Healthy Auto-Prune <none> https://github.com/arlonproj/arlon.git bundles/capi-cluster-autoscaler HEAD cas-cluster-profile-dynamic-cas https://kubernetes.default.svc argocd default Synced Healthy Auto-Prune <none> ${ WORKSPACE_REPO_URL } profiles/dynamic-cas/mgmt","title":"Cluster creation"}]}